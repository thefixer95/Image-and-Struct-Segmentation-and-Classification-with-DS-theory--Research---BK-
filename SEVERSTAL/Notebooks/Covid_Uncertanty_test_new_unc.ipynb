{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Michele\\anaconda3\\envs\\tfNew2023_2\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:37: UserWarning:\n",
      "\n",
      "You are currently using a nightly version of TensorFlow (2.10.0-dev20220413). \n",
      "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
      "If you encounter a bug, do not file an issue on GitHub.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]\n",
      "Tensor Flow Version: 2.10.0-dev20220413\n",
      "Keras Version: 2.9.0\n",
      "Pandas 1.4.3\n",
      "Keras Version: 1.22.4\n",
      "\n",
      "GPU is available\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # SEVERSTAL CLASSIFICATION\n",
    "\n",
    "# %% [markdown]\n",
    "# # Imports\n",
    "\n",
    "# %%\n",
    "# What version of Python do you have?\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image, ImageOps\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from skimage import exposure\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "import copy\n",
    "import cv2\n",
    "import keras.backend as K\n",
    "\n",
    "# import sklearn as sk\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "\n",
    "# %%\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, smart_resize, ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.utils import get_file, plot_model, to_categorical\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import plot as plotly_plot\n",
    "import plotly.express as px\n",
    "\n",
    "# %%\n",
    "#IMPORT ALL LAYERS AND KERAS/TENSORFLOW PARAMS\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import ZeroPadding2D, average, Average, Multiply, StringLookup, LeakyReLU, CategoryEncoding, GlobalMaxPooling2D, Rescaling, SeparableConv2D, BatchNormalization, Conv2D, Conv2DTranspose, MaxPool2D, Activation, Dropout, Dense, Flatten, Input, Concatenate, Add, AveragePooling2D, GlobalAveragePooling2D, AveragePooling1D, Reshape, UpSampling2D, Convolution2DTranspose, Subtract\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from keras.activations import relu,leaky_relu\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "import keras_ocr \n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# %%\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, log_loss, confusion_matrix, hinge_loss, roc_curve, auc, precision_recall_curve, classification_report\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# %%\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Keras Version: {np.__version__}\")\n",
    "print()\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")\n",
    "\n",
    "# %%\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# %%\n",
    "# !nvidia-smi\n",
    "\n",
    "# %%\n",
    "sys.path.append('../../Libs/DSlib-SM/')\n",
    "import ds_layer #Dempster-Shafer layer\n",
    "import utility_layer_train #Utility layer for training\n",
    "import utility_layer_test #Utility layer for training\n",
    "# import AU_imprecision #Metric average utility for set-valued classification\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Paths\n",
    "\n",
    "# %%\n",
    "nameOfExperiment = \"NewTestLambda\"\n",
    "\n",
    "# %%\n",
    "checkpointPath = Path('../../Outputs/COVID/'+nameOfExperiment+'/CHECKPOINTS/')\n",
    "graphPath = Path('../../Outputs/COVID/'+nameOfExperiment+'/GRAPHS/')\n",
    "pathSavedModel = Path('../../Outputs/COVID/'+nameOfExperiment+'/MODELS-PB/')\n",
    "plotpath = Path('../../Outputs/COVID/'+nameOfExperiment+'/PLTS/')\n",
    "evalspath = Path('../../Outputs/COVID/'+nameOfExperiment+'/EVALUATIONS/')\n",
    "\n",
    "# %%\n",
    "checkpointPath.mkdir(parents=True, exist_ok=True)\n",
    "graphPath.mkdir(parents=True, exist_ok=True)\n",
    "pathSavedModel.mkdir(parents=True, exist_ok=True)\n",
    "plotpath.mkdir(parents=True, exist_ok=True)\n",
    "evalspath.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    \n",
    "    '''\n",
    "    A simple callback for finding the optimal learning rate range for your model + dataset. \n",
    "    \n",
    "    # Usage\n",
    "        ```python\n",
    "            lr_finder = LRFinder(min_lr=1e-5, \n",
    "                                 max_lr=1e-2, \n",
    "                                 steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
    "                                 epochs=3)\n",
    "            model.fit(X_train, Y_train, callbacks=[lr_finder])\n",
    "            \n",
    "            lr_finder.plot_loss()\n",
    "        ```\n",
    "    \n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n",
    "        \n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: https://arxiv.org/abs/1506.01186\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.total_iterations = steps_per_epoch * epochs\n",
    "        self.iteration = 0\n",
    "        self.history = {}\n",
    "        \n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        x = self.iteration / self.total_iterations \n",
    "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
    "        \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.iteration += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.iteration)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    " \n",
    "    def plot_lr(self):\n",
    "        '''Helper function to quickly inspect the learning rate schedule.'''\n",
    "        plt.plot(self.history['iterations'], self.history['lr'])\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Learning rate')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        '''Helper function to quickly observe the learning rate experiment results.'''\n",
    "        plt.plot(self.history['lr'], self.history['loss'])\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
    "    '''\n",
    "    def schedule(epoch):\n",
    "        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
    "    \n",
    "    return LearningRateScheduler(schedule)\n",
    "\n",
    "lr_sched = step_decay_schedule(initial_lr=1e-4, decay_factor=0.75, step_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDRScheduler(Callback):\n",
    "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
    "    # Usage\n",
    "        ```python\n",
    "            schedule = SGDRScheduler(min_lr=1e-5,\n",
    "                                     max_lr=1e-2,\n",
    "                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
    "                                     lr_decay=0.9,\n",
    "                                     cycle_length=5,\n",
    "                                     mult_factor=1.5)\n",
    "            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n",
    "        ```\n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        lr_decay: Reduce the max_lr after the completion of each cycle.\n",
    "                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n",
    "        cycle_length: Initial number of epochs in a cycle.\n",
    "        mult_factor: Scale epochs_to_restart after each full cycle completion.\n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: http://arxiv.org/abs/1608.03983\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=10,\n",
    "                 mult_factor=2):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        print('epoch end, checking to see if end of cycle...')\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            print('cycle finished, saving weights...')\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        return self.clr()\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        print('finished training, reloading weights from end of last cycle...')\n",
    "        self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "SEED = 42\n",
    "# SEED = 3\n",
    "# %%\n",
    "# # IMAGE_SIZE = [1600,256] #original size\n",
    "# # IMAGE_SIZE = [256,1600] #original size\n",
    "# IMAGE_SIZE = [256,160] #original size CROPPED\n",
    "IMAGE_SIZE = [224,224]\n",
    "# IMAGE_SIZE = [512,512]\n",
    "# IMAGE_SIZE = [256,256]\n",
    "# CLASS_NAMES = [1,2,3,4]   #classes\n",
    "CLASS_NAMES = [\"MILD\", \"SEVERE\"]   #classes with 0 where 0 is NO PROBLEM\n",
    "\n",
    "# NUMBER_SLICES = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def printAndSaveGraphs(pathWsave,nameOfModelGraph, history, Metrics, saveit = True, dpi = 300):\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model '+nameOfModelGraph+ ' loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation-HKonly'], loc='upper left')\n",
    "    if saveit:\n",
    "        plt.savefig(Path(pathWsave) / Path(nameOfModelGraph + '_loss.png'),dpi=dpi)\n",
    "    plt.show()\n",
    "\n",
    "    for met in Metrics:\n",
    "        # summarize history for accuracy\n",
    "        plt.plot(history.history[met.name])\n",
    "        plt.plot(history.history['val_'+met.name])\n",
    "        plt.title('model '+nameOfModelGraph+ met.name)\n",
    "        plt.ylabel(met.name)\n",
    "        # plt.plot(history.history[met])\n",
    "        # plt.plot(history.history['val_'+met])\n",
    "        # plt.title('model '+nameOfModelGraph+ met)\n",
    "        # plt.ylabel(met)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "        if saveit:\n",
    "            plt.savefig(Path(pathWsave) / Path(nameOfModelGraph + met.name + '.png'),dpi=dpi)\n",
    "            # plt.savefig(Path(pathWsave) / Path(nameOfModelGraph + met + '.png'),dpi=dpi)\n",
    "        plt.show()\n",
    "\n",
    "# %%\n",
    "def loadPNGtoArray(paths,basePath, outSize = IMAGE_SIZE, resize = True):\n",
    "    imgs = []\n",
    "    for p in tqdm(paths):\n",
    "        p = Path(p)\n",
    "        # print(p)\n",
    "        # fullPath = Path(str(basePath) + str(p))\n",
    "        fullPath = basePath / p\n",
    "        # print(basePath)\n",
    "        # print(fullPath)\n",
    "        if resize:\n",
    "            img = load_img(fullPath,target_size=(outSize[0],outSize[1]))\n",
    "        else:\n",
    "            img = load_img(fullPath)\n",
    "\n",
    "        \n",
    "        img = img_to_array(img) / 255.0\n",
    "        \n",
    "        imgs.append(img)\n",
    "    imgs = np.array([np.array(fname) for fname in imgs])  #transform each element of list in numpy array\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def loadPNGtoArrayNoDicom(paths,basePath, outSize = IMAGE_SIZE, resize = True):\n",
    "    imgs = []\n",
    "    for p in tqdm(paths):\n",
    "        p = Path(p).stem + '.png'\n",
    "        fullPath = basePath / p\n",
    "        if resize:\n",
    "            img = load_img(fullPath,target_size=(outSize[0],outSize[1]))\n",
    "        else:\n",
    "            img = load_img(fullPath)\n",
    "\n",
    "\n",
    "        img = img_to_array(img) / 255.0\n",
    "\n",
    "        imgs.append(img)\n",
    "    imgs = np.array([np.array(fname) for fname in imgs])  #transform each element of list in numpy array    \n",
    "    return imgs\n",
    "\n",
    "# %%\n",
    "def sliceArrays(inputArray,ns = 10):\n",
    "    outArray = np.zeros((inputArray.shape[0]*ns,inputArray.shape[1],int(inputArray.shape[2]/ns),inputArray.shape[3])).astype('float32')\n",
    "    # print(outArray.shape)\n",
    "    count = 0\n",
    "    for im in inputArray:\n",
    "        for i in range(ns):\n",
    "            m = int(i * (inputArray.shape[2]/ns))\n",
    "            j = int((i+1) * (inputArray.shape[2]/ns))\n",
    "            # print(m)\n",
    "            # print(j)\n",
    "            outArray[count*ns+i] = im[:,m:j,:]\n",
    "        count = count + 1\n",
    "\n",
    "    return outArray\n",
    "\n",
    "# %%\n",
    "#cropping of images in numpy array\n",
    "def cropNpArrayIm(npArray,h_w): #sqare crop h = w = h_w\n",
    "    outArray = np.empty((0,npArray.shape[1],h_w,3));\n",
    "    #here a for is used to take each image of the array and crop using the numpy function array_split on each axis\n",
    "    for im in npArray:\n",
    "        # imageCropX = np.array(np.array_split(im,int(npArray.shape[1]/h_w),axis=0));             #the image is split in (shape[1]/h_w) parts horizontally\n",
    "        imageCropY = np.array(np.array_split(im,int(npArray.shape[2]/h_w),axis=1));     #the images resulted from the firts split are split in (shape[2]/h_w) parts vertically\n",
    "        # imageCropY = np.concatenate(imageCropY, axis=0)                                         #the cropped images are concatenated to result a single array\n",
    "        outArray = np.concatenate((outArray,imageCropY),axis=0)                                 #the out array is fulled by the images\n",
    "    return outArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1103, 46)\n",
      "MILD: 535.0\n",
      "SEVERE: 568.0\n",
      "total errors: 1103.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## IMPORT images COVID\n",
    "\n",
    "# %% [markdown]\n",
    "# ### SETUP Datasets\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Import Database DICOM\n",
    "\n",
    "# %%\n",
    "databasePath = Path('M:/UNIUD/COVID 2022/FUSION 2022/DATABASE_CNN/COVIDARCHIVEdicom/')\n",
    "# databasePathIMAGES = Path('M:/UNIUD/COVID 2022/FUSION 2022/DATABASE_CNN/COVIDARCHIVEdicom/images/')\n",
    "# databasePathIMAGES = Path('M:/UNIUD/COVID 2022/FUSION 2022/DATABASE_CNN/Img_Full_Size_From_DICOM_SEGMENTATION/')\n",
    "databasePathIMAGES = Path('M:/UNIUD/COVID 2022/FUSION 2022/DATABASE_CNN/Img_Full_Size_From_DICOM_EQ/')\n",
    "\n",
    "# databasePathIMAGES = Path('Img_Full_Size_Saved_From_DICOM')\n",
    "# databasePathIMAGES = Path('Img_Full_Size_From_DICOM_EQ')\n",
    "# databasePathIMAGES = Path('Img_Full_Size_From_DICOM_SEGMENTATION_BBOX')\n",
    "\n",
    "# %%\n",
    "# read in all our data\n",
    "COVID19_dataHK = pd.read_csv(databasePath / 'es_dump_data.csv')\n",
    "#take only y and filepath\n",
    "# COVID19_dataHK = pd.DataFrame(COVID19_dataHK[['ImageFile','Prognosis','sha256']])\n",
    "COVID19_dataHK['ImageFile'] = COVID19_dataHK['sha256'] + '-' + COVID19_dataHK['ImageFile']\n",
    "COVID19_dataHK = COVID19_dataHK.drop(columns=['sha256'])\n",
    "print(COVID19_dataHK.shape)\n",
    "\n",
    "# %%\n",
    "COVID19_dataHK.head()\n",
    "\n",
    "# %%\n",
    "mapping = {}\n",
    "for x in range(len(CLASS_NAMES)):\n",
    "    mapping[CLASS_NAMES[x]] = x\n",
    "\n",
    "\n",
    "\n",
    "takename = copy.deepcopy(COVID19_dataHK['Prognosis'])\n",
    "for x in range(len(takename)):\n",
    "    takename[x] = mapping[takename[x]]\n",
    "\n",
    "\n",
    "one_h_enc = to_categorical(takename)\n",
    "\n",
    "COVID19_dataHK['MILD'] = one_h_enc[:,0]\n",
    "COVID19_dataHK['SEVERE'] = one_h_enc[:,1]\n",
    "\n",
    "# %%\n",
    "#one hot encoder Hospitals\n",
    "HOSPITAL_NAMES= ['A','B','C','D','E','F']\n",
    "\n",
    "mapping = {}\n",
    "for x in range(len(HOSPITAL_NAMES)):\n",
    "    mapping[HOSPITAL_NAMES[x]] = x\n",
    "\n",
    "takename = copy.deepcopy(COVID19_dataHK['Hospital'])\n",
    "for x in range(len(takename)):\n",
    "    takename[x] = mapping[takename[x]]\n",
    "\n",
    "\n",
    "one_h_enc = to_categorical(takename)\n",
    "COVID19_dataHK['Hospital-A'] = one_h_enc[:,0]\n",
    "COVID19_dataHK['Hospital-B'] = one_h_enc[:,1]\n",
    "COVID19_dataHK['Hospital-C'] = one_h_enc[:,2]\n",
    "COVID19_dataHK['Hospital-D'] = one_h_enc[:,3]\n",
    "COVID19_dataHK['Hospital-E'] = one_h_enc[:,4]\n",
    "COVID19_dataHK['Hospital-F'] = one_h_enc[:,5]\n",
    "\n",
    "# %%\n",
    "COVID19_dataHK.head()\n",
    "\n",
    "# %%\n",
    "#CHECK PROGNOSIS\n",
    "# X_full = pd.DataFrame(COVID19_dataHK[['ImageFile']])\n",
    "X_full = pd.DataFrame(COVID19_dataHK.drop(['Row_number','Prognosis','MILD','SEVERE','Death','Hospital'], axis='columns'))\n",
    "#without images:\n",
    "Y_full = pd.DataFrame(COVID19_dataHK[['MILD','SEVERE']])\n",
    "\n",
    "# %%\n",
    "# # NO CROP\n",
    "\n",
    "class1 = Y_full.sum()[0]\n",
    "print('MILD: ' + str(class1))\n",
    "class2 = Y_full.sum()[1]\n",
    "print('SEVERE: ' + str(class2))\n",
    "total = Y_full.sum().sum()\n",
    "print('total errors: ' + str(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michele\\AppData\\Local\\Temp\\ipykernel_11316\\3098966665.py:16: DeprecationWarning:\n",
      "\n",
      "distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications import EfficientNetB1, EfficientNetV2B1, DenseNet121, EfficientNetB7\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import get_source_inputs\n",
    "import keras\n",
    "from distutils.version import StrictVersion\n",
    "if StrictVersion(keras.__version__) < StrictVersion('2.2.0'):\n",
    "    from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "else:\n",
    "    from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "#   layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  layers.RandomCrop( IMAGE_SIZE[0], IMAGE_SIZE[1],),\n",
    "  layers.RandomZoom(\n",
    "    height_factor=0.3,\n",
    "    width_factor=0.3,\n",
    "    fill_mode='nearest',\n",
    "    interpolation='bilinear',\n",
    "    # seed=None,\n",
    "    fill_value=0.0,\n",
    "  ),\n",
    "#   layers.experimental.preprocessing.RandomContrast(0.1),\n",
    "  layers.RandomRotation(\n",
    "    0.2,\n",
    "    fill_mode='nearest',\n",
    "    interpolation='bilinear',\n",
    "    seed=None,\n",
    "    fill_value=0.0,\n",
    "    ),\n",
    "])\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def prepareTrainValSet(X,Y, augment=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    # Batch all datasets.\n",
    "    ds = ds.batch(batch_size)\n",
    "    # Use data augmentation only on the training set.\n",
    "    if augment:\n",
    "        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), \n",
    "                num_parallel_calls=AUTOTUNE)\n",
    "    # Use buffered prefetching on all datasets.\n",
    "    return ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "class TimingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        self.logs=[]\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.starttime = timer()\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(timer()-self.starttime)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      Unnamed: 0  Theta_1_mass  Theta_2_mass  ThetaOther  Theta_1_BEL  \\\n",
       "0             0      0.093346      0.071867    0.834787     0.928133   \n",
       "1             1      0.081313      0.053321    0.865366     0.946679   \n",
       "2             2      0.092862      0.072014    0.835124     0.927986   \n",
       "3             3      0.100617      0.076964    0.822419     0.923036   \n",
       "4             4      0.089792      0.066034    0.844174     0.933966   \n",
       "..          ...           ...           ...         ...          ...   \n",
       "106         106      0.085180      0.054828    0.859993     0.945172   \n",
       "107         107      0.097447      0.079363    0.823190     0.920637   \n",
       "108         108      0.088090      0.072703    0.839207     0.927297   \n",
       "109         109      0.081364      0.059537    0.859099     0.940463   \n",
       "110         110      0.094516      0.074085    0.831398     0.925915   \n",
       "\n",
       "     Theta_2_BEL  Theta_Other_BEL  Theta_1_PL  Theta_2_PL  Theta_Other_PL  \\\n",
       "0       0.906654         0.834787    0.928133    0.906654             1.0   \n",
       "1       0.918687         0.865366    0.946679    0.918687             1.0   \n",
       "2       0.907138         0.835124    0.927986    0.907138             1.0   \n",
       "3       0.899383         0.822419    0.923036    0.899383             1.0   \n",
       "4       0.910208         0.844174    0.933966    0.910208             1.0   \n",
       "..           ...              ...         ...         ...             ...   \n",
       "106     0.914821         0.859993    0.945172    0.914821             1.0   \n",
       "107     0.902553         0.823190    0.920637    0.902553             1.0   \n",
       "108     0.911910         0.839207    0.927297    0.911910             1.0   \n",
       "109     0.918636         0.859099    0.940463    0.918636             1.0   \n",
       "110     0.905484         0.831398    0.925915    0.905484             1.0   \n",
       "\n",
       "     Theta_1_UNC  Theta_2_UNC  Theta_Other_UNC  S(Theta_1)  S(Theta_2)  \\\n",
       "0            0.0          0.0         0.165213    0.221364    0.189221   \n",
       "1            0.0          0.0         0.134634    0.204050    0.156307   \n",
       "2            0.0          0.0         0.164876    0.220699    0.189462   \n",
       "3            0.0          0.0         0.177581    0.231061    0.197368   \n",
       "4            0.0          0.0         0.155826    0.216422    0.179453   \n",
       "..           ...          ...              ...         ...         ...   \n",
       "106          0.0          0.0         0.140007    0.209797    0.159195   \n",
       "107          0.0          0.0         0.176810    0.226901    0.201083   \n",
       "108          0.0          0.0         0.160793    0.214006    0.190581   \n",
       "109          0.0          0.0         0.140901    0.204128    0.167963   \n",
       "110          0.0          0.0         0.168602    0.222962    0.192810   \n",
       "\n",
       "     S(Theta_Other)  UNCERTAINTY  \n",
       "0          0.153135     0.563721  \n",
       "1          0.126413     0.486771  \n",
       "2          0.152844     0.563005  \n",
       "3          0.163770     0.592198  \n",
       "4          0.144999     0.540874  \n",
       "..              ...          ...  \n",
       "106        0.131155     0.500147  \n",
       "107        0.163110     0.591094  \n",
       "108        0.149311     0.553898  \n",
       "109        0.131941     0.504032  \n",
       "110        0.156058     0.571830  \n",
       "\n",
       "[111 rows x 17 columns]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### test con numpy\n",
    "\n",
    "# filename = '../massesFromProb_theta_bel.csv'\n",
    "filename = '../Print_Outcome_massesAndUnc.csv'\n",
    "data_masses = pd.read_csv(filename)\n",
    "data_masses.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_masses = data_masses[data_masses.columns[1:4]]\n",
    "# data_masses = data_masses.drop(0)\n",
    "data_masses.head\n",
    "\n",
    "data_masses_np = data_masses.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111, 3)\n",
      "[0.0933458  0.07186708 0.83478713]\n"
     ]
    }
   ],
   "source": [
    "print(data_masses_np.shape)\n",
    "print(data_masses_np[0])\n",
    "data_masses_np_rp = np.reshape(data_masses_np,(data_masses_np.shape[0],1,data_masses_np.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111, 1, 3)\n",
      "[[[0.0933458  0.07186708 0.83478713]]\n",
      "\n",
      " [[0.08131266 0.05332135 0.865366  ]]]\n",
      "(3, 3)\n",
      "[[1 0 1]\n",
      " [0 1 1]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(data_masses_np_rp.shape)\n",
    "print(data_masses_np_rp[0:2])\n",
    "\n",
    "\n",
    "numpy_inc = np.array([[1,0,1],[0,1,1],[0,0,1]])\n",
    "print(numpy_inc.shape)\n",
    "print(numpy_inc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bel = np.matmul(data_masses_np_rp,np.transpose(numpy_inc))\n",
    "# bel = np.matmul(data_masses_np_rp,numpy_inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.92813293 0.90665421 0.83478713]]\n"
     ]
    }
   ],
   "source": [
    "print(bel[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "[[1 0 1]\n",
      " [0 1 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "numpy_int = np.array([[1,0,1],[0,1,1],[1,1,1]])\n",
    "print(numpy_int.shape)\n",
    "print(numpy_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = np.matmul(data_masses_np_rp,numpy_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.92813293 0.90665421 1.00000001]]\n"
     ]
    }
   ],
   "source": [
    "print(pl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unc = pl - bel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.16521288]]\n"
     ]
    }
   ],
   "source": [
    "print(unc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.         0.83478712]]\n"
     ]
    }
   ],
   "source": [
    "p1 = 1 - unc\n",
    "print(p1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0933458  0.07186708 0.69686954]]\n"
     ]
    }
   ],
   "source": [
    "p1 = p1 * data_masses_np_rp\n",
    "print(p1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.22136437 -0.18922149 -0.12583967]]\n"
     ]
    }
   ],
   "source": [
    "p1 = p1 * np.log(data_masses_np_rp)\n",
    "print(p1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.22136437 0.18922149 0.12583967]]\n"
     ]
    }
   ],
   "source": [
    "p1 = p1 * (-1)\n",
    "print(p1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9066542  0.92813292 0.16521287]]\n"
     ]
    }
   ],
   "source": [
    "p2 = 1 - data_masses_np_rp\n",
    "print(p2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.02729529]]\n"
     ]
    }
   ],
   "source": [
    "p2 = unc * p2\n",
    "print(p2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.22136437 0.18922149 0.15313497]]\n"
     ]
    }
   ],
   "source": [
    "S_x = p1+p2\n",
    "print(S_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56372083]\n"
     ]
    }
   ],
   "source": [
    "UNC = np.sum(S_x,axis=-1)\n",
    "print(UNC[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def ResFBlock(x,filt,kernel,stride):\n",
    "#     # x = Conv2D(filters=filt, kernel_size=(kernel,kernel),strides=(stride,stride), kernel_initializer='he_normal', padding='same', kernel_regularizer=l2(0.0005))(x)\n",
    "#     x = Conv2D(filters=filt, kernel_size=(kernel,kernel),strides=(stride,stride), padding='same', kernel_regularizer=l2(0.0005))(x)\n",
    "#     # x = Conv2D(filters=filt, kernel_size=(kernel,kernel),strides=(stride,stride), padding='same')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = Dropout(0.6)(x)\n",
    "#     return x\n",
    "\n",
    "# # def DSmassCalcLayer(lin):\n",
    "\n",
    "# #     # print(lin.shape[1])\n",
    "\n",
    "# #     ED = ds_layer.DS1(prototypes,lin.shape[1])(lin)\n",
    "# #     ED_ac = ds_layer.DS1_activate(prototypes)(ED)\n",
    "# #     mass_prototypes = ds_layer.DS2(prototypes, num_class)(ED_ac)\n",
    "# #     mass_omega = ds_layer.DS2_omega(prototypes, num_class)(mass_prototypes)\n",
    "# #     # mass_Dempster = ds_layer.DS3_Dempster(prototypes, num_class)(mass_omega)\n",
    "# #     lout = ds_layer.DS3_Dempster(prototypes, num_class)(mass_omega)\n",
    "# #     # lout = ds_layer.DS3_normalize()(mass_Dempster)\n",
    "\n",
    "# #     return lout\n",
    "\n",
    "# def MassCalcLayer_ASI(lin, num_class, blockN):\n",
    "#     # lin = Dropout(0.5)(lin)\n",
    "#     print(\"block n: \" + str(blockN))\n",
    "#     weights_x = ds_layer.L1_wadd(lin.shape[1], num_class)(lin)\n",
    "#     # weights_x = LeakyReLU()(weights_x)\n",
    "#     distances = ds_layer.L1_wadd_activate(weights_x.shape[1],weights_x.shape[2])(weights_x)\n",
    "#     # distances = LeakyReLU()(distances)\n",
    "#     # distances = BatchNormalization()(distances)\n",
    "#     # distances = BatchNormalization()(weights_x)\n",
    "#     # massesCalc = ds_layer.L2_masses()(distances)\n",
    "\n",
    "#     return distances\n",
    "\n",
    "\n",
    "# # Backup code for Multilayer fusion (CONCATENATION)\n",
    "# def DSfusionDecisionsML(inputShape = (224,224,3), lastActivation = 'sigmoid'):\n",
    "#     inputs = Input(shape=inputShape)\n",
    "#     # x = Rescaling(1.0 / 255)(inputs)\n",
    "#     print(inputs.shape)\n",
    "\n",
    "#     x = ResFBlock(inputs,16,5,2)\n",
    "#     print(x.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     block1 = ResFBlock(x,32,3,2)\n",
    "#     print(block1.shape)\n",
    "#     block1 = ResFBlock(block1,32,3,1)\n",
    "#     print(block1.shape)\n",
    "#     # block1 = Dropout(0.6)(block1)\n",
    "#     b1_pass = ResFBlock(x,32,1,2)\n",
    "#     print(b1_pass.shape)\n",
    "#     # b1_pass = Dropout(0.6)(b1_pass)\n",
    "#     block1 = Add()([block1,b1_pass])\n",
    "#     block1 = AveragePooling2D(pool_size=(2, 2), strides=(2,2), padding='same')(block1)\n",
    "\n",
    "#     block2 = ResFBlock(block1,48,3,1)\n",
    "#     block2 = ResFBlock(block2,48,3,1)\n",
    "#     # block2 = Dropout(0.7)(block2)\n",
    "#     b2_pass = ResFBlock(block1,48,1,1)\n",
    "#     # b2_pass = Dropout(0.7)(b2_pass)\n",
    "#     block2 = Add()([block2,b2_pass])\n",
    "#     block2 = AveragePooling2D(pool_size=(2, 2), strides=(2,2), padding='same')(block2)\n",
    "\n",
    "#     block3 = ResFBlock(block2,64,3,1)\n",
    "#     block3 = ResFBlock(block3,64,3,1)\n",
    "#     # block3 = Dropout(0.7)(block3)\n",
    "#     b3_pass = ResFBlock(block2,64,1,1)\n",
    "#     # b3_pass = Dropout(0.7)(b3_pass)\n",
    "#     block3 = Add()([block3,b3_pass])\n",
    "#     block3 = AveragePooling2D(pool_size=(2, 2), strides=(2,2), padding='same')(block3)\n",
    "\n",
    "#     block4 = ResFBlock(block3,80,3,1)\n",
    "#     block4 = ResFBlock(block4,80,3,1)\n",
    "#     # block4 = Dropout(0.7)(block4)\n",
    "#     b4_pass = ResFBlock(block3,80,1,1)\n",
    "#     # b4_pass = Dropout(0.7)(b4_pass)\n",
    "#     block4 = Add()([block4,b4_pass])\n",
    "#     block4 = AveragePooling2D(pool_size=(2, 2), strides=(2,2), padding='same')(block4)\n",
    "\n",
    "#     block5 = ResFBlock(block4,96,3,1)\n",
    "#     block5 = ResFBlock(block5,96,3,1)\n",
    "#     # block5 = Dropout(0.7)(block5)\n",
    "#     b5_pass = ResFBlock(block4,96,1,1)\n",
    "#     # b5_pass = Dropout(0.7)(b5_pass)\n",
    "#     block5 = Add()([block5,b5_pass])\n",
    "#     block5 = AveragePooling2D(pool_size=(2, 2), strides=(2,2), padding='same')(block5)\n",
    "\n",
    "\n",
    "#     block1 = MaxPool2D(pool_size=(4,4), strides=(4,4))(block1)\n",
    "#     block2 = MaxPool2D(pool_size=(2,2), strides=(2,2))(block2)\n",
    "#     # block3 = MaxPool2D(pool_size=(2,2), strides=(2,2))(block3)\n",
    "    \n",
    "#     blockNumber = 0\n",
    "\n",
    "#     block1 = Flatten()(block1)\n",
    "#     block1 = Dense(32, activation='relu')(block1)\n",
    "#     block1 = Dense(8, activation='relu')(block1)\n",
    "#     block1 = Dense(2, activation=lastActivation)(block1)\n",
    "#     # block1 = Dense(32, activation='relu')(block1)\n",
    "#     block1 = MassCalcLayer_ASI(block1,2,blockNumber)\n",
    "#     blockNumber = blockNumber +1\n",
    "#     # uncBlock1 = ds_layer.Uncertanty_Layer()(block1)\n",
    "#     # block1 = Multiply()([block1,uncBlock1])\n",
    "#     uncBlock1 = ds_layer.Uncertanty_Layer()(block1)\n",
    "\n",
    "#     print(\"uncBlock1: \")\n",
    "#     print(uncBlock1.shape)\n",
    "\n",
    "#     # block1 = Concatenate(axis=-1)([block1,uncBlock1])\n",
    "\n",
    "#     block2 = Flatten()(block2)\n",
    "#     block2 = Dense(32, activation='relu')(block2)\n",
    "#     block2 = Dense(8, activation='relu')(block2)\n",
    "#     block2 = Dense(2, activation=lastActivation)(block2)\n",
    "#     # block2 = Dense(32, activation='relu')(block2)\n",
    "#     block2 = MassCalcLayer_ASI(block2,2,blockNumber)\n",
    "#     blockNumber = blockNumber +1\n",
    "#     # uncBlock2 = ds_layer.Uncertanty_Layer()(block2)\n",
    "#     # block2 = Multiply()([block2,uncBlock2])\n",
    "#     uncBlock2 = ds_layer.Uncertanty_Layer()(block2)\n",
    "#     # block2 = Concatenate(axis=-1)([block2,uncBlock2])\n",
    "\n",
    "#     block3 = Flatten()(block3)\n",
    "#     block3 = Dense(32, activation='relu')(block3)\n",
    "#     block3 = Dense(8, activation='relu')(block3)\n",
    "#     block3 = Dense(2, activation=lastActivation)(block3)\n",
    "#     # block3 = Dense(32, activation='relu')(block3)\n",
    "#     block3 = MassCalcLayer_ASI(block3,2,blockNumber)\n",
    "#     blockNumber = blockNumber +1\n",
    "#     # uncBlock3 = ds_layer.Uncertanty_Layer()(block3)\n",
    "#     # block3 = Multiply()([block3,uncBlock3])\n",
    "#     uncBlock3 = ds_layer.Uncertanty_Layer()(block3)\n",
    "#     # block3 = Concatenate(axis=-1)([block3,uncBlock3])\n",
    "\n",
    "#     block4 = Flatten()(block4)\n",
    "#     block4 = Dense(32, activation='relu')(block4)\n",
    "#     block4 = Dense(8, activation='relu')(block4)\n",
    "#     block4 = Dense(2, activation=lastActivation)(block4)\n",
    "#     # block4 = Dense(32, activation='relu')(block4)\n",
    "#     block4 = MassCalcLayer_ASI(block4,2,blockNumber)\n",
    "#     blockNumber = blockNumber +1\n",
    "#     # uncBlock4 = ds_layer.Uncertanty_Layer()(block4)\n",
    "#     # block4 = Multiply()([block4,uncBlock4])\n",
    "#     uncBlock4 = ds_layer.Uncertanty_Layer()(block4)\n",
    "#     # block4 = Concatenate(axis=-1)([block4,uncBlock4])\n",
    "\n",
    "#     block5 = Flatten()(block5)\n",
    "#     block5 = Dense(32, activation='relu')(block5)\n",
    "#     block5 = Dense(8, activation='relu')(block5)\n",
    "#     block5 = Dense(2, activation=lastActivation)(block5)\n",
    "#     # block5 = Dense(32, activation='relu')(block5)\n",
    "#     block5 = MassCalcLayer_ASI(block5,2,blockNumber)\n",
    "#     blockNumber = blockNumber +1\n",
    "#     # uncBlock5 = ds_layer.Uncertanty_Layer()(block5)\n",
    "#     # block5 = Multiply()([block5,uncBlock5])\n",
    "#     uncBlock5 = ds_layer.Uncertanty_Layer()(block5)\n",
    "#     # block5 = Concatenate(axis=-1)([block5,uncBlock5])\n",
    "\n",
    "#     block1 = Multiply(name='block1_unc_x_mass')([block1,uncBlock1])\n",
    "#     block2 = Multiply(name='block2_unc_x_mass')([block2,uncBlock2])\n",
    "#     block3 = Multiply(name='block3_unc_x_mass')([block3,uncBlock3])\n",
    "#     block4 = Multiply(name='block4_unc_x_mass')([block4,uncBlock4])\n",
    "#     block5 = Multiply(name='block5_unc_x_mass')([block5,uncBlock5])\n",
    "\n",
    "    \n",
    "\n",
    "#     massFusion = Concatenate(axis=1)([block1,block2,block3,block4,block5])\n",
    "\n",
    "#     # massFusion = Concatenate()([block1,block2,block3,block4,block5])\n",
    "#     # massFusion = Add()([block1,block2,block3,block4,block5])\n",
    "#     # massFusion = layers.Average()([block1,block2,block3,block4,block5])\n",
    "#     # massFusion = Concatenate(axis=1)([block1,block2,block3,block4,block5])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print(\"masses fused layer :\"+ str(massFusion.shape))\n",
    "#     massFusion = Reshape((massFusion.shape[1]+int(massFusion.shape[1]/(massFusion.shape[2]-1)), massFusion.shape[2]-1))(massFusion)\n",
    "#     print(\"masses fused layer :\"+ str(massFusion.shape))\n",
    "#     # print(massFusion)\n",
    "\n",
    "\n",
    "#     massFusion = BatchNormalization()(massFusion)\n",
    "\n",
    "\n",
    "#     # OUTPUT WITH DS-ASI\n",
    "#     print(\"Fusione finale\")\n",
    "#     # CALCOLO DELLE MASSE A PARTIRE DALLA LISTA DI MASSE DI OGNI LIVELLO\n",
    "#     print(massFusion.shape)\n",
    "#     weights_x = ds_layer.L1_wadd(massFusion.shape[1], 2)(massFusion)\n",
    "#     # weights_x = ds_layer.L1_wadd(massFusion.shape[1], 4)(massFusion)\n",
    "#     weights_x = LeakyReLU()(weights_x)\n",
    "#     print(weights_x.shape)\n",
    "#     distances = ds_layer.L1_wadd_activate(weights_x.shape[1],weights_x.shape[2])(weights_x)\n",
    "#     distances = LeakyReLU()(distances)\n",
    "#     # print(distances.shape)\n",
    "#     massesCalc = ds_layer.L2_masses()(distances)\n",
    "#     # massesCalc = ds_layer.L2_masses()(weights_x)\n",
    "#     print(massesCalc.shape)\n",
    "#     finalMasses = ds_layer.L3_combine_masses()(massesCalc)\n",
    "#     print(finalMasses.shape)\n",
    "\n",
    "\n",
    "#     # Uncertanties = Concatenate(axis=-1)([uncBlock1,uncBlock2,uncBlock3,uncBlock4,uncBlock5]) \n",
    "\n",
    "\n",
    "#     # # DA DS2 IN DS3\n",
    "#     # newNprot = (num_class+1)*5\n",
    "#     # mass_prototypes = ds_layer.DS2(newNprot, num_class)(massFusion)\n",
    "#     # mass_omega = ds_layer.DS2_omega(newNprot, num_class)(mass_prototypes)\n",
    "#     # mass_Dempster = ds_layer.DS3_Dempster(newNprot, num_class)(mass_omega)\n",
    "#     # mass_Dempster_normalize = ds_layer.DS3_normalize()(mass_Dempster)\n",
    "    \n",
    "\n",
    "#     # #Utility layer for testing\n",
    "#     outputs = utility_layer_train.DM_pignistic(2)(finalMasses)\n",
    "#     print(outputs.shape)\n",
    "\n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "#     # model = Model(inputs=inputs, outputs=[outputs])\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer, Lambda\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class massesFromProbabilities(Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super(massesFromProbabilities,self).__init__(**kwargs)\n",
    "        self.output_dim=output_dim\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W = self.add_weight(name='kernel',\n",
    "                                #  shape=(self.output_dim),\n",
    "                                 shape=(1,self.output_dim),\n",
    "                                 initializer='random_normal',\n",
    "                                #  initializer='uniform',\n",
    "                                 trainable=True,\n",
    "                                 )\n",
    "        # print(self.W.shape)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self,x):\n",
    "        print(self.W.shape)\n",
    "        print(x.shape)\n",
    "        # return K.dot(x,self.W)\n",
    "        return K.square(x - self.W)\n",
    "    \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "\n",
    "\n",
    "def otherMass(x):\n",
    "    # print(x.shape)\n",
    "    sumMasses = K.sum(x, axis=1, keepdims=True)\n",
    "    # print(sumMasses.shape)\n",
    "    # print(sumMasses)\n",
    "    other = 1-sumMasses\n",
    "    # print(other.shape)\n",
    "    # print(other)\n",
    "    x = K.concatenate([x,other], axis=-1)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Inc_2 = np.array([[1,0,1],[0,1,1],[0,0,1]])\n",
    "\n",
    "class Inclusion_2(tf.keras.initializers.Initializer):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.null = 0\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "      return np.array([[1,0,1],[0,1,1],[0,0,1]])\n",
    "\n",
    "    def get_config(self):  # To support serialization\n",
    "      return {'mean': self.mean, 'stddev': self.stddev}\n",
    "\n",
    "class Intersection_2(tf.keras.initializers.Initializer):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.null = 0\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "      return np.array([[1,0,1],[0,1,1],[1,1,1]])\n",
    "\n",
    "    def get_config(self):  # To support serialization\n",
    "      return {'mean': self.mean, 'stddev': self.stddev}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class computeBeleaf(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(computeBeleaf,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.Inc = K.constant([[1,0,1],[0,1,1],[0,0,1]])\n",
    "        # self.Inc = self.add_weight(name='kernel',\n",
    "        #                          shape=(input_shape[-1],input_shape[-1]),\n",
    "        #                          initializer=Inclusion_2(),\n",
    "        #                         #  initializer='uniform',\n",
    "        #                          trainable=True,\n",
    "        #                          )\n",
    "        self.built = True\n",
    "\n",
    "    def call(self,x):\n",
    "        print(K.transpose(x).shape)\n",
    "        print(self.Inc.shape)\n",
    "        # print(tf.print(self.Inc))\n",
    "        return K.dot(x,K.transpose(self.Inc))\n",
    "        # return K.dot(K.transpose(self.Inc),K.transpose(x))\n",
    "        # return x * self.Inc\n",
    "    \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "\n",
    "\n",
    "class computePlausibility(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(computePlausibility,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.Int = K.constant([[1,0,1],[0,1,1],[1,1,1]])\n",
    "        # self.Int = self.add_weight(name='kernel',\n",
    "        #                          shape=(input_shape[-1],input_shape[-1]),\n",
    "        #                          initializer=Intersection_2(),\n",
    "        #                         #  initializer='uniform',\n",
    "        #                          trainable=True,\n",
    "        #                          )\n",
    "        self.built = True\n",
    "\n",
    "    def call(self,x):\n",
    "        # print(\"WAAAAAAAA\")\n",
    "        print(K.transpose(x).shape)\n",
    "        print(self.Int.shape)\n",
    "        # print(tf.print(self.Inc))\n",
    "        return K.dot(x,self.Int)\n",
    "        # return K.dot(K.transpose(self.Inc),K.transpose(x))\n",
    "        # return x * self.Inc\n",
    "    \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Int_2 = np.array([[1,0,1],[0,1,1],[1,1,1]])\n",
    "\n",
    "# print(Inc_2)\n",
    "# print(Int_2)\n",
    "\n",
    "\n",
    "# def computeBeleaf(x):\n",
    "#     # print(x.shape)\n",
    "#     # print(Inc_2.shape)\n",
    "#     # print(Inc_2)\n",
    "#     # print(K.transpose(Inc_2))\n",
    "#     # for el in x:\n",
    "#     #     print(el.shape)\n",
    "    \n",
    "#     bel =  K.dot(K.transpose(Inc_2),x)\n",
    "\n",
    "#     return bel\n",
    "\n",
    "def computeBeleaf_lambda(x):\n",
    "    Inc_2 = K.constant(np.array([[1,0,1],[0,1,1],[0,0,1]]))\n",
    "\n",
    "    return tf.matmul(x,K.transpose(Inc_2))\n",
    "\n",
    "\n",
    "def computePlausibility_lambda(x):\n",
    "    Int_2 = K.constant(np.array([[1,0,1],[0,1,1],[1,1,1]]))\n",
    "\n",
    "    return tf.matmul(x,K.transpose(Int_2))\n",
    "\n",
    "\n",
    "\n",
    "# def computePlausibility(x):\n",
    "#     pl =  K.dot(Int_2,x)\n",
    "#     return pl\n",
    "\n",
    "\n",
    "def safe_log(x):\n",
    "    max_value = tf.reduce_max(x)\n",
    "    normalized_x = x - max_value\n",
    "    return tf.math.log(tf.exp(normalized_x) + 1) + max_value\n",
    "\n",
    "def softplus_log(x, epsilon=1e-7):\n",
    "    return tf.math.log(tf.nn.softplus(x) + epsilon)\n",
    "\n",
    "\n",
    "def clipped_log(x, min_value=1e-7):\n",
    "    clipped_x = tf.maximum(x, min_value)\n",
    "    return tf.math.log(clipped_x)\n",
    "\n",
    "\n",
    "def S_UncertaintyGeneral(unc_mass):\n",
    "\n",
    "    unc = unc_mass[0]\n",
    "    mass = unc_mass[1]\n",
    "\n",
    "    p1 = 1 - unc\n",
    "    p1 = p1 * mass\n",
    "\n",
    "    p1 = p1 * clipped_log(mass)\n",
    "    p1 = p1 * (-1)\n",
    "\n",
    "    p2 = 1 - mass\n",
    "    p2 = unc * p2\n",
    "\n",
    "    S_X = p1 + p2\n",
    "\n",
    "    return S_X\n",
    "\n",
    "\n",
    "def UncertaintyGeneral(S_X):\n",
    "    return K.sum(S_X, -1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def CombineMasses(masses):\n",
    "    Int_2 = K.constant(np.array([[1,0,1],[0,1,1],[1,1,1]]))\n",
    "    ### CHATGPT\n",
    "    c = 0\n",
    "    combination = 1\n",
    "    for mass in masses:\n",
    "        if (c<1):\n",
    "            mass1 = mass\n",
    "        else:\n",
    "            print(mass1.shape)\n",
    "            print(mass.shape)\n",
    "\n",
    "\n",
    "\n",
    "            k12 = tf.matmul(mass1,Int_2)\n",
    "            print(k12.shape)\n",
    "            mass = tf.transpose(mass,perm=[0,2,1])\n",
    "            print(mass.shape)\n",
    "\n",
    "            k12 = tf.matmul(k12,mass)\n",
    "            k12 = 1 - k12\n",
    "\n",
    "            mass = tf.transpose(mass,perm=[0,2,1])\n",
    "            print(mass.shape)\n",
    "\n",
    "            # combination = mass\n",
    "\n",
    "\n",
    "            combination = k12 * (tf.matmul(mass1,K.constant(np.array([[1,0,0],[0,0,0],[1,1,1]]))) * tf.matmul(mass,K.constant(np.array([[0,0,0],[0,1,0],[1,0,1]]))))\n",
    "\n",
    "            mass1 = combination\n",
    "\n",
    "\n",
    "        # combination = combination * mass\n",
    "        c +=1\n",
    "    return combination\n",
    "\n",
    "\n",
    "def mass_to_pignistic(masses):\n",
    "    # Calculate the pignistic probability distribution for each hypothesis and its complement\n",
    "    m_1_2 = masses[:,:,:-1]\n",
    "    m_3 = masses[:,:,-1]\n",
    "    m_3 = Reshape((1,m_3.shape[-1]))(m_3)\n",
    "\n",
    "    p_prob = m_1_2 / m_3\n",
    "\n",
    "    return p_prob\n",
    "\n",
    "\n",
    "\n",
    "def ModulateMassUnc(mass_unc):\n",
    "    mass = mass_unc[0]\n",
    "    unc = mass_unc[1]\n",
    "    \n",
    "    newMasses = mass * unc[:,:,0]\n",
    "    return newMasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MassesWithUncertainty(block, number=0):\n",
    "\n",
    "    masses = massesFromProbabilities(2, name='mass_prob'+str(number))(block)\n",
    "\n",
    "    masses = Lambda(otherMass, name='thetaOther'+str(number))(masses)\n",
    "    \n",
    "    masses = Reshape((1,masses.shape[-1]))(masses)\n",
    "    \n",
    "    # bel_block1 = computeBeleaf(name='beleaf')(block1)\n",
    "    bel_block1 = Lambda(computeBeleaf_lambda, name='beleaf'+str(number))(masses)\n",
    "\n",
    "    # pl_block1 = computePlausibility(name='plausibility')(block1)\n",
    "    pl_block1 = Lambda(computePlausibility_lambda, name='plausibility'+str(number))(masses)\n",
    "\n",
    "    unc = Subtract(name='uncertainty'+str(number))([pl_block1,bel_block1])\n",
    "\n",
    "    S_X = Lambda(S_UncertaintyGeneral, name='S_X'+str(number))((unc,masses))\n",
    "\n",
    "    uncertainty = Lambda(UncertaintyGeneral, name='unc_general'+str(number))(S_X)\n",
    "\n",
    "    return masses,uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ResFBlock(x,filt,kernel,stride):\n",
    "    # x = Conv2D(filters=filt, kernel_size=(kernel,kernel),strides=(stride,stride), kernel_initializer='he_normal', padding='same', kernel_regularizer=l2(0.0005))(x)\n",
    "    x = Conv2D(filters=filt, kernel_size=(kernel,kernel),strides=(stride,stride), padding='same', kernel_regularizer=l2(0.0005))(x)\n",
    "    # x = Conv2D(filters=filt, kernel_size=(kernel,kernel),strides=(stride,stride), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "    return x\n",
    "\n",
    "# def DSmassCalcLayer(lin):\n",
    "\n",
    "#     # print(lin.shape[1])\n",
    "\n",
    "#     ED = ds_layer.DS1(prototypes,lin.shape[1])(lin)\n",
    "#     ED_ac = ds_layer.DS1_activate(prototypes)(ED)\n",
    "#     mass_prototypes = ds_layer.DS2(prototypes, num_class)(ED_ac)\n",
    "#     mass_omega = ds_layer.DS2_omega(prototypes, num_class)(mass_prototypes)\n",
    "#     # mass_Dempster = ds_layer.DS3_Dempster(prototypes, num_class)(mass_omega)\n",
    "#     lout = ds_layer.DS3_Dempster(prototypes, num_class)(mass_omega)\n",
    "#     # lout = ds_layer.DS3_normalize()(mass_Dempster)\n",
    "\n",
    "#     return lout\n",
    "\n",
    "def MassCalcLayer_ASI(lin, num_class, blockN):\n",
    "    # lin = Dropout(0.5)(lin)\n",
    "    print(\"block n: \" + str(blockN))\n",
    "    weights_x = ds_layer.L1_wadd(lin.shape[1], num_class)(lin)\n",
    "    # weights_x = LeakyReLU()(weights_x)\n",
    "    distances = ds_layer.L1_wadd_activate(weights_x.shape[1],weights_x.shape[2])(weights_x)\n",
    "    # distances = LeakyReLU()(distances)\n",
    "    # distances = BatchNormalization()(distances)\n",
    "    # distances = BatchNormalization()(weights_x)\n",
    "    # massesCalc = ds_layer.L2_masses()(distances)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "# Backup code for Multilayer fusion (CONCATENATION)\n",
    "def DSfusionDecisionsML(inputShape = (224,224,3), lastActivation = 'sigmoid'):\n",
    "    inputs = Input(shape=inputShape)\n",
    "    # x = Rescaling(1.0 / 255)(inputs)\n",
    "    print(inputs.shape)\n",
    "\n",
    "    x = ResFBlock(inputs,16,5,2)\n",
    "    print(x.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    block1 = ResFBlock(x,32,3,2)\n",
    "    print(block1.shape)\n",
    "    block1 = ResFBlock(block1,32,3,1)\n",
    "    print(block1.shape)\n",
    "    # block1 = Dropout(0.6)(block1)\n",
    "    b1_pass = ResFBlock(x,32,1,2)\n",
    "    print(b1_pass.shape)\n",
    "    # b1_pass = Dropout(0.6)(b1_pass)\n",
    "    block1 = Add()([block1,b1_pass])\n",
    "    block1 = AveragePooling2D(pool_size=(2, 2), strides=(2,2), padding='same')(block1)\n",
    "\n",
    "    block2 = ResFBlock(block1,48,3,1)\n",
    "    block2 = ResFBlock(block2,48,3,1)\n",
    "    # block2 = Dropout(0.7)(block2)\n",
    "    b2_pass = ResFBlock(block1,48,1,1)\n",
    "    # b2_pass = Dropout(0.7)(b2_pass)\n",
    "    block2 = Add()([block2,b2_pass])\n",
    "    block2 = AveragePooling2D(pool_size=(2, 2), strides=(2,2), padding='same')(block2)\n",
    "\n",
    "    block3 = ResFBlock(block2,64,3,1)\n",
    "    block3 = ResFBlock(block3,64,3,1)\n",
    "    # block3 = Dropout(0.7)(block3)\n",
    "    b3_pass = ResFBlock(block2,64,1,1)\n",
    "    # b3_pass = Dropout(0.7)(b3_pass)\n",
    "    block3 = Add()([block3,b3_pass])\n",
    "    block3 = AveragePooling2D(pool_size=(2, 2), strides=(2,2), padding='same')(block3)\n",
    "\n",
    "    block4 = ResFBlock(block3,80,3,1)\n",
    "    block4 = ResFBlock(block4,80,3,1)\n",
    "    # block4 = Dropout(0.7)(block4)\n",
    "    b4_pass = ResFBlock(block3,80,1,1)\n",
    "    # b4_pass = Dropout(0.7)(b4_pass)\n",
    "    block4 = Add()([block4,b4_pass])\n",
    "    block4 = AveragePooling2D(pool_size=(2, 2), strides=(2,2), padding='same')(block4)\n",
    "\n",
    "    block5 = ResFBlock(block4,96,3,1)\n",
    "    block5 = ResFBlock(block5,96,3,1)\n",
    "    # block5 = Dropout(0.7)(block5)\n",
    "    b5_pass = ResFBlock(block4,96,1,1)\n",
    "    # b5_pass = Dropout(0.7)(b5_pass)\n",
    "    block5 = Add()([block5,b5_pass])\n",
    "    block5 = AveragePooling2D(pool_size=(2, 2), strides=(2,2), padding='same')(block5)\n",
    "\n",
    "\n",
    "    block1 = MaxPool2D(pool_size=(4,4), strides=(4,4))(block1)\n",
    "    block2 = MaxPool2D(pool_size=(2,2), strides=(2,2))(block2)\n",
    "    # block3 = MaxPool2D(pool_size=(2,2), strides=(2,2))(block3)\n",
    "    \n",
    "    blockNumber = 0\n",
    "\n",
    "    block1 = Flatten()(block1)\n",
    "    block1 = Dense(32, activation='relu')(block1)\n",
    "    block1 = Dense(8, activation='relu')(block1)\n",
    "    block1 = Dense(2, activation=lastActivation)(block1)        #### DECISIONE 1 (A o B)\n",
    "\n",
    "    masses1, unc1 = MassesWithUncertainty(block1,1)\n",
    "\n",
    "\n",
    "\n",
    "    block2 = Flatten()(block2)\n",
    "    block2 = Dense(32, activation='relu')(block2)\n",
    "    block2 = Dense(8, activation='relu')(block2)\n",
    "    block2 = Dense(2, activation=lastActivation)(block2)        #### DECISIONE 1 (A o B)\n",
    "\n",
    "    masses2, unc2 = MassesWithUncertainty(block2,2)\n",
    "\n",
    "\n",
    "\n",
    "    block3 = Flatten()(block3)\n",
    "    block3 = Dense(32, activation='relu')(block3)\n",
    "    block3 = Dense(8, activation='relu')(block3)\n",
    "    block3 = Dense(2, activation=lastActivation)(block3)        #### DECISIONE 1 (A o B)\n",
    "\n",
    "    masses3, unc2 = MassesWithUncertainty(block3,3)\n",
    "\n",
    "    \n",
    "\n",
    "    block4 = Flatten()(block4)\n",
    "    block4 = Dense(32, activation='relu')(block4)\n",
    "    block4 = Dense(8, activation='relu')(block4)\n",
    "    block4 = Dense(2, activation=lastActivation)(block4)        #### DECISIONE 1 (A o B)\n",
    "\n",
    "    masses4, unc4 = MassesWithUncertainty(block4,4)\n",
    "    masses4 = Multiply()([masses4,unc4])\n",
    "\n",
    "    block5 = Flatten()(block5)\n",
    "    block5 = Dense(32, activation='relu')(block5)\n",
    "    block5 = Dense(8, activation='relu')(block5)\n",
    "    block5 = Dense(2, activation=lastActivation)(block5)        #### DECISIONE 1 (A o B)\n",
    "\n",
    "    masses5, unc5 = MassesWithUncertainty(block5,5)\n",
    "    masses5 = Multiply()([masses5,unc5])\n",
    "    \n",
    "\n",
    "\n",
    "    # combination = Lambda(CombineMasses, name='massCombination')((masses1,masses2,masses3,masses4,masses5))\n",
    "    combination = Lambda(CombineMasses, name='massCombination')((masses4,masses5))\n",
    "\n",
    "    # print(combination.shape)\n",
    "\n",
    "    pigProb = Lambda(mass_to_pignistic, name='pigProb')(combination)\n",
    "\n",
    "    print(pigProb.shape)\n",
    "\n",
    "\n",
    "\n",
    "    pigProb = Reshape((pigProb.shape[-1],))(pigProb)\n",
    "\n",
    "    print(pigProb.shape)\n",
    "\n",
    "    # block1 = Flatten()(pigProb)\n",
    "    # block1 = Dense(2, activation=lastActivation)(block1)        #### DECISIONE 2 (A o B)\n",
    "\n",
    "\n",
    "    outProvv = pigProb\n",
    "\n",
    "\n",
    "    # block2 = Flatten()(block2)\n",
    "    # block2 = Dense(32, activation='relu')(block2)\n",
    "    # block2 = Dense(8, activation='relu')(block2)\n",
    "    # block2 = Dense(2, activation=lastActivation)(block2)        #### DECISIONE 2 (A o B)\n",
    "\n",
    "\n",
    "\n",
    "    # block3 = Flatten()(block3)\n",
    "    # block3 = Dense(32, activation='relu')(block3)\n",
    "    # block3 = Dense(8, activation='relu')(block3)\n",
    "    # block3 = Dense(2, activation=lastActivation)(block3)        #### DECISIONE 3 (A o B)\n",
    "\n",
    "\n",
    "\n",
    "    # block4 = Flatten()(block4)\n",
    "    # block4 = Dense(32, activation='relu')(block4)\n",
    "    # block4 = Dense(8, activation='relu')(block4)\n",
    "    # block4 = Dense(2, activation=lastActivation)(block4)        #### DECISIONE 4 (A o B)\n",
    "\n",
    "\n",
    "\n",
    "    # block5 = Flatten()(block5)\n",
    "    # block5 = Dense(32, activation='relu')(block5)\n",
    "    # block5 = Dense(8, activation='relu')(block5)\n",
    "    # block5 = Dense(2, activation=lastActivation)(block5)        #### DECISIONE 5 (A o B)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # massFusion = Concatenate(axis=1)([block1,block2,block3,block4,block5])\n",
    "\n",
    "    # # massFusion = Concatenate()([block1,block2,block3,block4,block5])\n",
    "    # # massFusion = Add()([block1,block2,block3,block4,block5])\n",
    "    # # massFusion = layers.Average()([block1,block2,block3,block4,block5])\n",
    "    # # massFusion = Concatenate(axis=1)([block1,block2,block3,block4,block5])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # print(\"masses fused layer :\"+ str(massFusion.shape))\n",
    "    # massFusion = Reshape((massFusion.shape[1]+int(massFusion.shape[1]/(massFusion.shape[2]-1)), massFusion.shape[2]-1))(massFusion)\n",
    "    # print(\"masses fused layer :\"+ str(massFusion.shape))\n",
    "    # # print(massFusion)\n",
    "\n",
    "\n",
    "    # massFusion = BatchNormalization()(massFusion)\n",
    "\n",
    "\n",
    "    # # OUTPUT WITH DS-ASI\n",
    "    # print(\"Fusione finale\")\n",
    "    # # CALCOLO DELLE MASSE A PARTIRE DALLA LISTA DI MASSE DI OGNI LIVELLO\n",
    "    # print(massFusion.shape)\n",
    "    # weights_x = ds_layer.L1_wadd(massFusion.shape[1], 2)(massFusion)\n",
    "    # # weights_x = ds_layer.L1_wadd(massFusion.shape[1], 4)(massFusion)\n",
    "    # weights_x = LeakyReLU()(weights_x)\n",
    "    # print(weights_x.shape)\n",
    "    # distances = ds_layer.L1_wadd_activate(weights_x.shape[1],weights_x.shape[2])(weights_x)\n",
    "    # distances = LeakyReLU()(distances)\n",
    "    # # print(distances.shape)\n",
    "    # massesCalc = ds_layer.L2_masses()(distances)\n",
    "    # # massesCalc = ds_layer.L2_masses()(weights_x)\n",
    "    # print(massesCalc.shape)\n",
    "    # finalMasses = ds_layer.L3_combine_masses()(massesCalc)\n",
    "    # print(finalMasses.shape)\n",
    "\n",
    "\n",
    "    # # #Utility layer for testing\n",
    "    # outputs = utility_layer_train.DM_pignistic(2)(finalMasses)\n",
    "    # print(outputs.shape)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outProvv)\n",
    "    # model = Model(inputs=inputs, outputs=[outputs])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 224, 224, 3)\n",
      "(None, 112, 112, 16)\n",
      "(None, 56, 56, 32)\n",
      "(None, 56, 56, 32)\n",
      "(None, 56, 56, 32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 2)\n",
      "(None, 2)\n",
      "<keras.engine.functional.Functional object at 0x000001F1E82ED2E0>\n"
     ]
    }
   ],
   "source": [
    "print(DSfusionDecisionsML())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD MEAN TO COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the mean value of the column\n",
    "def meanOfCol(db,col_names):\n",
    "    for col in col_names:\n",
    "        n = 0\n",
    "        coln = pd.DataFrame(db[col])\n",
    "        s = coln.sum().sum()\n",
    "        n = len(coln)-coln.isnull().sum().sum()\n",
    "        m = s/n\n",
    "        db[col] = db[col].fillna(m)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michele\\AppData\\Local\\Temp\\ipykernel_11316\\4033450140.py:9: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: (992, 2)\n",
      "validation: (111, 2)\n",
      "TRAIN\n",
      "MILD: 481.0\n",
      "SEVERE: 511.0\n",
      "total errors: 992.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 992/992 [00:58<00:00, 17.02it/s]\n",
      "100%|██████████| 111/111 [00:06<00:00, 17.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(992, 224, 224, 3)\n",
      "(111, 224, 224, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f1fc338760>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nameOfExperiment = \"NuovoTest_Not_Kfold\"\n",
    "checkpointPath = Path('../../Outputs/COVID/LR/'+nameOfExperiment+'/CHECKPOINTS/')\n",
    "graphPath = Path('../../Outputs/COVID/LR/'+nameOfExperiment+'/GRAPHS/')\n",
    "pathSavedModel = Path('../../Outputs/COVID/LR/'+nameOfExperiment+'/MODELS-PB/')\n",
    "plotpath = Path('../../Outputs/COVID/LR/'+nameOfExperiment+'/PLTS/')\n",
    "evalspath = Path('../../Outputs/COVID/LR/'+nameOfExperiment+'/EVALUATIONS/')\n",
    "\n",
    "checkpointPath.mkdir(parents=True, exist_ok=True)\n",
    "graphPath.mkdir(parents=True, exist_ok=True)\n",
    "pathSavedModel.mkdir(parents=True, exist_ok=True)\n",
    "plotpath.mkdir(parents=True, exist_ok=True)\n",
    "evalspath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "\n",
    "foldToCalculate = 0\n",
    "trainFold = []\n",
    "valFold = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X_full)):\n",
    "    if i == foldToCalculate:\n",
    "        trainFold = train_index\n",
    "        valFold = val_index\n",
    "\n",
    "\n",
    "X_train = X_full.iloc[trainFold]\n",
    "X_val = X_full.iloc[valFold]\n",
    "Y_train = Y_full.iloc[trainFold]\n",
    "Y_val = Y_full.iloc[valFold]\n",
    "\n",
    "\n",
    "# search all feature that are numeric\n",
    "nameColsNumber = list(X_full.select_dtypes(exclude=['string','object','bool','datetime']).columns)\n",
    "\n",
    "# Generates the mean of the column and change NAN values with the mean of the column\n",
    "X_train = meanOfCol(X_train,nameColsNumber)\n",
    "X_val = meanOfCol(X_val,nameColsNumber)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print( \"training: \"+str(Y_train.shape))\n",
    "print( \"validation: \"+str(Y_val.shape))\n",
    "# NO CROP\n",
    "print('TRAIN')\n",
    "class1 = Y_train.sum()[0]\n",
    "print('MILD: ' + str(class1))\n",
    "class2 = Y_train.sum()[1]\n",
    "print('SEVERE: ' + str(class2))\n",
    "total = Y_train.sum().sum()\n",
    "print('total errors: ' + str(total))\n",
    "\n",
    "\n",
    "# Y_tr_unc = np.zeros((Y_train.shape[0],2,5)) \n",
    "# print('uncertanty shape training: ' + str(Y_tr_unc.shape))\n",
    "# Y_val_unc = np.zeros((Y_val.shape[0],2,5)) \n",
    "# print('uncertanty shape validation: ' + str(Y_val_unc.shape))\n",
    "\n",
    "\n",
    "# X_train_IMGS = loadPNGtoArrayNoDicom(X_train['ImageFile'][:10],basePath=databasePathIMAGES)\n",
    "# X_val_IMGS = loadPNGtoArrayNoDicom(X_val['ImageFile'][:3],basePath=databasePathIMAGES)\n",
    "# Y_train = Y_train[:10]\n",
    "# Y_val = Y_val[:3]\n",
    "\n",
    "\n",
    "X_train_IMGS = loadPNGtoArrayNoDicom(X_train['ImageFile'],basePath=databasePathIMAGES)\n",
    "X_val_IMGS = loadPNGtoArrayNoDicom(X_val['ImageFile'],basePath=databasePathIMAGES)\n",
    "# Y_train = Y_train\n",
    "# Y_val = Y_val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# X_train_IMGS = loadPNGtoArrayNoDicom(X_train['ImageFile'],basePath=databasePathIMAGES)\n",
    "# X_val_IMGS = loadPNGtoArrayNoDicom(X_val['ImageFile'],basePath=databasePathIMAGES)\n",
    "print(X_train_IMGS.shape)\n",
    "print(X_val_IMGS.shape)\n",
    "plt.figure()\n",
    "plt.imshow(X_val_IMGS[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageFile</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Positivity at admission</th>\n",
       "      <th>Temp_C</th>\n",
       "      <th>DaysFever</th>\n",
       "      <th>Cough</th>\n",
       "      <th>DifficultyInBreathing</th>\n",
       "      <th>Therapy_anti-inflammatory</th>\n",
       "      <th>Therapy_Tocilizumab</th>\n",
       "      <th>...</th>\n",
       "      <th>Chronic Kidney disease</th>\n",
       "      <th>RespiratoryFailure</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>Position</th>\n",
       "      <th>Hospital-A</th>\n",
       "      <th>Hospital-B</th>\n",
       "      <th>Hospital-C</th>\n",
       "      <th>Hospital-D</th>\n",
       "      <th>Hospital-E</th>\n",
       "      <th>Hospital-F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a987bf874b9bf185714a47a3ca526cde01909f182db0a8...</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d0b85929c67c0cc4c9ec1b575fd38528fc54866ef40c66...</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32015df154187a5a5544f8f863eacbf61fbfe12d130dd8...</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f4fbdd06f358873f3c5a2bf3c3144c6f406e46ba8f6dbc...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8918c72266726a30ce36d65bc774b0e188a2dc48584af0...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           ImageFile   Age  Sex  \\\n",
       "0  a987bf874b9bf185714a47a3ca526cde01909f182db0a8...  46.0    1   \n",
       "1  d0b85929c67c0cc4c9ec1b575fd38528fc54866ef40c66...  69.0    0   \n",
       "2  32015df154187a5a5544f8f863eacbf61fbfe12d130dd8...  47.0    1   \n",
       "3  f4fbdd06f358873f3c5a2bf3c3144c6f406e46ba8f6dbc...  68.0    1   \n",
       "4  8918c72266726a30ce36d65bc774b0e188a2dc48584af0...  70.0    1   \n",
       "\n",
       "   Positivity at admission  Temp_C  DaysFever  Cough  DifficultyInBreathing  \\\n",
       "0                      1.0     NaN        3.0    1.0                    1.0   \n",
       "1                      1.0    37.7        NaN    0.0                    0.0   \n",
       "2                      1.0     NaN        2.0    1.0                    0.0   \n",
       "3                      1.0    39.8        1.0    1.0                    0.0   \n",
       "4                      1.0    38.5        4.0    0.0                    0.0   \n",
       "\n",
       "   Therapy_anti-inflammatory   Therapy_Tocilizumab  ...  \\\n",
       "0                         0.0                  0.0  ...   \n",
       "1                         0.0                  0.0  ...   \n",
       "2                         0.0                  0.0  ...   \n",
       "3                         0.0                  0.0  ...   \n",
       "4                         1.0                  0.0  ...   \n",
       "\n",
       "   Chronic Kidney disease  RespiratoryFailure  Obesity  Position  Hospital-A  \\\n",
       "0                     0.0                 0.0      1.0       1.0         0.0   \n",
       "1                     0.0                 1.0      NaN       1.0         0.0   \n",
       "2                     0.0                 0.0      NaN       1.0         0.0   \n",
       "3                     0.0                 0.0      NaN       1.0         0.0   \n",
       "4                     0.0                 0.0      NaN       1.0         0.0   \n",
       "\n",
       "   Hospital-B  Hospital-C  Hospital-D  Hospital-E  Hospital-F  \n",
       "0         0.0         0.0         0.0         0.0         1.0  \n",
       "1         0.0         0.0         0.0         0.0         1.0  \n",
       "2         0.0         0.0         0.0         0.0         1.0  \n",
       "3         0.0         0.0         0.0         0.0         1.0  \n",
       "4         0.0         0.0         0.0         0.0         1.0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageFile</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Positivity at admission</th>\n",
       "      <th>Temp_C</th>\n",
       "      <th>DaysFever</th>\n",
       "      <th>Cough</th>\n",
       "      <th>DifficultyInBreathing</th>\n",
       "      <th>Therapy_anti-inflammatory</th>\n",
       "      <th>Therapy_Tocilizumab</th>\n",
       "      <th>...</th>\n",
       "      <th>Chronic Kidney disease</th>\n",
       "      <th>RespiratoryFailure</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>Position</th>\n",
       "      <th>Hospital-A</th>\n",
       "      <th>Hospital-B</th>\n",
       "      <th>Hospital-C</th>\n",
       "      <th>Hospital-D</th>\n",
       "      <th>Hospital-E</th>\n",
       "      <th>Hospital-F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a987bf874b9bf185714a47a3ca526cde01909f182db0a8...</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.560895</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d0b85929c67c0cc4c9ec1b575fd38528fc54866ef40c66...</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.700000</td>\n",
       "      <td>2.726452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.106339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32015df154187a5a5544f8f863eacbf61fbfe12d130dd8...</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.560895</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f4fbdd06f358873f3c5a2bf3c3144c6f406e46ba8f6dbc...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8918c72266726a30ce36d65bc774b0e188a2dc48584af0...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           ImageFile   Age  Sex  \\\n",
       "0  a987bf874b9bf185714a47a3ca526cde01909f182db0a8...  46.0    1   \n",
       "1  d0b85929c67c0cc4c9ec1b575fd38528fc54866ef40c66...  69.0    0   \n",
       "2  32015df154187a5a5544f8f863eacbf61fbfe12d130dd8...  47.0    1   \n",
       "3  f4fbdd06f358873f3c5a2bf3c3144c6f406e46ba8f6dbc...  68.0    1   \n",
       "4  8918c72266726a30ce36d65bc774b0e188a2dc48584af0...  70.0    1   \n",
       "\n",
       "   Positivity at admission     Temp_C  DaysFever  Cough  \\\n",
       "0                      1.0  37.560895   3.000000    1.0   \n",
       "1                      1.0  37.700000   2.726452    0.0   \n",
       "2                      1.0  37.560895   2.000000    1.0   \n",
       "3                      1.0  39.800000   1.000000    1.0   \n",
       "4                      1.0  38.500000   4.000000    0.0   \n",
       "\n",
       "   DifficultyInBreathing  Therapy_anti-inflammatory   Therapy_Tocilizumab  \\\n",
       "0                    1.0                         0.0                  0.0   \n",
       "1                    0.0                         0.0                  0.0   \n",
       "2                    0.0                         0.0                  0.0   \n",
       "3                    0.0                         0.0                  0.0   \n",
       "4                    0.0                         1.0                  0.0   \n",
       "\n",
       "   ...  Chronic Kidney disease  RespiratoryFailure   Obesity  Position  \\\n",
       "0  ...                     0.0                 0.0  1.000000       1.0   \n",
       "1  ...                     0.0                 1.0  0.106339       1.0   \n",
       "2  ...                     0.0                 0.0  0.106339       1.0   \n",
       "3  ...                     0.0                 0.0  0.106339       1.0   \n",
       "4  ...                     0.0                 0.0  0.106339       1.0   \n",
       "\n",
       "   Hospital-A  Hospital-B  Hospital-C  Hospital-D  Hospital-E  Hospital-F  \n",
       "0         0.0         0.0         0.0         0.0         0.0         1.0  \n",
       "1         0.0         0.0         0.0         0.0         0.0         1.0  \n",
       "2         0.0         0.0         0.0         0.0         0.0         1.0  \n",
       "3         0.0         0.0         0.0         0.0         0.0         1.0  \n",
       "4         0.0         0.0         0.0         0.0         0.0         1.0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop('ImageFile', axis='columns')\n",
    "X_val = X_val.drop('ImageFile', axis='columns')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns,index=X_train.index)\n",
    "X_val = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns,index=X_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Positivity at admission</th>\n",
       "      <th>Temp_C</th>\n",
       "      <th>DaysFever</th>\n",
       "      <th>Cough</th>\n",
       "      <th>DifficultyInBreathing</th>\n",
       "      <th>Therapy_anti-inflammatory</th>\n",
       "      <th>Therapy_Tocilizumab</th>\n",
       "      <th>Therapy_Anakinra</th>\n",
       "      <th>...</th>\n",
       "      <th>Chronic Kidney disease</th>\n",
       "      <th>RespiratoryFailure</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>Position</th>\n",
       "      <th>Hospital-A</th>\n",
       "      <th>Hospital-B</th>\n",
       "      <th>Hospital-C</th>\n",
       "      <th>Hospital-D</th>\n",
       "      <th>Hospital-E</th>\n",
       "      <th>Hospital-F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.345679</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.354245</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.377049</td>\n",
       "      <td>0.681613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.106339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.358025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.354245</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.617284</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.641975</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age  Sex  Positivity at admission    Temp_C  DaysFever  Cough  \\\n",
       "0  0.345679  1.0                      1.0  0.354245   0.750000    1.0   \n",
       "1  0.629630  0.0                      1.0  0.377049   0.681613    0.0   \n",
       "2  0.358025  1.0                      1.0  0.354245   0.500000    1.0   \n",
       "3  0.617284  1.0                      1.0  0.721311   0.250000    1.0   \n",
       "4  0.641975  1.0                      1.0  0.508197   1.000000    0.0   \n",
       "\n",
       "   DifficultyInBreathing  Therapy_anti-inflammatory   Therapy_Tocilizumab  \\\n",
       "0                    1.0                         0.0                  0.0   \n",
       "1                    0.0                         0.0                  0.0   \n",
       "2                    0.0                         0.0                  0.0   \n",
       "3                    0.0                         0.0                  0.0   \n",
       "4                    0.0                         1.0                  0.0   \n",
       "\n",
       "   Therapy_Anakinra  ...  Chronic Kidney disease  RespiratoryFailure  \\\n",
       "0               0.0  ...                     0.0                 0.0   \n",
       "1               0.0  ...                     0.0                 1.0   \n",
       "2               0.0  ...                     0.0                 0.0   \n",
       "3               0.0  ...                     0.0                 0.0   \n",
       "4               0.0  ...                     0.0                 0.0   \n",
       "\n",
       "    Obesity  Position  Hospital-A  Hospital-B  Hospital-C  Hospital-D  \\\n",
       "0  1.000000       1.0         0.0         0.0         0.0         0.0   \n",
       "1  0.106339       1.0         0.0         0.0         0.0         0.0   \n",
       "2  0.106339       1.0         0.0         0.0         0.0         0.0   \n",
       "3  0.106339       1.0         0.0         0.0         0.0         0.0   \n",
       "4  0.106339       1.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   Hospital-E  Hospital-F  \n",
       "0         0.0         1.0  \n",
       "1         0.0         1.0  \n",
       "2         0.0         1.0  \n",
       "3         0.0         1.0  \n",
       "4         0.0         1.0  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDESTRAMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# trainset = tf.data.Dataset.from_tensor_slices((X_train_IMGS, [Y_train,Y_tr_unc]))\n",
    "# trainset = trainset.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "# trainset = trainset.batch(batch_size)\n",
    "# trainset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "# validset = tf.data.Dataset.from_tensor_slices((X_val_IMGS, [Y_val,Y_val_unc]))\n",
    "# validset = validset.batch(batch_size)\n",
    "# validset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lrates = []\n",
    "\n",
    "# def step_decay(epoch):\n",
    "#     return lrate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trainset)\n",
    "# print(np.ceil(X_train_IMGS.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeModelLayers(model, addname):\n",
    "    for layer in model.layers:\n",
    "        layer._name = layer.name + str(addname)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### addestramento multilayer fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 224, 224, 3)\n",
      "(None, 112, 112, 16)\n",
      "(None, 56, 56, 32)\n",
      "(None, 56, 56, 32)\n",
      "(None, 56, 56, 32)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 2)\n",
      "(None, 2)\n",
      "..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\resnet_Classification_acc\n",
      "Epoch 1/500\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.4788 - categorical_accuracy: 0.5025 - f1_score: 0.3960(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\ML_DF_U_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\ML_DF_U_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\ML_DF_U_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\ML_DF_U_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 15s 345ms/step - loss: 1.4788 - categorical_accuracy: 0.5025 - f1_score: 0.3960 - val_loss: 1.4700 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 2/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.2352 - categorical_accuracy: 0.5005 - f1_score: 0.3371 - val_loss: 1.3932 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 3/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.1784 - categorical_accuracy: 0.4955 - f1_score: 0.4370 - val_loss: 1.2675 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 4/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1386 - categorical_accuracy: 0.5307 - f1_score: 0.5248 - val_loss: 1.1502 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 5/500\n",
      "31/32 [============================>.] - ETA: 0s - loss: 1.1501 - categorical_accuracy: 0.5010 - f1_score: 0.4992(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\ML_DF_U_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\ML_DF_U_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\ML_DF_U_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\ML_DF_U_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 10s 324ms/step - loss: 1.1502 - categorical_accuracy: 0.5005 - f1_score: 0.4987 - val_loss: 1.1317 - val_categorical_accuracy: 0.6273 - val_f1_score: 0.4273\n",
      "Epoch 6/500\n",
      "31/32 [============================>.] - ETA: 0s - loss: 1.1457 - categorical_accuracy: 0.5161 - f1_score: 0.4876(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\ML_DF_U_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\ML_DF_U_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 5s 166ms/step - loss: 1.1457 - categorical_accuracy: 0.5156 - f1_score: 0.4871 - val_loss: 1.0939 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 7/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.1534 - categorical_accuracy: 0.5096 - f1_score: 0.4717 - val_loss: 1.0956 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 8/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.1296 - categorical_accuracy: 0.5478 - f1_score: 0.5465 - val_loss: 1.1037 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 9/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.1337 - categorical_accuracy: 0.5378 - f1_score: 0.5318 - val_loss: 1.3909 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 10/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1282 - categorical_accuracy: 0.5519 - f1_score: 0.5341 - val_loss: 1.2548 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 11/500\n",
      "31/32 [============================>.] - ETA: 0s - loss: 1.1220 - categorical_accuracy: 0.5554 - f1_score: 0.5485(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\ML_DF_U_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\ML_DF_U_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 5s 165ms/step - loss: 1.1223 - categorical_accuracy: 0.5549 - f1_score: 0.5479 - val_loss: 4.4458 - val_categorical_accuracy: 0.4818 - val_f1_score: 0.4797\n",
      "Epoch 12/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.1521 - categorical_accuracy: 0.5327 - f1_score: 0.5266 - val_loss: 1.5280 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 13/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 1.1166 - categorical_accuracy: 0.5529 - f1_score: 0.5527 - val_loss: 1.4056 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 14/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1217 - categorical_accuracy: 0.5257 - f1_score: 0.4896 - val_loss: 6.1428 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 15/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1184 - categorical_accuracy: 0.5277 - f1_score: 0.5105 - val_loss: 5.9024 - val_categorical_accuracy: 0.3909 - val_f1_score: 0.3384\n",
      "Epoch 16/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1284 - categorical_accuracy: 0.5458 - f1_score: 0.5458 - val_loss: 6.1391 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 17/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1231 - categorical_accuracy: 0.5660 - f1_score: 0.5620 - val_loss: 6.1372 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 18/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1123 - categorical_accuracy: 0.5519 - f1_score: 0.5420 - val_loss: 1.1214 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 19/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.1449 - categorical_accuracy: 0.5126 - f1_score: 0.3680 - val_loss: 6.0105 - val_categorical_accuracy: 0.3727 - val_f1_score: 0.3021\n",
      "Epoch 20/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0979 - categorical_accuracy: 0.5609 - f1_score: 0.5567 - val_loss: 6.1345 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 21/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.1368 - categorical_accuracy: 0.5106 - f1_score: 0.4271 - val_loss: 1.0935 - val_categorical_accuracy: 0.6364 - val_f1_score: 0.3889\n",
      "Epoch 22/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.1822 - categorical_accuracy: 0.5196 - f1_score: 0.4707 - val_loss: 1.0719 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 23/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1025 - categorical_accuracy: 0.5358 - f1_score: 0.5342 - val_loss: 1.0805 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 24/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.1066 - categorical_accuracy: 0.5368 - f1_score: 0.5346 - val_loss: 1.0909 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 25/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.1032 - categorical_accuracy: 0.5418 - f1_score: 0.5139 - val_loss: 1.1243 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 26/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0818 - categorical_accuracy: 0.5791 - f1_score: 0.5761 - val_loss: 3.6244 - val_categorical_accuracy: 0.4636 - val_f1_score: 0.4505\n",
      "Epoch 27/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0944 - categorical_accuracy: 0.5569 - f1_score: 0.5495 - val_loss: 5.9029 - val_categorical_accuracy: 0.4091 - val_f1_score: 0.3650\n",
      "Epoch 28/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0842 - categorical_accuracy: 0.5851 - f1_score: 0.5777 - val_loss: 5.9921 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2873\n",
      "Epoch 29/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1812 - categorical_accuracy: 0.5156 - f1_score: 0.4294 - val_loss: 2.3194 - val_categorical_accuracy: 0.5273 - val_f1_score: 0.4773\n",
      "Epoch 30/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.2475 - categorical_accuracy: 0.4975 - f1_score: 0.3322 - val_loss: 1.4854 - val_categorical_accuracy: 0.3727 - val_f1_score: 0.2927\n",
      "Epoch 31/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.2248 - categorical_accuracy: 0.4945 - f1_score: 0.3378 - val_loss: 1.3494 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 32/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.2426 - categorical_accuracy: 0.4894 - f1_score: 0.3530 - val_loss: 1.2872 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 33/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.2344 - categorical_accuracy: 0.4935 - f1_score: 0.3881 - val_loss: 1.2408 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 34/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.1950 - categorical_accuracy: 0.4924 - f1_score: 0.3838 - val_loss: 1.1957 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 35/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1359 - categorical_accuracy: 0.5096 - f1_score: 0.3785 - val_loss: 1.1556 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 36/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1273 - categorical_accuracy: 0.5176 - f1_score: 0.4753 - val_loss: 1.1332 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 37/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1372 - categorical_accuracy: 0.4743 - f1_score: 0.4211 - val_loss: 1.1183 - val_categorical_accuracy: 0.5636 - val_f1_score: 0.4381\n",
      "Epoch 38/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1234 - categorical_accuracy: 0.5076 - f1_score: 0.4754 - val_loss: 1.1083 - val_categorical_accuracy: 0.6273 - val_f1_score: 0.4074\n",
      "Epoch 39/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1409 - categorical_accuracy: 0.5257 - f1_score: 0.5234 - val_loss: 1.0954 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 40/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1484 - categorical_accuracy: 0.5196 - f1_score: 0.5176 - val_loss: 1.0968 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.4155\n",
      "Epoch 41/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1267 - categorical_accuracy: 0.5106 - f1_score: 0.4699 - val_loss: 1.0903 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 42/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1163 - categorical_accuracy: 0.5388 - f1_score: 0.5387 - val_loss: 1.0820 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 43/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1113 - categorical_accuracy: 0.5398 - f1_score: 0.5395 - val_loss: 1.0810 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 44/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1181 - categorical_accuracy: 0.5106 - f1_score: 0.4860 - val_loss: 1.0852 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 45/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1219 - categorical_accuracy: 0.5478 - f1_score: 0.5478 - val_loss: 1.0762 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 46/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1435 - categorical_accuracy: 0.5468 - f1_score: 0.5464 - val_loss: 1.0796 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 47/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1268 - categorical_accuracy: 0.5509 - f1_score: 0.5487 - val_loss: 1.0721 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 48/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.1221 - categorical_accuracy: 0.5378 - f1_score: 0.5377 - val_loss: 1.0677 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 49/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.1120 - categorical_accuracy: 0.5418 - f1_score: 0.5367 - val_loss: 1.0662 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 50/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1090 - categorical_accuracy: 0.5589 - f1_score: 0.5582 - val_loss: 1.0660 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 51/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0988 - categorical_accuracy: 0.5629 - f1_score: 0.5523 - val_loss: 1.0664 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 52/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1016 - categorical_accuracy: 0.5287 - f1_score: 0.4992 - val_loss: 1.0646 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 53/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0961 - categorical_accuracy: 0.5488 - f1_score: 0.5262 - val_loss: 1.0630 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 54/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0953 - categorical_accuracy: 0.5559 - f1_score: 0.5493 - val_loss: 1.0659 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 55/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1006 - categorical_accuracy: 0.5297 - f1_score: 0.5212 - val_loss: 1.0663 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 56/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0911 - categorical_accuracy: 0.5539 - f1_score: 0.5394 - val_loss: 1.0668 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 57/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0910 - categorical_accuracy: 0.5559 - f1_score: 0.5534 - val_loss: 1.0860 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 58/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0824 - categorical_accuracy: 0.5730 - f1_score: 0.5661 - val_loss: 1.0937 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 59/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0836 - categorical_accuracy: 0.5458 - f1_score: 0.5458 - val_loss: 1.1824 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 60/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1032 - categorical_accuracy: 0.5629 - f1_score: 0.5629 - val_loss: 1.1395 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 61/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0814 - categorical_accuracy: 0.5710 - f1_score: 0.5654 - val_loss: 1.2908 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.3923\n",
      "Epoch 62/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0977 - categorical_accuracy: 0.5660 - f1_score: 0.5634 - val_loss: 1.6761 - val_categorical_accuracy: 0.6182 - val_f1_score: 0.4034\n",
      "Epoch 63/500\n",
      "31/32 [============================>.] - ETA: 0s - loss: 1.0762 - categorical_accuracy: 0.5837 - f1_score: 0.5778(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\ML_DF_U_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\ML_DF_U_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 5s 167ms/step - loss: 1.0762 - categorical_accuracy: 0.5831 - f1_score: 0.5772 - val_loss: 3.7962 - val_categorical_accuracy: 0.5727 - val_f1_score: 0.5453\n",
      "Epoch 64/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0794 - categorical_accuracy: 0.5811 - f1_score: 0.5750 - val_loss: 5.8919 - val_categorical_accuracy: 0.3727 - val_f1_score: 0.3107\n",
      "Epoch 65/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0672 - categorical_accuracy: 0.6073 - f1_score: 0.6065 - val_loss: 5.9855 - val_categorical_accuracy: 0.3727 - val_f1_score: 0.3021\n",
      "Epoch 66/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0732 - categorical_accuracy: 0.5891 - f1_score: 0.5812 - val_loss: 5.8908 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2964\n",
      "Epoch 67/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0694 - categorical_accuracy: 0.5861 - f1_score: 0.5844 - val_loss: 5.9859 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 68/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0835 - categorical_accuracy: 0.5690 - f1_score: 0.5687 - val_loss: 5.9897 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 69/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0637 - categorical_accuracy: 0.5871 - f1_score: 0.5870 - val_loss: 5.9881 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 70/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0959 - categorical_accuracy: 0.5972 - f1_score: 0.5965 - val_loss: 5.9852 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 71/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0530 - categorical_accuracy: 0.6032 - f1_score: 0.6014 - val_loss: 5.9944 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 72/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0639 - categorical_accuracy: 0.5962 - f1_score: 0.5962 - val_loss: 5.9965 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 73/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0660 - categorical_accuracy: 0.5650 - f1_score: 0.5510 - val_loss: 5.9854 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 74/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0551 - categorical_accuracy: 0.6083 - f1_score: 0.6012 - val_loss: 6.1062 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 75/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0543 - categorical_accuracy: 0.5992 - f1_score: 0.5962 - val_loss: 6.1052 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 76/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0538 - categorical_accuracy: 0.6042 - f1_score: 0.6031 - val_loss: 6.0077 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 77/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0478 - categorical_accuracy: 0.6143 - f1_score: 0.6134 - val_loss: 6.1035 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 78/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0504 - categorical_accuracy: 0.5932 - f1_score: 0.5884 - val_loss: 6.1026 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 79/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0501 - categorical_accuracy: 0.5972 - f1_score: 0.5945 - val_loss: 6.1017 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 80/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0817 - categorical_accuracy: 0.5881 - f1_score: 0.5835 - val_loss: 5.9872 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 81/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0787 - categorical_accuracy: 0.5962 - f1_score: 0.5948 - val_loss: 5.9924 - val_categorical_accuracy: 0.3455 - val_f1_score: 0.2568\n",
      "Epoch 82/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0600 - categorical_accuracy: 0.6042 - f1_score: 0.5988 - val_loss: 6.0992 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 83/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0612 - categorical_accuracy: 0.5992 - f1_score: 0.5976 - val_loss: 5.9711 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 84/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0802 - categorical_accuracy: 0.5962 - f1_score: 0.5875 - val_loss: 5.9732 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 85/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0364 - categorical_accuracy: 0.6083 - f1_score: 0.6063 - val_loss: 5.9867 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 86/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0741 - categorical_accuracy: 0.5962 - f1_score: 0.5925 - val_loss: 6.0962 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 87/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0381 - categorical_accuracy: 0.6415 - f1_score: 0.6374 - val_loss: 6.0952 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 88/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0304 - categorical_accuracy: 0.6002 - f1_score: 0.5977 - val_loss: 6.0944 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 89/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0386 - categorical_accuracy: 0.6062 - f1_score: 0.5994 - val_loss: 6.0937 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 90/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0345 - categorical_accuracy: 0.5972 - f1_score: 0.5935 - val_loss: 6.0928 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 91/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0365 - categorical_accuracy: 0.6032 - f1_score: 0.5917 - val_loss: 6.0921 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 92/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0421 - categorical_accuracy: 0.6173 - f1_score: 0.6099 - val_loss: 6.0912 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 93/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0293 - categorical_accuracy: 0.6073 - f1_score: 0.6039 - val_loss: 6.0905 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 94/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0590 - categorical_accuracy: 0.6012 - f1_score: 0.5967 - val_loss: 6.0897 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 95/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0433 - categorical_accuracy: 0.6274 - f1_score: 0.6218 - val_loss: 5.9834 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 96/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0399 - categorical_accuracy: 0.6213 - f1_score: 0.6174 - val_loss: 6.0882 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 97/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0716 - categorical_accuracy: 0.6203 - f1_score: 0.6183 - val_loss: 5.9891 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 98/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0429 - categorical_accuracy: 0.5972 - f1_score: 0.5894 - val_loss: 5.9643 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 99/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0291 - categorical_accuracy: 0.5871 - f1_score: 0.5776 - val_loss: 5.9712 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 100/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0259 - categorical_accuracy: 0.5992 - f1_score: 0.5937 - val_loss: 6.0855 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 101/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0337 - categorical_accuracy: 0.6143 - f1_score: 0.6123 - val_loss: 6.0847 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 102/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 1.0106 - categorical_accuracy: 0.6093 - f1_score: 0.6070 - val_loss: 6.0839 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 103/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.0348 - categorical_accuracy: 0.6032 - f1_score: 0.5976 - val_loss: 6.0832 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 104/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0248 - categorical_accuracy: 0.6203 - f1_score: 0.6149 - val_loss: 6.0825 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 105/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.0240 - categorical_accuracy: 0.6213 - f1_score: 0.6160 - val_loss: 6.0818 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2617\n",
      "Epoch 106/500\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 1.0080 - categorical_accuracy: 0.6173 - f1_score: 0.6155 - val_loss: 6.0812 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 107/500\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1.0267 - categorical_accuracy: 0.6093 - f1_score: 0.6055 - val_loss: 6.0809 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 108/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0169 - categorical_accuracy: 0.5982 - f1_score: 0.5934 - val_loss: 5.9581 - val_categorical_accuracy: 0.3727 - val_f1_score: 0.3021\n",
      "Epoch 109/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0114 - categorical_accuracy: 0.6193 - f1_score: 0.6143 - val_loss: 5.9703 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 110/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.0285 - categorical_accuracy: 0.5881 - f1_score: 0.5782 - val_loss: 1.3850 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.4363\n",
      "Epoch 111/500\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 1.0580 - categorical_accuracy: 0.5347 - f1_score: 0.4397 - val_loss: 1.0463 - val_categorical_accuracy: 0.6273 - val_f1_score: 0.4273\n",
      "Epoch 112/500\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 1.0531 - categorical_accuracy: 0.5398 - f1_score: 0.4637 - val_loss: 1.0392 - val_categorical_accuracy: 0.6273 - val_f1_score: 0.4273\n",
      "Epoch 113/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.0623 - categorical_accuracy: 0.5539 - f1_score: 0.5022 - val_loss: 1.0376 - val_categorical_accuracy: 0.6455 - val_f1_score: 0.4363\n",
      "Epoch 114/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.0582 - categorical_accuracy: 0.5710 - f1_score: 0.5432 - val_loss: 1.0432 - val_categorical_accuracy: 0.6273 - val_f1_score: 0.3855\n",
      "Epoch 115/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0356 - categorical_accuracy: 0.5730 - f1_score: 0.5486 - val_loss: 1.0596 - val_categorical_accuracy: 0.6273 - val_f1_score: 0.3855\n",
      "Epoch 116/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.0268 - categorical_accuracy: 0.6022 - f1_score: 0.5874 - val_loss: 1.1275 - val_categorical_accuracy: 0.6364 - val_f1_score: 0.3889\n",
      "Epoch 117/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.0265 - categorical_accuracy: 0.5911 - f1_score: 0.5837 - val_loss: 1.2612 - val_categorical_accuracy: 0.6273 - val_f1_score: 0.3855\n",
      "Epoch 118/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.0234 - categorical_accuracy: 0.5750 - f1_score: 0.5675 - val_loss: 1.8662 - val_categorical_accuracy: 0.6364 - val_f1_score: 0.4824\n",
      "Epoch 119/500\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 1.0315 - categorical_accuracy: 0.5911 - f1_score: 0.5830 - val_loss: 1.9998 - val_categorical_accuracy: 0.5909 - val_f1_score: 0.4672\n",
      "Epoch 120/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0179 - categorical_accuracy: 0.6173 - f1_score: 0.6144 - val_loss: 2.8393 - val_categorical_accuracy: 0.4909 - val_f1_score: 0.4500\n",
      "Epoch 121/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.0139 - categorical_accuracy: 0.6244 - f1_score: 0.6214 - val_loss: 4.4967 - val_categorical_accuracy: 0.4364 - val_f1_score: 0.4356\n",
      "Epoch 122/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.0220 - categorical_accuracy: 0.6103 - f1_score: 0.6085 - val_loss: 4.7458 - val_categorical_accuracy: 0.4364 - val_f1_score: 0.4334\n",
      "Epoch 123/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0094 - categorical_accuracy: 0.6234 - f1_score: 0.6210 - val_loss: 5.1712 - val_categorical_accuracy: 0.4000 - val_f1_score: 0.3835\n",
      "Epoch 124/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.0174 - categorical_accuracy: 0.6083 - f1_score: 0.6044 - val_loss: 5.4741 - val_categorical_accuracy: 0.4000 - val_f1_score: 0.3700\n",
      "Epoch 125/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0085 - categorical_accuracy: 0.6274 - f1_score: 0.6227 - val_loss: 5.8269 - val_categorical_accuracy: 0.4091 - val_f1_score: 0.3650\n",
      "Epoch 126/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.0124 - categorical_accuracy: 0.6224 - f1_score: 0.6196 - val_loss: 5.9460 - val_categorical_accuracy: 0.3818 - val_f1_score: 0.3165\n",
      "Epoch 127/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0093 - categorical_accuracy: 0.6163 - f1_score: 0.6148 - val_loss: 5.9454 - val_categorical_accuracy: 0.3727 - val_f1_score: 0.3021\n",
      "Epoch 128/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0033 - categorical_accuracy: 0.6153 - f1_score: 0.6147 - val_loss: 5.9456 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2873\n",
      "Epoch 129/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.0122 - categorical_accuracy: 0.6012 - f1_score: 0.6008 - val_loss: 5.9474 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2873\n",
      "Epoch 130/500\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 1.0002 - categorical_accuracy: 0.6203 - f1_score: 0.6196 - val_loss: 5.9481 - val_categorical_accuracy: 0.3727 - val_f1_score: 0.3021\n",
      "Epoch 131/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9942 - categorical_accuracy: 0.6314 - f1_score: 0.6301 - val_loss: 5.9479 - val_categorical_accuracy: 0.3727 - val_f1_score: 0.3021\n",
      "Epoch 132/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0122 - categorical_accuracy: 0.6294 - f1_score: 0.6277 - val_loss: 5.9480 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2873\n",
      "Epoch 133/500\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.9992 - categorical_accuracy: 0.6093 - f1_score: 0.6079 - val_loss: 5.9504 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 134/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.0072 - categorical_accuracy: 0.6254 - f1_score: 0.6247 - val_loss: 5.9554 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 135/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0194 - categorical_accuracy: 0.6133 - f1_score: 0.6125 - val_loss: 5.9710 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 136/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9951 - categorical_accuracy: 0.6203 - f1_score: 0.6182 - val_loss: 5.9520 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 137/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9961 - categorical_accuracy: 0.5992 - f1_score: 0.5966 - val_loss: 5.9579 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 138/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.0066 - categorical_accuracy: 0.6032 - f1_score: 0.6000 - val_loss: 6.0675 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 139/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9979 - categorical_accuracy: 0.6213 - f1_score: 0.6200 - val_loss: 6.0664 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 140/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0122 - categorical_accuracy: 0.6264 - f1_score: 0.6244 - val_loss: 6.0663 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 141/500\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.9914 - categorical_accuracy: 0.6183 - f1_score: 0.6160 - val_loss: 6.0658 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 142/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9859 - categorical_accuracy: 0.6344 - f1_score: 0.6315 - val_loss: 6.0653 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 143/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9868 - categorical_accuracy: 0.6224 - f1_score: 0.6178 - val_loss: 6.0648 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 144/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.0117 - categorical_accuracy: 0.6153 - f1_score: 0.6119 - val_loss: 6.0643 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 145/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9954 - categorical_accuracy: 0.6062 - f1_score: 0.6031 - val_loss: 6.0639 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 146/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0012 - categorical_accuracy: 0.6113 - f1_score: 0.6081 - val_loss: 6.0636 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 147/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9850 - categorical_accuracy: 0.6304 - f1_score: 0.6279 - val_loss: 6.0638 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 148/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9810 - categorical_accuracy: 0.6284 - f1_score: 0.6257 - val_loss: 6.0631 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 149/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.0262 - categorical_accuracy: 0.6103 - f1_score: 0.6058 - val_loss: 6.0640 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 150/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9850 - categorical_accuracy: 0.6274 - f1_score: 0.6225 - val_loss: 6.0633 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 151/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9961 - categorical_accuracy: 0.6123 - f1_score: 0.6096 - val_loss: 6.0629 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 152/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9900 - categorical_accuracy: 0.6093 - f1_score: 0.6068 - val_loss: 6.0624 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 153/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9801 - categorical_accuracy: 0.6203 - f1_score: 0.6168 - val_loss: 6.0616 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 154/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9950 - categorical_accuracy: 0.6203 - f1_score: 0.6176 - val_loss: 6.0609 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 155/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9946 - categorical_accuracy: 0.6193 - f1_score: 0.6157 - val_loss: 5.9596 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 156/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9783 - categorical_accuracy: 0.6284 - f1_score: 0.6261 - val_loss: 6.0593 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 157/500\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.9875 - categorical_accuracy: 0.6284 - f1_score: 0.6257 - val_loss: 5.9484 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 158/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9982 - categorical_accuracy: 0.6314 - f1_score: 0.6277 - val_loss: 5.9425 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 159/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9896 - categorical_accuracy: 0.6254 - f1_score: 0.6201 - val_loss: 5.9436 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 160/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9859 - categorical_accuracy: 0.6143 - f1_score: 0.6086 - val_loss: 5.9409 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 161/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.0031 - categorical_accuracy: 0.6203 - f1_score: 0.6147 - val_loss: 5.9392 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 162/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9790 - categorical_accuracy: 0.6375 - f1_score: 0.6341 - val_loss: 5.9534 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 163/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9757 - categorical_accuracy: 0.6516 - f1_score: 0.6495 - val_loss: 6.0571 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 164/500\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.9908 - categorical_accuracy: 0.6254 - f1_score: 0.6237 - val_loss: 6.0565 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 165/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0079 - categorical_accuracy: 0.6143 - f1_score: 0.6125 - val_loss: 6.0560 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 166/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0165 - categorical_accuracy: 0.6365 - f1_score: 0.6342 - val_loss: 6.0560 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 167/500\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.9703 - categorical_accuracy: 0.6203 - f1_score: 0.6187 - val_loss: 6.0557 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 168/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9755 - categorical_accuracy: 0.6183 - f1_score: 0.6160 - val_loss: 6.0557 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 169/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9877 - categorical_accuracy: 0.6244 - f1_score: 0.6213 - val_loss: 6.0551 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 170/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9785 - categorical_accuracy: 0.6284 - f1_score: 0.6262 - val_loss: 6.0550 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 171/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9979 - categorical_accuracy: 0.6123 - f1_score: 0.6094 - val_loss: 5.9393 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 172/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9940 - categorical_accuracy: 0.6183 - f1_score: 0.6129 - val_loss: 5.9353 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2873\n",
      "Epoch 173/500\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.9773 - categorical_accuracy: 0.6324 - f1_score: 0.6248 - val_loss: 5.9362 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2873\n",
      "Epoch 174/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9827 - categorical_accuracy: 0.6294 - f1_score: 0.6255 - val_loss: 5.9507 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 175/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9707 - categorical_accuracy: 0.6284 - f1_score: 0.6274 - val_loss: 6.0546 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 176/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9793 - categorical_accuracy: 0.6143 - f1_score: 0.6119 - val_loss: 6.0543 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 177/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9708 - categorical_accuracy: 0.6213 - f1_score: 0.6204 - val_loss: 6.0539 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 178/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0002 - categorical_accuracy: 0.6365 - f1_score: 0.6349 - val_loss: 6.0531 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 179/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9708 - categorical_accuracy: 0.6244 - f1_score: 0.6231 - val_loss: 6.0531 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 180/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0043 - categorical_accuracy: 0.6123 - f1_score: 0.6095 - val_loss: 6.0533 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 181/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9715 - categorical_accuracy: 0.6173 - f1_score: 0.6148 - val_loss: 6.0531 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 182/500\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 0.9839 - categorical_accuracy: 0.6415 - f1_score: 0.6391 - val_loss: 6.0524 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 183/500\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.9740 - categorical_accuracy: 0.6344 - f1_score: 0.6305 - val_loss: 5.9258 - val_categorical_accuracy: 0.3727 - val_f1_score: 0.3021\n",
      "Epoch 184/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9782 - categorical_accuracy: 0.6163 - f1_score: 0.6050 - val_loss: 5.9249 - val_categorical_accuracy: 0.3818 - val_f1_score: 0.3165\n",
      "Epoch 185/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9734 - categorical_accuracy: 0.6395 - f1_score: 0.6295 - val_loss: 5.9247 - val_categorical_accuracy: 0.3727 - val_f1_score: 0.3021\n",
      "Epoch 186/500\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.9674 - categorical_accuracy: 0.6304 - f1_score: 0.6231 - val_loss: 5.9259 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2873\n",
      "Epoch 187/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9724 - categorical_accuracy: 0.6052 - f1_score: 0.5976 - val_loss: 5.9280 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2873\n",
      "Epoch 188/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9640 - categorical_accuracy: 0.6576 - f1_score: 0.6536 - val_loss: 5.9313 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 189/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9866 - categorical_accuracy: 0.6435 - f1_score: 0.6401 - val_loss: 5.9350 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 190/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9720 - categorical_accuracy: 0.6274 - f1_score: 0.6237 - val_loss: 5.9314 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2873\n",
      "Epoch 191/500\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 0.9712 - categorical_accuracy: 0.6455 - f1_score: 0.6409 - val_loss: 5.9346 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2873\n",
      "Epoch 192/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9692 - categorical_accuracy: 0.6475 - f1_score: 0.6424 - val_loss: 5.9324 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2873\n",
      "Epoch 193/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9896 - categorical_accuracy: 0.6274 - f1_score: 0.6239 - val_loss: 5.9365 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 194/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9653 - categorical_accuracy: 0.6244 - f1_score: 0.6190 - val_loss: 5.9335 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2873\n",
      "Epoch 195/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9702 - categorical_accuracy: 0.6274 - f1_score: 0.6237 - val_loss: 5.9408 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2873\n",
      "Epoch 196/500\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.9798 - categorical_accuracy: 0.6334 - f1_score: 0.6303 - val_loss: 6.0496 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 197/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9786 - categorical_accuracy: 0.6334 - f1_score: 0.6313 - val_loss: 6.0492 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 198/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9940 - categorical_accuracy: 0.6183 - f1_score: 0.6164 - val_loss: 6.0489 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 199/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9814 - categorical_accuracy: 0.6234 - f1_score: 0.6215 - val_loss: 6.0486 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 200/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9926 - categorical_accuracy: 0.6294 - f1_score: 0.6277 - val_loss: 6.0485 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 201/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9713 - categorical_accuracy: 0.6274 - f1_score: 0.6244 - val_loss: 5.9340 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2873\n",
      "Epoch 202/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9603 - categorical_accuracy: 0.6274 - f1_score: 0.6253 - val_loss: 5.9365 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 203/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9653 - categorical_accuracy: 0.6304 - f1_score: 0.6271 - val_loss: 5.9483 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 204/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9645 - categorical_accuracy: 0.6354 - f1_score: 0.6330 - val_loss: 6.0481 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 205/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9597 - categorical_accuracy: 0.6314 - f1_score: 0.6292 - val_loss: 6.0477 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 206/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9662 - categorical_accuracy: 0.6314 - f1_score: 0.6287 - val_loss: 6.0478 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 207/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9722 - categorical_accuracy: 0.6244 - f1_score: 0.6199 - val_loss: 6.0477 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 208/500\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.9655 - categorical_accuracy: 0.6344 - f1_score: 0.6313 - val_loss: 6.0476 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 209/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9896 - categorical_accuracy: 0.6203 - f1_score: 0.6168 - val_loss: 6.0474 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 210/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9622 - categorical_accuracy: 0.6344 - f1_score: 0.6317 - val_loss: 6.0469 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 211/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9796 - categorical_accuracy: 0.6435 - f1_score: 0.6405 - val_loss: 6.0467 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 212/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0031 - categorical_accuracy: 0.6455 - f1_score: 0.6437 - val_loss: 6.0463 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 213/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9809 - categorical_accuracy: 0.6284 - f1_score: 0.6256 - val_loss: 6.0459 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 214/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9531 - categorical_accuracy: 0.6234 - f1_score: 0.6197 - val_loss: 6.0456 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 215/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9651 - categorical_accuracy: 0.6254 - f1_score: 0.6237 - val_loss: 6.0456 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 216/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9585 - categorical_accuracy: 0.6354 - f1_score: 0.6328 - val_loss: 6.0456 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 217/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9547 - categorical_accuracy: 0.6385 - f1_score: 0.6361 - val_loss: 6.0453 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 218/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9751 - categorical_accuracy: 0.6254 - f1_score: 0.6231 - val_loss: 6.0452 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 219/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9732 - categorical_accuracy: 0.6445 - f1_score: 0.6426 - val_loss: 6.0452 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 220/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9679 - categorical_accuracy: 0.6385 - f1_score: 0.6360 - val_loss: 6.0449 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 221/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9752 - categorical_accuracy: 0.6163 - f1_score: 0.6139 - val_loss: 6.0446 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 222/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9778 - categorical_accuracy: 0.6193 - f1_score: 0.6168 - val_loss: 6.0443 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 223/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9912 - categorical_accuracy: 0.6244 - f1_score: 0.6225 - val_loss: 6.0442 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 224/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9580 - categorical_accuracy: 0.6284 - f1_score: 0.6262 - val_loss: 6.0442 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 225/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9581 - categorical_accuracy: 0.6284 - f1_score: 0.6261 - val_loss: 6.0445 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 226/500\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.9531 - categorical_accuracy: 0.6495 - f1_score: 0.6479 - val_loss: 6.0447 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 227/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9855 - categorical_accuracy: 0.6244 - f1_score: 0.6232 - val_loss: 6.0444 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 228/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9650 - categorical_accuracy: 0.6244 - f1_score: 0.6223 - val_loss: 6.0442 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 229/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9872 - categorical_accuracy: 0.6365 - f1_score: 0.6343 - val_loss: 6.0443 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 230/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9746 - categorical_accuracy: 0.6385 - f1_score: 0.6357 - val_loss: 6.0443 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 231/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9849 - categorical_accuracy: 0.6354 - f1_score: 0.6319 - val_loss: 6.0440 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 232/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9663 - categorical_accuracy: 0.6435 - f1_score: 0.6419 - val_loss: 6.0439 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 233/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9659 - categorical_accuracy: 0.6435 - f1_score: 0.6420 - val_loss: 6.0442 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 234/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9921 - categorical_accuracy: 0.6203 - f1_score: 0.6179 - val_loss: 6.0438 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 235/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9522 - categorical_accuracy: 0.6314 - f1_score: 0.6296 - val_loss: 6.0435 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 236/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9696 - categorical_accuracy: 0.6455 - f1_score: 0.6437 - val_loss: 6.0432 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 237/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9919 - categorical_accuracy: 0.6304 - f1_score: 0.6274 - val_loss: 6.0432 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 238/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9977 - categorical_accuracy: 0.6536 - f1_score: 0.6513 - val_loss: 6.0430 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 239/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9622 - categorical_accuracy: 0.6153 - f1_score: 0.6120 - val_loss: 6.0432 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 240/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9600 - categorical_accuracy: 0.6234 - f1_score: 0.6200 - val_loss: 6.0438 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 241/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9672 - categorical_accuracy: 0.6203 - f1_score: 0.6165 - val_loss: 5.9583 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 242/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9494 - categorical_accuracy: 0.6485 - f1_score: 0.6438 - val_loss: 5.9403 - val_categorical_accuracy: 0.3545 - val_f1_score: 0.2722\n",
      "Epoch 243/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9726 - categorical_accuracy: 0.6365 - f1_score: 0.6329 - val_loss: 6.0432 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 244/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9508 - categorical_accuracy: 0.6334 - f1_score: 0.6307 - val_loss: 6.0428 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 245/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9700 - categorical_accuracy: 0.6103 - f1_score: 0.6066 - val_loss: 6.0428 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 246/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9573 - categorical_accuracy: 0.6284 - f1_score: 0.6253 - val_loss: 6.0425 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 247/500\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.9618 - categorical_accuracy: 0.6284 - f1_score: 0.6263 - val_loss: 6.0425 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 248/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9727 - categorical_accuracy: 0.6304 - f1_score: 0.6286 - val_loss: 6.0425 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 249/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9489 - categorical_accuracy: 0.6395 - f1_score: 0.6372 - val_loss: 6.0423 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 250/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9602 - categorical_accuracy: 0.6042 - f1_score: 0.6007 - val_loss: 6.0421 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 251/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9535 - categorical_accuracy: 0.6213 - f1_score: 0.6180 - val_loss: 6.0419 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 252/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9469 - categorical_accuracy: 0.6244 - f1_score: 0.6211 - val_loss: 6.0415 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 253/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9536 - categorical_accuracy: 0.6375 - f1_score: 0.6347 - val_loss: 6.0414 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 254/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9584 - categorical_accuracy: 0.6425 - f1_score: 0.6395 - val_loss: 6.0411 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 255/500\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.9532 - categorical_accuracy: 0.6284 - f1_score: 0.6265 - val_loss: 6.0410 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 256/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9774 - categorical_accuracy: 0.6415 - f1_score: 0.6390 - val_loss: 6.0409 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 257/500\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.9664 - categorical_accuracy: 0.6405 - f1_score: 0.6385 - val_loss: 6.0405 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 258/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9534 - categorical_accuracy: 0.6203 - f1_score: 0.6173 - val_loss: 6.0405 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 259/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9472 - categorical_accuracy: 0.6445 - f1_score: 0.6410 - val_loss: 6.0405 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 260/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9535 - categorical_accuracy: 0.6143 - f1_score: 0.6115 - val_loss: 6.0405 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 261/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9710 - categorical_accuracy: 0.6193 - f1_score: 0.6161 - val_loss: 6.0403 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 262/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9780 - categorical_accuracy: 0.6365 - f1_score: 0.6332 - val_loss: 6.0404 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 263/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9699 - categorical_accuracy: 0.6314 - f1_score: 0.6287 - val_loss: 6.0402 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 264/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9664 - categorical_accuracy: 0.6375 - f1_score: 0.6344 - val_loss: 6.0401 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 265/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9612 - categorical_accuracy: 0.6375 - f1_score: 0.6345 - val_loss: 6.0399 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 266/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9668 - categorical_accuracy: 0.6234 - f1_score: 0.6206 - val_loss: 6.0398 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 267/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9611 - categorical_accuracy: 0.6173 - f1_score: 0.6145 - val_loss: 6.0395 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 268/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9597 - categorical_accuracy: 0.6324 - f1_score: 0.6299 - val_loss: 6.0394 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 269/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9487 - categorical_accuracy: 0.6344 - f1_score: 0.6308 - val_loss: 6.0394 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 270/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9522 - categorical_accuracy: 0.6294 - f1_score: 0.6264 - val_loss: 6.0395 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 271/500\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.9535 - categorical_accuracy: 0.6314 - f1_score: 0.6268 - val_loss: 6.0400 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 272/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9659 - categorical_accuracy: 0.6385 - f1_score: 0.6346 - val_loss: 6.0399 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 273/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9508 - categorical_accuracy: 0.6375 - f1_score: 0.6325 - val_loss: 6.0397 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 274/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9508 - categorical_accuracy: 0.6385 - f1_score: 0.6340 - val_loss: 6.0395 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 275/500\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9692 - categorical_accuracy: 0.6052 - f1_score: 0.6009 - val_loss: 6.0394 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 276/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9478 - categorical_accuracy: 0.6314 - f1_score: 0.6291 - val_loss: 6.0392 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 277/500\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.9606 - categorical_accuracy: 0.6344 - f1_score: 0.6304 - val_loss: 6.0391 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 278/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9881 - categorical_accuracy: 0.6324 - f1_score: 0.6288 - val_loss: 6.0390 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 279/500\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.9727 - categorical_accuracy: 0.6425 - f1_score: 0.6390 - val_loss: 6.0389 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 280/500\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9616 - categorical_accuracy: 0.6153 - f1_score: 0.6120 - val_loss: 6.0390 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 281/500\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.9585 - categorical_accuracy: 0.6123 - f1_score: 0.6078 - val_loss: 6.0390 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 282/500\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.9695 - categorical_accuracy: 0.6354 - f1_score: 0.6318 - val_loss: 6.0390 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 283/500\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.9978 - categorical_accuracy: 0.6324 - f1_score: 0.6288 - val_loss: 6.0387 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 284/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9694 - categorical_accuracy: 0.6405 - f1_score: 0.6376 - val_loss: 6.0387 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 285/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9698 - categorical_accuracy: 0.6314 - f1_score: 0.6283 - val_loss: 6.0386 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 286/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9802 - categorical_accuracy: 0.6314 - f1_score: 0.6287 - val_loss: 6.0386 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 287/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9658 - categorical_accuracy: 0.6385 - f1_score: 0.6357 - val_loss: 6.0384 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 288/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9480 - categorical_accuracy: 0.6264 - f1_score: 0.6233 - val_loss: 6.0385 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 289/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9628 - categorical_accuracy: 0.6234 - f1_score: 0.6207 - val_loss: 6.0384 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 290/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9488 - categorical_accuracy: 0.6254 - f1_score: 0.6229 - val_loss: 6.0383 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 291/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9554 - categorical_accuracy: 0.6224 - f1_score: 0.6201 - val_loss: 6.0382 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 292/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9635 - categorical_accuracy: 0.6354 - f1_score: 0.6326 - val_loss: 6.0380 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 293/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9510 - categorical_accuracy: 0.6334 - f1_score: 0.6312 - val_loss: 6.0379 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 294/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9521 - categorical_accuracy: 0.6254 - f1_score: 0.6216 - val_loss: 6.0375 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 295/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9573 - categorical_accuracy: 0.6294 - f1_score: 0.6264 - val_loss: 6.0375 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 296/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9569 - categorical_accuracy: 0.6354 - f1_score: 0.6318 - val_loss: 6.0375 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 297/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9764 - categorical_accuracy: 0.6314 - f1_score: 0.6274 - val_loss: 6.0375 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 298/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9652 - categorical_accuracy: 0.6526 - f1_score: 0.6498 - val_loss: 6.0373 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 299/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9603 - categorical_accuracy: 0.6365 - f1_score: 0.6327 - val_loss: 6.0373 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 300/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9558 - categorical_accuracy: 0.6405 - f1_score: 0.6379 - val_loss: 6.0372 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 301/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9706 - categorical_accuracy: 0.6304 - f1_score: 0.6285 - val_loss: 6.0372 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 302/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 0.9521 - categorical_accuracy: 0.6324 - f1_score: 0.6295 - val_loss: 6.0371 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 303/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9613 - categorical_accuracy: 0.6435 - f1_score: 0.6412 - val_loss: 6.0371 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 304/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9707 - categorical_accuracy: 0.6516 - f1_score: 0.6492 - val_loss: 6.0370 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 305/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9415 - categorical_accuracy: 0.6536 - f1_score: 0.6511 - val_loss: 6.0369 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 306/500\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.9582 - categorical_accuracy: 0.6294 - f1_score: 0.6255 - val_loss: 6.0369 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 307/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9615 - categorical_accuracy: 0.6354 - f1_score: 0.6328 - val_loss: 6.0369 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 308/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9622 - categorical_accuracy: 0.6304 - f1_score: 0.6283 - val_loss: 6.0369 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 309/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9387 - categorical_accuracy: 0.6375 - f1_score: 0.6340 - val_loss: 6.0369 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 310/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9478 - categorical_accuracy: 0.6395 - f1_score: 0.6372 - val_loss: 6.0368 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 311/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9661 - categorical_accuracy: 0.6425 - f1_score: 0.6405 - val_loss: 6.0368 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 312/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9430 - categorical_accuracy: 0.6536 - f1_score: 0.6500 - val_loss: 6.0367 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 313/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9666 - categorical_accuracy: 0.6254 - f1_score: 0.6238 - val_loss: 6.0367 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 314/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9792 - categorical_accuracy: 0.6274 - f1_score: 0.6245 - val_loss: 6.0367 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 315/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9441 - categorical_accuracy: 0.6254 - f1_score: 0.6231 - val_loss: 6.0367 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 316/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9791 - categorical_accuracy: 0.6254 - f1_score: 0.6230 - val_loss: 6.0366 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 317/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 0.9651 - categorical_accuracy: 0.6274 - f1_score: 0.6259 - val_loss: 6.0367 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 318/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9543 - categorical_accuracy: 0.6405 - f1_score: 0.6384 - val_loss: 6.0366 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 319/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9614 - categorical_accuracy: 0.6224 - f1_score: 0.6191 - val_loss: 6.0366 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 320/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9701 - categorical_accuracy: 0.6244 - f1_score: 0.6228 - val_loss: 6.0365 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 321/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9447 - categorical_accuracy: 0.6395 - f1_score: 0.6363 - val_loss: 6.0364 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 322/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9703 - categorical_accuracy: 0.6224 - f1_score: 0.6194 - val_loss: 6.0366 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 323/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9555 - categorical_accuracy: 0.6203 - f1_score: 0.6175 - val_loss: 6.0366 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 324/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9653 - categorical_accuracy: 0.6324 - f1_score: 0.6290 - val_loss: 6.0366 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 325/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9498 - categorical_accuracy: 0.6435 - f1_score: 0.6416 - val_loss: 6.0366 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 326/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9547 - categorical_accuracy: 0.6395 - f1_score: 0.6366 - val_loss: 6.0366 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 327/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9604 - categorical_accuracy: 0.6213 - f1_score: 0.6174 - val_loss: 6.0366 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 328/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9497 - categorical_accuracy: 0.6193 - f1_score: 0.6164 - val_loss: 6.0366 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 329/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9729 - categorical_accuracy: 0.6344 - f1_score: 0.6327 - val_loss: 6.0366 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 330/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9438 - categorical_accuracy: 0.6284 - f1_score: 0.6257 - val_loss: 6.0365 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 331/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9616 - categorical_accuracy: 0.6375 - f1_score: 0.6354 - val_loss: 6.0365 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 332/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9449 - categorical_accuracy: 0.6415 - f1_score: 0.6388 - val_loss: 6.0365 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 333/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9488 - categorical_accuracy: 0.6264 - f1_score: 0.6245 - val_loss: 6.0365 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 334/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9540 - categorical_accuracy: 0.6395 - f1_score: 0.6371 - val_loss: 6.0364 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 335/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9785 - categorical_accuracy: 0.6244 - f1_score: 0.6211 - val_loss: 6.0363 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 336/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 0.9755 - categorical_accuracy: 0.6425 - f1_score: 0.6395 - val_loss: 6.0363 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 337/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9732 - categorical_accuracy: 0.6344 - f1_score: 0.6319 - val_loss: 6.0362 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 338/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9667 - categorical_accuracy: 0.6324 - f1_score: 0.6296 - val_loss: 6.0364 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 339/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9638 - categorical_accuracy: 0.6193 - f1_score: 0.6158 - val_loss: 6.0365 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 340/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9436 - categorical_accuracy: 0.6123 - f1_score: 0.6098 - val_loss: 6.0364 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 341/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9597 - categorical_accuracy: 0.6405 - f1_score: 0.6379 - val_loss: 6.0364 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 342/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9425 - categorical_accuracy: 0.6314 - f1_score: 0.6279 - val_loss: 6.0363 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 343/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9386 - categorical_accuracy: 0.6375 - f1_score: 0.6347 - val_loss: 6.0362 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 344/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9643 - categorical_accuracy: 0.6385 - f1_score: 0.6359 - val_loss: 6.0363 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 345/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9673 - categorical_accuracy: 0.6264 - f1_score: 0.6233 - val_loss: 6.0363 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 346/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9534 - categorical_accuracy: 0.6193 - f1_score: 0.6169 - val_loss: 6.0363 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 347/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9628 - categorical_accuracy: 0.6213 - f1_score: 0.6193 - val_loss: 6.0362 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 348/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9577 - categorical_accuracy: 0.6334 - f1_score: 0.6318 - val_loss: 6.0362 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 349/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9510 - categorical_accuracy: 0.6576 - f1_score: 0.6558 - val_loss: 6.0362 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 350/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9510 - categorical_accuracy: 0.6123 - f1_score: 0.6094 - val_loss: 6.0362 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 351/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9552 - categorical_accuracy: 0.6274 - f1_score: 0.6241 - val_loss: 6.0360 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 352/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9586 - categorical_accuracy: 0.6324 - f1_score: 0.6299 - val_loss: 6.0360 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 353/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9598 - categorical_accuracy: 0.6224 - f1_score: 0.6185 - val_loss: 6.0360 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 354/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9470 - categorical_accuracy: 0.6385 - f1_score: 0.6359 - val_loss: 6.0361 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 355/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9832 - categorical_accuracy: 0.6244 - f1_score: 0.6207 - val_loss: 6.0361 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 356/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9415 - categorical_accuracy: 0.6405 - f1_score: 0.6372 - val_loss: 6.0360 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 357/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9671 - categorical_accuracy: 0.6143 - f1_score: 0.6114 - val_loss: 6.0360 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 358/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9610 - categorical_accuracy: 0.6254 - f1_score: 0.6223 - val_loss: 6.0360 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 359/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9793 - categorical_accuracy: 0.6475 - f1_score: 0.6454 - val_loss: 6.0359 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 360/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9782 - categorical_accuracy: 0.6475 - f1_score: 0.6457 - val_loss: 6.0359 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 361/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9394 - categorical_accuracy: 0.6264 - f1_score: 0.6236 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 362/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9530 - categorical_accuracy: 0.6284 - f1_score: 0.6246 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 363/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9391 - categorical_accuracy: 0.6284 - f1_score: 0.6260 - val_loss: 6.0359 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 364/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9808 - categorical_accuracy: 0.6284 - f1_score: 0.6255 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 365/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9508 - categorical_accuracy: 0.6244 - f1_score: 0.6222 - val_loss: 6.0359 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 366/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9756 - categorical_accuracy: 0.6405 - f1_score: 0.6393 - val_loss: 6.0359 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 367/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9706 - categorical_accuracy: 0.6365 - f1_score: 0.6342 - val_loss: 6.0359 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 368/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9526 - categorical_accuracy: 0.6405 - f1_score: 0.6379 - val_loss: 6.0359 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 369/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9636 - categorical_accuracy: 0.6294 - f1_score: 0.6270 - val_loss: 6.0359 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 370/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9545 - categorical_accuracy: 0.6324 - f1_score: 0.6298 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 371/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9657 - categorical_accuracy: 0.6183 - f1_score: 0.6146 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 372/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9632 - categorical_accuracy: 0.6314 - f1_score: 0.6283 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 373/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9446 - categorical_accuracy: 0.6274 - f1_score: 0.6245 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 374/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9587 - categorical_accuracy: 0.6385 - f1_score: 0.6357 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 375/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9501 - categorical_accuracy: 0.6254 - f1_score: 0.6227 - val_loss: 6.0359 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 376/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9465 - categorical_accuracy: 0.6274 - f1_score: 0.6235 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 377/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9585 - categorical_accuracy: 0.6455 - f1_score: 0.6435 - val_loss: 6.0359 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 378/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9435 - categorical_accuracy: 0.6354 - f1_score: 0.6321 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 379/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9610 - categorical_accuracy: 0.6244 - f1_score: 0.6220 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 380/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9470 - categorical_accuracy: 0.6254 - f1_score: 0.6223 - val_loss: 6.0359 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 381/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9376 - categorical_accuracy: 0.6344 - f1_score: 0.6315 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 382/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9770 - categorical_accuracy: 0.6314 - f1_score: 0.6287 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 383/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9582 - categorical_accuracy: 0.6506 - f1_score: 0.6484 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 384/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9605 - categorical_accuracy: 0.6203 - f1_score: 0.6173 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 385/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9495 - categorical_accuracy: 0.6375 - f1_score: 0.6344 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 386/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9654 - categorical_accuracy: 0.6284 - f1_score: 0.6258 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 387/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9561 - categorical_accuracy: 0.6344 - f1_score: 0.6315 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 388/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9499 - categorical_accuracy: 0.6455 - f1_score: 0.6433 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 389/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 0.9435 - categorical_accuracy: 0.6254 - f1_score: 0.6232 - val_loss: 6.0357 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 390/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9616 - categorical_accuracy: 0.6385 - f1_score: 0.6367 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 391/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9639 - categorical_accuracy: 0.6234 - f1_score: 0.6210 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 392/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9510 - categorical_accuracy: 0.6153 - f1_score: 0.6129 - val_loss: 6.0357 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 393/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9462 - categorical_accuracy: 0.6244 - f1_score: 0.6215 - val_loss: 6.0357 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 394/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9746 - categorical_accuracy: 0.6304 - f1_score: 0.6285 - val_loss: 6.0357 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 395/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9586 - categorical_accuracy: 0.6224 - f1_score: 0.6191 - val_loss: 6.0357 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 396/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9522 - categorical_accuracy: 0.6385 - f1_score: 0.6353 - val_loss: 6.0357 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 397/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9554 - categorical_accuracy: 0.6415 - f1_score: 0.6387 - val_loss: 6.0358 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 398/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9632 - categorical_accuracy: 0.6284 - f1_score: 0.6258 - val_loss: 6.0357 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 399/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9584 - categorical_accuracy: 0.6314 - f1_score: 0.6286 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 400/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9537 - categorical_accuracy: 0.6224 - f1_score: 0.6189 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 401/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9929 - categorical_accuracy: 0.6445 - f1_score: 0.6417 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 402/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9589 - categorical_accuracy: 0.6435 - f1_score: 0.6409 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 403/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9601 - categorical_accuracy: 0.6526 - f1_score: 0.6502 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 404/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9467 - categorical_accuracy: 0.6354 - f1_score: 0.6337 - val_loss: 6.0357 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 405/500\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.9601 - categorical_accuracy: 0.6365 - f1_score: 0.6330 - val_loss: 6.0357 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 406/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9494 - categorical_accuracy: 0.6264 - f1_score: 0.6230 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 407/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9594 - categorical_accuracy: 0.6304 - f1_score: 0.6277 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 408/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9728 - categorical_accuracy: 0.6244 - f1_score: 0.6208 - val_loss: 6.0357 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 409/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9482 - categorical_accuracy: 0.6254 - f1_score: 0.6229 - val_loss: 6.0357 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 410/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9506 - categorical_accuracy: 0.6324 - f1_score: 0.6300 - val_loss: 6.0357 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 411/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9629 - categorical_accuracy: 0.6294 - f1_score: 0.6269 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 412/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9602 - categorical_accuracy: 0.6395 - f1_score: 0.6360 - val_loss: 6.0357 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 413/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9710 - categorical_accuracy: 0.6193 - f1_score: 0.6168 - val_loss: 6.0357 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 414/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9387 - categorical_accuracy: 0.6405 - f1_score: 0.6374 - val_loss: 6.0357 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 415/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9596 - categorical_accuracy: 0.6254 - f1_score: 0.6223 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 416/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9581 - categorical_accuracy: 0.6365 - f1_score: 0.6346 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 417/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9762 - categorical_accuracy: 0.6344 - f1_score: 0.6323 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 418/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9344 - categorical_accuracy: 0.6435 - f1_score: 0.6406 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 419/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9335 - categorical_accuracy: 0.6354 - f1_score: 0.6336 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 420/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9421 - categorical_accuracy: 0.6264 - f1_score: 0.6234 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 421/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9584 - categorical_accuracy: 0.6365 - f1_score: 0.6340 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 422/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9427 - categorical_accuracy: 0.6113 - f1_score: 0.6074 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 423/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9718 - categorical_accuracy: 0.6395 - f1_score: 0.6366 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 424/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9642 - categorical_accuracy: 0.6294 - f1_score: 0.6263 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 425/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9485 - categorical_accuracy: 0.6143 - f1_score: 0.6102 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 426/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9589 - categorical_accuracy: 0.6344 - f1_score: 0.6322 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 427/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9595 - categorical_accuracy: 0.6445 - f1_score: 0.6410 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 428/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9397 - categorical_accuracy: 0.6334 - f1_score: 0.6296 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 429/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9564 - categorical_accuracy: 0.6365 - f1_score: 0.6330 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 430/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9453 - categorical_accuracy: 0.6224 - f1_score: 0.6196 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 431/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9414 - categorical_accuracy: 0.6344 - f1_score: 0.6322 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 432/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9769 - categorical_accuracy: 0.6354 - f1_score: 0.6324 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 433/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9409 - categorical_accuracy: 0.6506 - f1_score: 0.6480 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 434/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9573 - categorical_accuracy: 0.6516 - f1_score: 0.6481 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 435/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9814 - categorical_accuracy: 0.6284 - f1_score: 0.6247 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 436/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9551 - categorical_accuracy: 0.6163 - f1_score: 0.6134 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 437/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9490 - categorical_accuracy: 0.6445 - f1_score: 0.6413 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 438/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9495 - categorical_accuracy: 0.6304 - f1_score: 0.6289 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 439/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9436 - categorical_accuracy: 0.6455 - f1_score: 0.6429 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 440/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9629 - categorical_accuracy: 0.6375 - f1_score: 0.6350 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 441/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9409 - categorical_accuracy: 0.6284 - f1_score: 0.6261 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 442/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9575 - categorical_accuracy: 0.6626 - f1_score: 0.6602 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 443/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9530 - categorical_accuracy: 0.6314 - f1_score: 0.6283 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 444/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9441 - categorical_accuracy: 0.6183 - f1_score: 0.6164 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 445/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9564 - categorical_accuracy: 0.6103 - f1_score: 0.6075 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 446/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9444 - categorical_accuracy: 0.6495 - f1_score: 0.6470 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 447/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9898 - categorical_accuracy: 0.6395 - f1_score: 0.6364 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 448/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9457 - categorical_accuracy: 0.6475 - f1_score: 0.6447 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 449/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9761 - categorical_accuracy: 0.6354 - f1_score: 0.6326 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 450/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9754 - categorical_accuracy: 0.6375 - f1_score: 0.6340 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 451/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9453 - categorical_accuracy: 0.6213 - f1_score: 0.6175 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 452/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9410 - categorical_accuracy: 0.6213 - f1_score: 0.6191 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 453/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9584 - categorical_accuracy: 0.6274 - f1_score: 0.6239 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 454/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9439 - categorical_accuracy: 0.6455 - f1_score: 0.6420 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 455/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9694 - categorical_accuracy: 0.6405 - f1_score: 0.6369 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 456/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9398 - categorical_accuracy: 0.6405 - f1_score: 0.6386 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 457/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9575 - categorical_accuracy: 0.6314 - f1_score: 0.6284 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 458/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9572 - categorical_accuracy: 0.6224 - f1_score: 0.6194 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 459/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9547 - categorical_accuracy: 0.6365 - f1_score: 0.6337 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 460/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9742 - categorical_accuracy: 0.6314 - f1_score: 0.6274 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 461/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9394 - categorical_accuracy: 0.6445 - f1_score: 0.6425 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 462/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9602 - categorical_accuracy: 0.6405 - f1_score: 0.6379 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 463/500\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.9445 - categorical_accuracy: 0.6213 - f1_score: 0.6180 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 464/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9588 - categorical_accuracy: 0.6455 - f1_score: 0.6424 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 465/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9619 - categorical_accuracy: 0.6375 - f1_score: 0.6338 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 466/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9633 - categorical_accuracy: 0.6415 - f1_score: 0.6386 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 467/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9717 - categorical_accuracy: 0.6425 - f1_score: 0.6405 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 468/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9587 - categorical_accuracy: 0.6415 - f1_score: 0.6387 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 469/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9724 - categorical_accuracy: 0.6224 - f1_score: 0.6201 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 470/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9519 - categorical_accuracy: 0.6425 - f1_score: 0.6404 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 471/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9457 - categorical_accuracy: 0.6425 - f1_score: 0.6403 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 472/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9520 - categorical_accuracy: 0.6284 - f1_score: 0.6244 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 473/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9772 - categorical_accuracy: 0.6354 - f1_score: 0.6321 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 474/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9566 - categorical_accuracy: 0.6375 - f1_score: 0.6348 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 475/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9470 - categorical_accuracy: 0.6213 - f1_score: 0.6184 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 476/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9700 - categorical_accuracy: 0.6294 - f1_score: 0.6258 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 477/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9443 - categorical_accuracy: 0.6385 - f1_score: 0.6359 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 478/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9423 - categorical_accuracy: 0.6375 - f1_score: 0.6341 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 479/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9694 - categorical_accuracy: 0.6485 - f1_score: 0.6461 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 480/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9569 - categorical_accuracy: 0.6344 - f1_score: 0.6314 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 481/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9625 - categorical_accuracy: 0.6284 - f1_score: 0.6257 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 482/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9565 - categorical_accuracy: 0.6365 - f1_score: 0.6332 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 483/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9455 - categorical_accuracy: 0.6365 - f1_score: 0.6336 - val_loss: 6.0354 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 484/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 0.9522 - categorical_accuracy: 0.6405 - f1_score: 0.6365 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 485/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9481 - categorical_accuracy: 0.6435 - f1_score: 0.6412 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 486/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9620 - categorical_accuracy: 0.6546 - f1_score: 0.6518 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 487/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9395 - categorical_accuracy: 0.6445 - f1_score: 0.6418 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 488/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9545 - categorical_accuracy: 0.6415 - f1_score: 0.6391 - val_loss: 6.0356 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 489/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9595 - categorical_accuracy: 0.6344 - f1_score: 0.6318 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 490/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9407 - categorical_accuracy: 0.6173 - f1_score: 0.6141 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 491/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9576 - categorical_accuracy: 0.6375 - f1_score: 0.6345 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 492/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9446 - categorical_accuracy: 0.6294 - f1_score: 0.6274 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 493/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9566 - categorical_accuracy: 0.6334 - f1_score: 0.6298 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 494/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9479 - categorical_accuracy: 0.6314 - f1_score: 0.6290 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 495/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9622 - categorical_accuracy: 0.6304 - f1_score: 0.6277 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 496/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9599 - categorical_accuracy: 0.6133 - f1_score: 0.6107 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 497/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9586 - categorical_accuracy: 0.6314 - f1_score: 0.6284 - val_loss: 6.0354 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 498/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9454 - categorical_accuracy: 0.6435 - f1_score: 0.6409 - val_loss: 6.0354 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 499/500\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 0.9710 - categorical_accuracy: 0.6375 - f1_score: 0.6334 - val_loss: 6.0355 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n",
      "Epoch 500/500\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 0.9822 - categorical_accuracy: 0.6344 - f1_score: 0.6326 - val_loss: 6.0354 - val_categorical_accuracy: 0.3636 - val_f1_score: 0.2774\n"
     ]
    }
   ],
   "source": [
    "MlFFU = DSfusionDecisionsML((IMAGE_SIZE[0],IMAGE_SIZE[1],3))\n",
    "\n",
    "plot_model(MlFFU, to_file=plotpath / Path('ML_DF_U.png'), show_shapes=True, show_layer_names=True)\n",
    "\n",
    "\n",
    "\n",
    "# initial_learning_rate = 1.5e-5      # NON VA BENE\n",
    "# initial_learning_rate = 1.5e-4      #acc 0.66\n",
    "initial_learning_rate = 1e-3  # migliore come accuracy massima  0.69\n",
    "\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=100, decay_rate=0.98, staircase=False)\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=100, decay_rate=0.95, staircase=False)\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=np.ceil(X_train_IMGS.shape[0]/batch_size), decay_rate=0.99, staircase=False)\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=np.ceil((X_train_IMGS.shape[0]/batch_size)/2), decay_rate=0.99, staircase=True)\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=np.ceil((X_train_IMGS.shape[0]/batch_size)), decay_rate=0.99, staircase=True)\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=300, decay_rate=0.99, staircase=False)\n",
    "    \n",
    "\n",
    "MlFFU.compile(\n",
    "        # optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-2, decay=1e-4),   #FUSION 2022\n",
    "        optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "        # optimizer=tf.keras.optimizers.Adam(0.0011),\n",
    "        # optimizer = tf.keras.optimizers.Adam(1e-2),\n",
    "        # optimizer=tf.keras.optimizers.Adam(1.1e-3),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        # loss=\"categorical_crossentropy\",\n",
    "        # metrics=[\"accuracy\",dice_coef,\"categorical_accuracy\"],\n",
    "        # metrics=[\"categorical_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=4)],\n",
    "        metrics=[\"categorical_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=2)],\n",
    "        # metrics=[\"categorical_accuracy\"],\n",
    ")\n",
    "# CREATE CALLBACKS\n",
    "checkpointClassification = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        checkpointPath / Path('ML_DF_U_Classification_acc'),\n",
    "                        monitor='val_categorical_accuracy', verbose=0, \n",
    "                        # monitor='val_dm_pignistic_16_categorical_accuracy', verbose=0, \n",
    "                        save_best_only=True, mode='max')\n",
    "                                                # CREATE CALLBACKS\n",
    "\n",
    "\n",
    "checkpointClassificationF1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        checkpointPath / Path('ML_DF_U_Classification_F1'), \n",
    "                        monitor='val_f1_score', verbose=0, \n",
    "                        save_best_only=True, mode='max')\n",
    "                                                # CREATE CALLBACKS\n",
    "\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "\n",
    "\n",
    "    \n",
    "lr_finder = LRFinder(min_lr=1e-3,\n",
    "                     max_lr=1e-3*9,\n",
    "                             steps_per_epoch=np.ceil(X_train_IMGS.shape[0]/batch_size), \n",
    "                             epochs=10)\n",
    "\n",
    "\n",
    "schedule_lr = SGDRScheduler(min_lr=0.0009,\n",
    "                                max_lr=0.01,\n",
    "                                steps_per_epoch=np.ceil(X_train_IMGS.shape[0]/batch_size),\n",
    "                                lr_decay=0.8,\n",
    "                                cycle_length=10,\n",
    "                                mult_factor=3.5)\n",
    "\n",
    "\n",
    "    \n",
    "# lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "\n",
    "cb = TimingCallback()\n",
    "\n",
    "print(checkpointPath / Path('resnet_Classification_acc'))\n",
    "\n",
    "# callbacks_list = [checkpointClassification]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb, schedule_lr,lrate]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb, schedule_lr]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, cb, schedule_lr]\n",
    "callbacks_list = [checkpointClassification, checkpointClassificationF1, cb]\n",
    "# callbacks_list = [checkpointClassification, cb]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb]\n",
    "# callbacks_list = [checkpointClassificationF1, earlyStopping]\n",
    "\n",
    "history = MlFFU.fit(\n",
    "        # x = X_train_IMGS,\n",
    "        # trainset,\n",
    "        x = X_train_IMGS,\n",
    "        # y = [Y_train,Y_tr_unc],\n",
    "        y = Y_train,\n",
    "        # epochs = 1,\n",
    "        epochs = 500,\n",
    "        validation_data = [X_val_IMGS,Y_val],\n",
    "        # validation_data = [X_val_IMGS,[Y_val,Y_val_unc]],\n",
    "        # validation_data = validset,\n",
    "        callbacks = callbacks_list,    #some problems with DS layers and callbacks\n",
    "        batch_size=32,\n",
    "        # batch_size=1,\n",
    "        # class_weight=c_w,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.703890800000003, 1.0069823999999983, 0.6218789999999927, 0.6296227999999928, 10.065035999999992, 5.361273600000004, 0.8302711999999985, 0.6190347000000003, 0.6187005999999968, 0.6298341000000107, 5.147708100000003, 0.7665519999999901, 0.659836900000002, 0.629547500000001, 0.6357748000000072, 0.6304295999999994, 0.6418180999999947, 0.6271935999999982, 0.6218489000000034, 0.6217108000000025, 0.6169158999999951, 0.6131697000000003, 0.6317272000000003, 0.6156912000000005, 0.6211623000000088, 0.6260998000000058, 0.6473134000000016, 0.6160548000000006, 0.6268303000000088, 0.6392414000000031, 0.6272115999999954, 0.6290851999999916, 0.6368634999999898, 0.6222153000000077, 0.633775, 0.6341922000000011, 0.6478275999999994, 0.6274185999999986, 0.6346276999999958, 0.6386159000000049, 0.6339949000000047, 0.6450899999999962, 0.6421407999999929, 0.6289560999999964, 0.627363900000006, 0.6234036999999972, 0.6392905999999954, 0.6216995999999995, 0.6199354999999969, 0.6373861000000147, 0.6364182000000085, 0.6357815000000073, 0.6261838000000068, 0.6426052000000197, 0.6264831999999956, 0.6307784000000254, 0.6379287999999974, 0.6369898000000092, 0.6279227999999932, 0.6285358000000087, 0.6311610999999857, 0.622195099999999, 5.179621300000008, 0.8816083000000106, 0.6260897000000227, 0.6315767000000108, 0.6168481999999926, 0.617576200000002, 0.614882300000005, 0.6319056000000103, 0.635331199999996, 0.6160278000000119, 0.6135086999999828, 0.6315099999999916, 0.6237747999999783, 0.6219336000000055, 0.6216048999999941, 0.6133024000000091, 0.6180263999999909, 0.6148553000000163, 0.6312653000000239, 0.6193083999999942, 0.6209704999999985, 0.6212273000000152, 0.6169731999999897, 0.6304341999999963, 0.6231973999999809, 0.6248705000000143, 0.6166039999999953, 0.6178106999999784, 0.619143500000007, 0.6250590000000216, 0.633780999999999, 0.6073301000000129, 0.6133153000000107, 0.5992707999999993, 0.621703999999994, 0.6227796000000012, 0.6339577999999904, 0.6351252999999986, 0.6009573000000046, 0.679201699999993, 0.6479628999999818, 0.6133264000000054, 0.5951327000000219, 0.5844806000000062, 0.6918630000000121, 0.7956245000000024, 0.7981355000000008, 0.7458276000000126, 0.8256370999999945, 0.8194943000000023, 0.7676123999999902, 0.7724754000000189, 0.8117809000000022, 0.7521366, 0.7698261999999829, 0.7641432999999722, 0.8213281999999822, 0.8178335000000061, 0.7400992999999971, 0.7696028000000013, 0.8308043000000112, 0.7709193999999968, 0.808525799999984, 0.7819716999999855, 0.7969374000000187, 0.7879227000000242, 0.7733786000000009, 0.8104968000000099, 0.7758353000000113, 0.7911622000000023, 0.8214404999999942, 0.7326467000000036, 0.7938957000000073, 0.8002529999999979, 0.8057739999999853, 0.7733900000000062, 0.7621757000000002, 0.7886046000000135, 0.822268600000001, 0.7557196000000204, 0.7477445000000102, 0.8207850999999948, 0.7877440000000036, 0.7928990999999996, 0.7810766000000058, 0.7541951999999981, 0.7521694999999795, 0.7778397000000155, 0.7582370999999739, 0.7631397000000106, 0.7850343000000066, 0.7774997999999869, 0.7844226999999933, 0.7667845, 0.8206888999999933, 0.7988084000000129, 0.7920078999999873, 0.7935118000000045, 0.7796659999999918, 0.7824291999999957, 0.753868099999977, 0.8458003000000076, 0.7876456999999846, 0.7959864999999979, 0.8370127999999966, 0.7467824999999948, 0.8087381999999934, 0.755346099999997, 0.770468099999988, 0.8087694999999826, 0.8187949000000003, 0.7296206000000041, 0.810288700000001, 0.8051512000000116, 0.7794654999999864, 0.8045586000000071, 0.7750874000000181, 0.8058929999999975, 0.7643514000000096, 0.8490807999999959, 0.7165515000000084, 0.7793756000000087, 0.7702980999999909, 0.8394775000000152, 0.7550202000000183, 0.7787591999999961, 0.7462188000000083, 0.8086960999999917, 0.8606622000000073, 0.7783691000000204, 0.7731787999999824, 0.7689076000000057, 0.7848930000000109, 0.8141856999999959, 0.7886613000000011, 0.766711799999996, 0.781214800000015, 0.7754598999999871, 0.764986499999992, 0.7721051000000045, 0.7585782000000165, 0.7624409000000014, 0.7743335000000116, 0.7502440000000092, 0.755190099999993, 0.8330233000000078, 0.809311699999995, 0.7939815999999951, 0.7811784000000159, 0.8011450999999852, 0.7928731000000084, 0.7656188999999927, 0.7774752999999919, 0.8142032000000086, 0.769424000000015, 0.8061629000000039, 0.7838225999999509, 0.793907900000022, 0.7669643999999494, 0.7862468999999805, 0.7676520999999639, 0.792367200000001, 0.7707874000000174, 0.8226232999999752, 0.7593048000000522, 0.809479500000009, 0.7522347999999965, 0.7649385000000279, 0.8046380000000113, 0.8064265000000432, 0.7856176999999889, 0.7677823999999873, 0.7598751999999536, 0.7661305999999968, 0.7749204999999506, 0.7546390000000542, 0.7758628999999928, 0.7700221000000056, 0.7416845000000194, 0.7830604000000108, 0.7349381000000221, 0.785324600000024, 0.7470453999999904, 0.7940055000000257, 0.7172575999999822, 0.7766471999999567, 0.6846834000000399, 0.7862025000000017, 0.7807545000000005, 0.7485827000000427, 0.7759803000000147, 0.7445315999999593, 0.7180946999999946, 0.7802244000000087, 0.705885099999989, 0.7500683000000095, 0.7756657000000473, 0.7577608999999939, 0.789152099999967, 0.7531066999999894, 0.7429170999999997, 0.8117240999999922, 0.7479493999999818, 0.7856391000000258, 0.8171065999999882, 0.7319109999999682, 0.7501240999999936, 0.7630245000000286, 0.7434407000000078, 0.781970699999988, 0.7781820000000153, 0.8057795999999939, 0.7651854000000071, 0.7930261999999857, 0.8351273999999762, 0.802490999999975, 0.8218138999999951, 0.7871870000000172, 0.8261883000000125, 0.8287104999999997, 0.7096312999999554, 0.6778595000000109, 0.6434431999999788, 0.6542182000000025, 0.6500556000000302, 0.657210100000043, 0.6496846999999661, 0.6732966999999803, 0.6679560000000038, 0.6736183000000437, 0.6417867000000115, 0.6552909000000113, 0.6761038000000212, 0.6740813999999773, 0.6474886000000311, 0.6311809999999696, 0.6393719000000146, 0.6598546000000169, 0.6845471999999972, 0.6217128000000116, 0.6567274999999881, 0.6528448000000253, 0.6396796999999879, 0.6911936999999853, 0.6503127999999947, 0.6544769999999858, 0.6644607999999721, 0.6698665999999776, 0.6627867999999921, 0.6578175999999871, 0.6273611000000301, 0.6422587000000135, 0.6486424000000284, 0.6507713000000308, 0.6211010000000101, 0.6340301000000181, 0.6309716000000094, 0.6297036000000276, 0.6391304999999647, 0.6352390999999784, 0.6353591000000165, 0.6253607000000443, 0.6321857000000364, 0.6267421000000013, 0.6400209999999902, 0.6325367000000028, 0.6231428000000392, 0.6346609999999941, 0.635336499999994, 0.6340918999999872, 0.6328609999999912, 0.6237265999999977, 0.6363288999999668, 0.6212729000000081, 0.6285310999999751, 0.635292800000002, 0.643830400000013, 0.6365446000000361, 0.6385409000000095, 0.6314734000000044, 0.6290220000000204, 0.6371216000000004, 0.6241036000000122, 0.6378014000000007, 0.6266357999999741, 0.6425123999999869, 0.6266352000000097, 0.6318752999999901, 0.6311807000000158, 0.6394652999999835, 0.6308434000000034, 0.6289808000000221, 0.6361699999999928, 0.6251217999999881, 0.6277902000000495, 0.6249063999999862, 0.6352268999999637, 0.6358389000000102, 0.6308554999999956, 0.6335801000000174, 0.6517895999999723, 0.6253167000000417, 0.6389800000000037, 0.6376802999999995, 0.652122200000008, 0.6365392999999813, 0.6436343999999963, 0.6484148999999775, 0.6399124000000143, 0.6379056000000105, 0.6383816999999681, 0.631415899999979, 0.6327352999999789, 0.6601053999999635, 0.6384969000000069, 0.6620384999999942, 0.6336779000000092, 0.6290430999999899, 0.6284168999999906, 0.6393053000000464, 0.6647207999999978, 0.6413594999999646, 0.6255080999999905, 0.6313935000000015, 0.6541651000000002, 0.6612137000000189, 0.6215104999999994, 0.6413794999999709, 0.631961600000011, 0.6536527999999748, 0.6410445999999865, 0.6286043000000063, 0.638503500000013, 0.6247495000000072, 0.63821310000003, 0.6250736999999731, 0.6506026000000134, 0.6335357000000386, 0.6405138000000079, 0.637657699999977, 0.6398514999999634, 0.6601110999999946, 0.6869746999999506, 0.6518637000000354, 0.6392856999999594, 0.6387945999999829, 0.6503572000000304, 0.6354517999999985, 0.6588179000000309, 0.629250299999967, 0.6572158000000172, 0.6336778000000436, 0.6684381999999687, 0.643096700000001, 0.6634146000000101, 0.6339850999999612, 0.653450399999997, 0.6407744999999636, 0.6374046000000249, 0.6291956000000027, 0.6563750999999911, 0.6377064999999789, 0.6863845999999967, 0.6305950999999936, 0.654661400000009, 0.6415446000000316, 0.6778180999999677, 0.6514410999999996, 0.6261053000000061, 0.6329564000000119, 0.6486360000000104, 0.6256594000000177, 0.6284983999999554, 0.6345618999999942, 0.6460583999999585, 0.6235123000000158, 0.6589697000000001, 0.6452443000000017, 0.6486476000000039, 0.6416303999999968, 0.6430401999999731, 0.6417700999999738, 0.6304508999999712, 0.6622019999999793, 0.6646745000000465, 0.6409361999999987, 0.6487332000000379, 0.6744178000000147, 0.6533357000000137, 0.6534827000000405, 0.6431802999999832, 0.6421225000000277, 0.6479304999999727, 0.6402503000000479, 0.6469541999999819, 0.6229306999999835, 0.6481275999999525, 0.6389218999999571, 0.6505875999999944, 0.65276369999998, 0.6539928000000259, 0.6388714999999934, 0.6361165000000142, 0.6268931999999836, 0.6431367000000137, 0.6368347000000085, 0.6288009999999531, 0.6286891999999966, 0.6497153000000253, 0.6441075000000183, 0.6277916999999889, 0.6423709000000031, 0.6397215000000074, 0.648458699999992, 0.6324598000000492, 0.6303668000000471, 0.6362687000000165, 0.6313070999999582, 0.6366466999999716, 0.6331291999999848, 0.6340191999999547, 0.6215424000000098, 0.6311356999999589, 0.649861699999974, 0.6444182999999839, 0.6416516999999544, 0.6401509000000374, 0.6389000000000351, 0.6365331000000083, 0.637629899999979, 0.6271262999999863, 0.6410208000000353, 0.6397492000000398, 0.6459674999999834, 0.6474005000000034, 0.6355064999999627, 0.6381989000000203, 0.6188384000000156]\n",
      "382.1277691000001\n"
     ]
    }
   ],
   "source": [
    "print(cb.logs)\n",
    "print(sum(cb.logs))\n",
    "with open(str(evalspath) + 'ML_DF_U_TRAINING_TIME.txt', 'w') as f:\n",
    "    f.write('Training Time for feature fusion'+'\\n\\n')\n",
    "    f.write('TRAINING TIMES: '+'\\n')\n",
    "    ep = 0\n",
    "    for t in cb.logs:\n",
    "        ep = ep + 1\n",
    "        f.write('Epoch: ' + str(ep) + ' >>> '+' time: ' + str(t) + '\\n')\n",
    "    f.write('\\n\\n')\n",
    "    f.write('TRAINING TIME SUM: '+'\\n')\n",
    "    f.write('\\n\\n')\n",
    "    f.write(str(sum(cb.logs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train loss",
         "type": "scatter",
         "y": [
          1.478796362876892,
          1.2351579666137695,
          1.178422212600708,
          1.1385635137557983,
          1.1501526832580566,
          1.1457324028015137,
          1.1533613204956055,
          1.12955904006958,
          1.1336780786514282,
          1.1282355785369873,
          1.1222566366195679,
          1.1520596742630005,
          1.1166467666625977,
          1.1216752529144287,
          1.1184134483337402,
          1.1283777952194214,
          1.1231048107147217,
          1.1123323440551758,
          1.1448564529418945,
          1.097946047782898,
          1.1367886066436768,
          1.1822007894515991,
          1.1024692058563232,
          1.106563925743103,
          1.1031759977340698,
          1.081778883934021,
          1.0943782329559326,
          1.0842270851135254,
          1.1811546087265015,
          1.2474781274795532,
          1.2248247861862183,
          1.2425607442855835,
          1.2343599796295166,
          1.1950255632400513,
          1.1359015703201294,
          1.1272990703582764,
          1.1372463703155518,
          1.123412013053894,
          1.1409441232681274,
          1.148398995399475,
          1.1266976594924927,
          1.116301417350769,
          1.111283540725708,
          1.1180599927902222,
          1.1218682527542114,
          1.1434988975524902,
          1.1268348693847656,
          1.1220725774765015,
          1.1119699478149414,
          1.109037160873413,
          1.0988218784332275,
          1.1016497611999512,
          1.0960732698440552,
          1.095261812210083,
          1.100618839263916,
          1.0911227464675903,
          1.0910128355026245,
          1.0823814868927002,
          1.083583950996399,
          1.1032209396362305,
          1.081373691558838,
          1.0977470874786377,
          1.0762035846710205,
          1.0793851613998413,
          1.0671513080596924,
          1.073180913925171,
          1.0693995952606201,
          1.0834983587265015,
          1.0636656284332275,
          1.0959081649780273,
          1.053017020225525,
          1.0639259815216064,
          1.0659964084625244,
          1.055096983909607,
          1.0543104410171509,
          1.0537590980529785,
          1.0478075742721558,
          1.0503816604614258,
          1.0500909090042114,
          1.0816593170166016,
          1.0787229537963867,
          1.0600336790084839,
          1.0611817836761475,
          1.080225944519043,
          1.0364103317260742,
          1.0740681886672974,
          1.0380778312683105,
          1.0303577184677124,
          1.038649559020996,
          1.0344570875167847,
          1.036544919013977,
          1.0421487092971802,
          1.0293346643447876,
          1.0589897632598877,
          1.0432575941085815,
          1.0399086475372314,
          1.071556806564331,
          1.0428671836853027,
          1.029125452041626,
          1.0259287357330322,
          1.0336660146713257,
          1.010601282119751,
          1.0347546339035034,
          1.0247998237609863,
          1.0240274667739868,
          1.0079542398452759,
          1.0266591310501099,
          1.0168840885162354,
          1.0114272832870483,
          1.0285173654556274,
          1.057984709739685,
          1.0530756711959839,
          1.0622605085372925,
          1.058226466178894,
          1.0355850458145142,
          1.02676260471344,
          1.0264780521392822,
          1.0234298706054688,
          1.0315159559249878,
          1.017899751663208,
          1.0138698816299438,
          1.0220144987106323,
          1.0094066858291626,
          1.0174448490142822,
          1.0084956884384155,
          1.0124330520629883,
          1.00931715965271,
          1.0032837390899658,
          1.0122268199920654,
          1.0001949071884155,
          0.994221568107605,
          1.0121756792068481,
          0.9992040395736694,
          1.0072141885757446,
          1.019447922706604,
          0.9951178431510925,
          0.996076226234436,
          1.0065549612045288,
          0.997904360294342,
          1.0122301578521729,
          0.9913886189460754,
          0.9858638048171997,
          0.986768901348114,
          1.0116573572158813,
          0.9954176545143127,
          1.001200556755066,
          0.9850120544433594,
          0.9810474514961243,
          1.0262264013290405,
          0.9850464463233948,
          0.996128261089325,
          0.9899567365646362,
          0.9800535440444946,
          0.9950314164161682,
          0.9946026802062988,
          0.978255033493042,
          0.9875267744064331,
          0.9981871247291565,
          0.9895609617233276,
          0.985854983329773,
          1.0030521154403687,
          0.9789977669715881,
          0.9757333397865295,
          0.9908202290534973,
          1.0078810453414917,
          1.0164737701416016,
          0.9703375697135925,
          0.9755280613899231,
          0.9877213835716248,
          0.9785231351852417,
          0.9978598952293396,
          0.9940372109413147,
          0.9772641062736511,
          0.9826640486717224,
          0.9706516861915588,
          0.9792694449424744,
          0.9708481431007385,
          1.000223994255066,
          0.9707809686660767,
          1.0042757987976074,
          0.971490204334259,
          0.9839130640029907,
          0.9740147590637207,
          0.9782431721687317,
          0.9734036922454834,
          0.9674264192581177,
          0.972368597984314,
          0.9639609456062317,
          0.9865830540657043,
          0.9720192551612854,
          0.9712139964103699,
          0.9691628813743591,
          0.989614725112915,
          0.96532142162323,
          0.9702358245849609,
          0.9797738194465637,
          0.9785711765289307,
          0.9940041303634644,
          0.9814488887786865,
          0.9925997257232666,
          0.9712800979614258,
          0.9602571725845337,
          0.9653173685073853,
          0.9645369648933411,
          0.9597284197807312,
          0.9661826491355896,
          0.9721770882606506,
          0.9654585123062134,
          0.9896281361579895,
          0.9621645212173462,
          0.9795984029769897,
          1.0031009912490845,
          0.9809205532073975,
          0.9531101584434509,
          0.9650707840919495,
          0.958545982837677,
          0.9546780586242676,
          0.9750931859016418,
          0.9732222557067871,
          0.9679396748542786,
          0.9752097129821777,
          0.9777753353118896,
          0.9911917448043823,
          0.9580035209655762,
          0.9581062197685242,
          0.9530698657035828,
          0.9855130910873413,
          0.9649960398674011,
          0.9872130751609802,
          0.9745984673500061,
          0.9848856329917908,
          0.966311514377594,
          0.9658867120742798,
          0.9921355247497559,
          0.9521552324295044,
          0.9695504307746887,
          0.9918937087059021,
          0.9977399706840515,
          0.9621879458427429,
          0.9600284099578857,
          0.9672424793243408,
          0.9494108557701111,
          0.9725720882415771,
          0.9507588744163513,
          0.9699621200561523,
          0.9572672843933105,
          0.9617993235588074,
          0.9726971387863159,
          0.9489008188247681,
          0.9601947665214539,
          0.953453004360199,
          0.9468891024589539,
          0.9535670876502991,
          0.9584493041038513,
          0.9531679153442383,
          0.977355420589447,
          0.9663830399513245,
          0.9534435272216797,
          0.9472225904464722,
          0.9535261392593384,
          0.9710357785224915,
          0.9780261516571045,
          0.9699217677116394,
          0.9663562178611755,
          0.961185097694397,
          0.9667559266090393,
          0.9611374139785767,
          0.9596521258354187,
          0.9486783742904663,
          0.9521666765213013,
          0.9535461664199829,
          0.9659119844436646,
          0.950829267501831,
          0.9508211016654968,
          0.969225287437439,
          0.9477912783622742,
          0.9606309533119202,
          0.9881374835968018,
          0.9727091193199158,
          0.9616396427154541,
          0.9584828615188599,
          0.9695255160331726,
          0.997750461101532,
          0.969400942325592,
          0.9698310494422913,
          0.9801926016807556,
          0.9657670855522156,
          0.9479598999023438,
          0.9627678394317627,
          0.9487661123275757,
          0.9553781151771545,
          0.9635350108146667,
          0.951007604598999,
          0.952061116695404,
          0.957313597202301,
          0.9569167494773865,
          0.9763849973678589,
          0.9651952981948853,
          0.9603225588798523,
          0.9557874798774719,
          0.9705770015716553,
          0.9520983695983887,
          0.9613385200500488,
          0.9707191586494446,
          0.9415412545204163,
          0.9581917524337769,
          0.9615327715873718,
          0.9622096419334412,
          0.9386837482452393,
          0.9477691650390625,
          0.9660700559616089,
          0.9430115818977356,
          0.9666261672973633,
          0.9792112112045288,
          0.9440824389457703,
          0.9791069626808167,
          0.9650794267654419,
          0.9543132781982422,
          0.9614118337631226,
          0.9701216816902161,
          0.9446831345558167,
          0.9703139066696167,
          0.9554598927497864,
          0.9652685523033142,
          0.9498181939125061,
          0.9546635150909424,
          0.9604195356369019,
          0.9496654272079468,
          0.9728655815124512,
          0.9437683820724487,
          0.9615782499313354,
          0.944888710975647,
          0.9488314986228943,
          0.9539565443992615,
          0.978529691696167,
          0.9755080938339233,
          0.9732173085212708,
          0.9667103886604309,
          0.9638236165046692,
          0.9435973167419434,
          0.9596549868583679,
          0.9424965381622314,
          0.9385900497436523,
          0.9643229246139526,
          0.9672935009002686,
          0.9534434676170349,
          0.9627804756164551,
          0.9576640725135803,
          0.9510104656219482,
          0.9510226845741272,
          0.9551887512207031,
          0.958596408367157,
          0.9598071575164795,
          0.9469873905181885,
          0.9831506013870239,
          0.9414533972740173,
          0.967088520526886,
          0.9609547853469849,
          0.9792700409889221,
          0.9781535863876343,
          0.9393964409828186,
          0.9529505372047424,
          0.9390825629234314,
          0.9808070659637451,
          0.9508113861083984,
          0.9756333231925964,
          0.9705826640129089,
          0.9525706171989441,
          0.9635664224624634,
          0.9545497894287109,
          0.9657012820243835,
          0.9632035493850708,
          0.9445573091506958,
          0.9586645364761353,
          0.9501256942749023,
          0.9465193152427673,
          0.958507239818573,
          0.9435272812843323,
          0.9609890580177307,
          0.9470135569572449,
          0.937560498714447,
          0.9770167469978333,
          0.9581533670425415,
          0.9605467915534973,
          0.9494850039482117,
          0.965440034866333,
          0.9560900330543518,
          0.9499434232711792,
          0.9435086846351624,
          0.9615664482116699,
          0.96390700340271,
          0.9510223865509033,
          0.9461895823478699,
          0.9746244549751282,
          0.9586262702941895,
          0.9522191286087036,
          0.9554272294044495,
          0.9632427096366882,
          0.9583922028541565,
          0.9536643624305725,
          0.992864191532135,
          0.9588854312896729,
          0.9601004719734192,
          0.9467162489891052,
          0.9601144790649414,
          0.9493744373321533,
          0.9593631625175476,
          0.9728114604949951,
          0.948152482509613,
          0.950593113899231,
          0.9629442095756531,
          0.9601620435714722,
          0.9710401892662048,
          0.9387347102165222,
          0.9595609307289124,
          0.9581482410430908,
          0.9762331247329712,
          0.9344409704208374,
          0.9334683418273926,
          0.9420634508132935,
          0.9584081768989563,
          0.9427370429039001,
          0.9718271493911743,
          0.9642015099525452,
          0.9485228061676025,
          0.958923876285553,
          0.9594824314117432,
          0.9397052526473999,
          0.9563703536987305,
          0.9453052878379822,
          0.9413827061653137,
          0.9768713116645813,
          0.9408917427062988,
          0.9572927951812744,
          0.9814404249191284,
          0.9551404714584351,
          0.9490092992782593,
          0.9495135545730591,
          0.9436217546463013,
          0.9628890752792358,
          0.9408778548240662,
          0.9575403928756714,
          0.952983021736145,
          0.9441397190093994,
          0.9563934803009033,
          0.9444148540496826,
          0.9898183345794678,
          0.9457144141197205,
          0.9760812520980835,
          0.9754332304000854,
          0.9453334808349609,
          0.9409677386283875,
          0.9584221243858337,
          0.9438959956169128,
          0.9693704843521118,
          0.9397872090339661,
          0.9574511647224426,
          0.9572149515151978,
          0.954737663269043,
          0.9741672873497009,
          0.9393569827079773,
          0.9602073431015015,
          0.94446861743927,
          0.9588356018066406,
          0.9618807435035706,
          0.963291347026825,
          0.9716582894325256,
          0.9587260484695435,
          0.9724404215812683,
          0.951886773109436,
          0.94565749168396,
          0.9520034193992615,
          0.977242648601532,
          0.9565702080726624,
          0.9470159411430359,
          0.9700073003768921,
          0.9442593455314636,
          0.9423370957374573,
          0.9694319367408752,
          0.9569454193115234,
          0.962482213973999,
          0.9564891457557678,
          0.9455239772796631,
          0.9521963000297546,
          0.9480907320976257,
          0.96198970079422,
          0.939476490020752,
          0.9544605612754822,
          0.9595066905021667,
          0.9406794309616089,
          0.9575551748275757,
          0.9445697069168091,
          0.9566382765769958,
          0.9479499459266663,
          0.9622495770454407,
          0.9599314332008362,
          0.9585930109024048,
          0.9453751444816589,
          0.9710493683815002,
          0.9821857810020447
         ]
        },
        {
         "name": "Validation loss",
         "type": "scatter",
         "y": [
          1.470015525817871,
          1.393172264099121,
          1.2674696445465088,
          1.150222659111023,
          1.1316691637039185,
          1.0939397811889648,
          1.0956424474716187,
          1.1037455797195435,
          1.3909050226211548,
          1.2548091411590576,
          4.445820331573486,
          1.5280194282531738,
          1.4056097269058228,
          6.142787456512451,
          5.902354717254639,
          6.139100074768066,
          6.1372270584106445,
          1.1214244365692139,
          6.010545253753662,
          6.134527206420898,
          1.0934950113296509,
          1.0719181299209595,
          1.0804693698883057,
          1.0908544063568115,
          1.1242645978927612,
          3.6243724822998047,
          5.902947425842285,
          5.99205207824707,
          2.3193984031677246,
          1.4854134321212769,
          1.349447250366211,
          1.287172794342041,
          1.2408210039138794,
          1.1956591606140137,
          1.1556448936462402,
          1.13324773311615,
          1.118252158164978,
          1.1083316802978516,
          1.0953634977340698,
          1.0967751741409302,
          1.0902845859527588,
          1.0820273160934448,
          1.081013798713684,
          1.0851941108703613,
          1.0762343406677246,
          1.0796078443527222,
          1.0721232891082764,
          1.0677387714385986,
          1.0662367343902588,
          1.0660206079483032,
          1.066392421722412,
          1.0646106004714966,
          1.0630182027816772,
          1.0658559799194336,
          1.0663093328475952,
          1.0668026208877563,
          1.0860105752944946,
          1.093722939491272,
          1.182407021522522,
          1.1394907236099243,
          1.2907843589782715,
          1.6761165857315063,
          3.796178102493286,
          5.8918633460998535,
          5.985530376434326,
          5.890807151794434,
          5.985940456390381,
          5.989719390869141,
          5.988123893737793,
          5.985210418701172,
          5.994439601898193,
          5.996547222137451,
          5.985433578491211,
          6.106155872344971,
          6.105244159698486,
          6.007711410522461,
          6.103488922119141,
          6.102633476257324,
          6.101749420166016,
          5.987226963043213,
          5.992356777191162,
          6.0992302894592285,
          5.971100807189941,
          5.973243236541748,
          5.986715316772461,
          6.096182346343994,
          6.095190525054932,
          6.094383716583252,
          6.093655109405518,
          6.09278678894043,
          6.092087268829346,
          6.091177463531494,
          6.090470790863037,
          6.089710712432861,
          5.9834065437316895,
          6.088200569152832,
          5.989114761352539,
          5.964308738708496,
          5.971208095550537,
          6.085453033447266,
          6.084681987762451,
          6.083938121795654,
          6.083193302154541,
          6.082500457763672,
          6.08181095123291,
          6.081221580505371,
          6.0809125900268555,
          5.958072662353516,
          5.970279216766357,
          1.3849979639053345,
          1.0462889671325684,
          1.0392045974731445,
          1.0375595092773438,
          1.0432215929031372,
          1.059626817703247,
          1.127488613128662,
          1.2612415552139282,
          1.8661906719207764,
          1.9998221397399902,
          2.839266300201416,
          4.496657371520996,
          4.7458319664001465,
          5.1711554527282715,
          5.4740800857543945,
          5.826887130737305,
          5.94597053527832,
          5.945367813110352,
          5.945614337921143,
          5.947356224060059,
          5.948081970214844,
          5.947898864746094,
          5.948027610778809,
          5.950429916381836,
          5.955424785614014,
          5.970952987670898,
          5.951993465423584,
          5.95793342590332,
          6.067498207092285,
          6.066375255584717,
          6.066332817077637,
          6.065837860107422,
          6.065340518951416,
          6.064765453338623,
          6.064318656921387,
          6.063936710357666,
          6.063588619232178,
          6.063754081726074,
          6.063145637512207,
          6.0640482902526855,
          6.063292980194092,
          6.062914848327637,
          6.062442779541016,
          6.061618804931641,
          6.060852527618408,
          5.959645748138428,
          6.059281826019287,
          5.9484453201293945,
          5.942542552947998,
          5.943633556365967,
          5.940869331359863,
          5.939167499542236,
          5.953402996063232,
          6.057073593139648,
          6.056483745574951,
          6.056023597717285,
          6.055963516235352,
          6.055665016174316,
          6.055678844451904,
          6.055118560791016,
          6.0550432205200195,
          5.939349174499512,
          5.935261249542236,
          5.936230659484863,
          5.950680732727051,
          6.054614067077637,
          6.054293155670166,
          6.0539326667785645,
          6.053110599517822,
          6.05314826965332,
          6.053256511688232,
          6.053138732910156,
          6.05235481262207,
          5.9258222579956055,
          5.924859046936035,
          5.924737453460693,
          5.9258503913879395,
          5.928013324737549,
          5.931319713592529,
          5.9350457191467285,
          5.931431770324707,
          5.934613227844238,
          5.932368278503418,
          5.936527729034424,
          5.933460235595703,
          5.940751552581787,
          6.049586296081543,
          6.049220561981201,
          6.048882961273193,
          6.048645973205566,
          6.0484619140625,
          5.934019565582275,
          5.936460018157959,
          5.948282241821289,
          6.048068046569824,
          6.047746658325195,
          6.047789573669434,
          6.0477399826049805,
          6.047630786895752,
          6.047427654266357,
          6.046861171722412,
          6.0467143058776855,
          6.046295642852783,
          6.0459208488464355,
          6.0456223487854,
          6.0455732345581055,
          6.045612335205078,
          6.045324802398682,
          6.045238494873047,
          6.045172214508057,
          6.044897079467773,
          6.044581413269043,
          6.044261455535889,
          6.044222831726074,
          6.044195175170898,
          6.044471740722656,
          6.044732093811035,
          6.0443830490112305,
          6.044179916381836,
          6.044270992279053,
          6.044254779815674,
          6.043952941894531,
          6.043876647949219,
          6.044175148010254,
          6.043778419494629,
          6.043452739715576,
          6.043218612670898,
          6.043182373046875,
          6.043022155761719,
          6.043249607086182,
          6.043833255767822,
          5.958329200744629,
          5.9403181076049805,
          6.043227672576904,
          6.042843341827393,
          6.042766094207764,
          6.042545318603516,
          6.042516708374023,
          6.042491436004639,
          6.042254447937012,
          6.042140007019043,
          6.041906833648682,
          6.041527271270752,
          6.041380405426025,
          6.041073322296143,
          6.040970325469971,
          6.0408549308776855,
          6.040542125701904,
          6.040472030639648,
          6.040523052215576,
          6.040452480316162,
          6.040256023406982,
          6.0403571128845215,
          6.040191173553467,
          6.040134429931641,
          6.039875030517578,
          6.0397539138793945,
          6.039534568786621,
          6.039422988891602,
          6.039374351501465,
          6.039495468139648,
          6.0400471687316895,
          6.039875030517578,
          6.039728164672852,
          6.0395121574401855,
          6.03938102722168,
          6.03921365737915,
          6.0391106605529785,
          6.039008617401123,
          6.038949489593506,
          6.039027690887451,
          6.039000511169434,
          6.03897762298584,
          6.0387468338012695,
          6.038650989532471,
          6.038611888885498,
          6.038567066192627,
          6.0384345054626465,
          6.0385050773620605,
          6.038448333740234,
          6.038289546966553,
          6.03818416595459,
          6.038012504577637,
          6.037937641143799,
          6.0375471115112305,
          6.037491321563721,
          6.037510871887207,
          6.037457466125488,
          6.037346363067627,
          6.037256240844727,
          6.037208080291748,
          6.037169933319092,
          6.037074089050293,
          6.037057876586914,
          6.037010669708252,
          6.036937236785889,
          6.03692626953125,
          6.036898136138916,
          6.0368971824646,
          6.036874294281006,
          6.036808013916016,
          6.036820888519287,
          6.036723613739014,
          6.036736488342285,
          6.036736488342285,
          6.036700248718262,
          6.036642551422119,
          6.036655902862549,
          6.036562442779541,
          6.036552429199219,
          6.03648567199707,
          6.0364298820495605,
          6.036554336547852,
          6.036618709564209,
          6.036564350128174,
          6.036618709564209,
          6.036590099334717,
          6.0366129875183105,
          6.0366129875183105,
          6.036562442779541,
          6.036514759063721,
          6.036489963531494,
          6.03648567199707,
          6.036476135253906,
          6.036394119262695,
          6.0363240242004395,
          6.03629207611084,
          6.036204814910889,
          6.036378383636475,
          6.036457061767578,
          6.036427974700928,
          6.036408424377441,
          6.036258697509766,
          6.0362324714660645,
          6.036290168762207,
          6.036262512207031,
          6.0362958908081055,
          6.036201000213623,
          6.036172866821289,
          6.036172866821289,
          6.0361647605896,
          6.036035537719727,
          6.036041736602783,
          6.036042213439941,
          6.036056041717529,
          6.0360918045043945,
          6.0360493659973145,
          6.036021709442139,
          6.0360283851623535,
          6.035944938659668,
          6.035915374755859,
          6.0358405113220215,
          6.035773754119873,
          6.035864353179932,
          6.035848617553711,
          6.035901069641113,
          6.035949230194092,
          6.0358991622924805,
          6.035885334014893,
          6.035930633544922,
          6.0358195304870605,
          6.035784721374512,
          6.035826683044434,
          6.035817623138428,
          6.035807132720947,
          6.03592586517334,
          6.035829067230225,
          6.035874843597412,
          6.035830974578857,
          6.035848140716553,
          6.035862445831299,
          6.035842418670654,
          6.035822868347168,
          6.035790920257568,
          6.035761833190918,
          6.035823822021484,
          6.035834312438965,
          6.035801887512207,
          6.0358076095581055,
          6.035747051239014,
          6.035782337188721,
          6.03577995300293,
          6.035736560821533,
          6.0357255935668945,
          6.035742282867432,
          6.035682678222656,
          6.035698413848877,
          6.035756587982178,
          6.03573751449585,
          6.035543441772461,
          6.035555362701416,
          6.035548686981201,
          6.035548210144043,
          6.035593509674072,
          6.035670280456543,
          6.03568172454834,
          6.035646915435791,
          6.035633563995361,
          6.035650730133057,
          6.035686016082764,
          6.035651206970215,
          6.035633087158203,
          6.035679340362549,
          6.0356669425964355,
          6.035661697387695,
          6.035644054412842,
          6.035619735717773,
          6.0356316566467285,
          6.035584449768066,
          6.035591125488281,
          6.035582542419434,
          6.035623550415039,
          6.035583019256592,
          6.035543441772461,
          6.0355916023254395,
          6.035611629486084,
          6.035613536834717,
          6.035550594329834,
          6.035517692565918,
          6.035537242889404,
          6.035519599914551,
          6.035516738891602,
          6.035530090332031,
          6.035515785217285,
          6.035534858703613,
          6.035525798797607,
          6.03555154800415,
          6.035571098327637,
          6.035603046417236,
          6.0356221199035645,
          6.035625457763672,
          6.035593509674072,
          6.035579204559326,
          6.035584926605225,
          6.0355658531188965,
          6.035562515258789,
          6.035567760467529,
          6.035548210144043,
          6.035562515258789,
          6.035589218139648,
          6.035533428192139,
          6.035577297210693,
          6.035575866699219,
          6.035526275634766,
          6.035523891448975,
          6.035531520843506,
          6.035540580749512,
          6.035556316375732,
          6.035553455352783,
          6.035580158233643,
          6.035519599914551,
          6.035505294799805,
          6.035520076751709,
          6.03555965423584,
          6.035604000091553,
          6.035629749298096,
          6.0356221199035645,
          6.035590648651123,
          6.035611629486084,
          6.035591125488281,
          6.035576820373535,
          6.0355753898620605,
          6.035597324371338,
          6.035530090332031,
          6.035569667816162,
          6.035572052001953,
          6.035588264465332,
          6.035597801208496,
          6.035536766052246,
          6.0355353355407715,
          6.035528182983398,
          6.035536766052246,
          6.035505771636963,
          6.035444736480713,
          6.035484313964844,
          6.03549337387085,
          6.035491943359375,
          6.035475730895996,
          6.035579681396484,
          6.035524845123291,
          6.035487651824951,
          6.035503387451172,
          6.035477638244629,
          6.035469055175781,
          6.035520076751709,
          6.035519123077393,
          6.035489082336426,
          6.035421371459961,
          6.035426616668701,
          6.035455226898193,
          6.035437107086182
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Loss CNN"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train accuracy",
         "type": "scatter",
         "y": [
          0.5025176405906677,
          0.5005035400390625,
          0.49546828866004944,
          0.5307149887084961,
          0.5005035400390625,
          0.5156092643737793,
          0.5095669627189636,
          0.5478348731994629,
          0.5377643704414368,
          0.5518630146980286,
          0.5548841953277588,
          0.5327290892601013,
          0.5528700947761536,
          0.5256797671318054,
          0.5276938676834106,
          0.5458207726478577,
          0.5659617185592651,
          0.5518630146980286,
          0.5125881433486938,
          0.5609264969825745,
          0.5105740427970886,
          0.5196374654769897,
          0.5357502698898315,
          0.5367572903633118,
          0.5417925715446472,
          0.5790534019470215,
          0.556898295879364,
          0.5850956439971924,
          0.5156092643737793,
          0.49748238921165466,
          0.4944612383842468,
          0.48942598700523376,
          0.4934541881084442,
          0.4924471378326416,
          0.5095669627189636,
          0.5176233649253845,
          0.4743202328681946,
          0.5075528621673584,
          0.5256797671318054,
          0.5196374654769897,
          0.5105740427970886,
          0.538771390914917,
          0.539778470993042,
          0.5105740427970886,
          0.5478348731994629,
          0.5468277931213379,
          0.5508559942245483,
          0.5377643704414368,
          0.5417925715446472,
          0.5589123964309692,
          0.5629405975341797,
          0.5287008881568909,
          0.5488418936729431,
          0.555891215801239,
          0.5297079682350159,
          0.5538771152496338,
          0.555891215801239,
          0.5730111002922058,
          0.5458207726478577,
          0.5629405975341797,
          0.5709969997406006,
          0.5659617185592651,
          0.5830815434455872,
          0.5810674428939819,
          0.6072507500648499,
          0.5891238451004028,
          0.5861027240753174,
          0.5689828991889954,
          0.5871097445487976,
          0.5971802473068237,
          0.6032225489616394,
          0.5961732268333435,
          0.5649546980857849,
          0.6082578301429749,
          0.599194347858429,
          0.6042296290397644,
          0.6143000721931458,
          0.5931520462036133,
          0.5971802473068237,
          0.5881168246269226,
          0.5961732268333435,
          0.6042296290397644,
          0.599194347858429,
          0.5961732268333435,
          0.6082578301429749,
          0.5961732268333435,
          0.6414904594421387,
          0.600201427936554,
          0.6062437295913696,
          0.5971802473068237,
          0.6032225489616394,
          0.617321252822876,
          0.6072507500648499,
          0.6012084484100342,
          0.6273917555809021,
          0.6213494539260864,
          0.6203423738479614,
          0.5971802473068237,
          0.5871097445487976,
          0.599194347858429,
          0.6143000721931458,
          0.6092648506164551,
          0.6032225489616394,
          0.6203423738479614,
          0.6213494539260864,
          0.617321252822876,
          0.6092648506164551,
          0.5981873273849487,
          0.6193353533744812,
          0.5881168246269226,
          0.5347431898117065,
          0.539778470993042,
          0.5538771152496338,
          0.5709969997406006,
          0.5730111002922058,
          0.6022155284881592,
          0.5911379456520081,
          0.575025200843811,
          0.5911379456520081,
          0.617321252822876,
          0.6243705749511719,
          0.6102719306945801,
          0.6233635544776917,
          0.6082578301429749,
          0.6273917555809021,
          0.6223564743995667,
          0.616314172744751,
          0.6153071522712708,
          0.6012084484100342,
          0.6203423738479614,
          0.6314199566841125,
          0.6294058561325073,
          0.6092648506164551,
          0.6253776550292969,
          0.6132930517196655,
          0.6203423738479614,
          0.599194347858429,
          0.6032225489616394,
          0.6213494539260864,
          0.6263846755027771,
          0.6183282732963562,
          0.634441077709198,
          0.6223564743995667,
          0.6153071522712708,
          0.6062437295913696,
          0.6112789511680603,
          0.6304128766059875,
          0.6283987760543823,
          0.6102719306945801,
          0.6273917555809021,
          0.6122860312461853,
          0.6092648506164551,
          0.6203423738479614,
          0.6203423738479614,
          0.6193353533744812,
          0.6283987760543823,
          0.6283987760543823,
          0.6314199566841125,
          0.6253776550292969,
          0.6143000721931458,
          0.6203423738479614,
          0.6374622583389282,
          0.65156090259552,
          0.6253776550292969,
          0.6143000721931458,
          0.6364551782608032,
          0.6203423738479614,
          0.6183282732963562,
          0.6243705749511719,
          0.6283987760543823,
          0.6122860312461853,
          0.6183282732963562,
          0.6324269771575928,
          0.6294058561325073,
          0.6283987760543823,
          0.6143000721931458,
          0.6213494539260864,
          0.6364551782608032,
          0.6243705749511719,
          0.6122860312461853,
          0.617321252822876,
          0.6414904594421387,
          0.634441077709198,
          0.616314172744751,
          0.6394763588905334,
          0.6304128766059875,
          0.6052366495132446,
          0.6576032042503357,
          0.6435045599937439,
          0.6273917555809021,
          0.6455186009407043,
          0.6475327014923096,
          0.6273917555809021,
          0.6243705749511719,
          0.6273917555809021,
          0.6334340572357178,
          0.6334340572357178,
          0.6183282732963562,
          0.6233635544776917,
          0.6294058561325073,
          0.6273917555809021,
          0.6273917555809021,
          0.6304128766059875,
          0.635448157787323,
          0.6314199566841125,
          0.6314199566841125,
          0.6243705749511719,
          0.634441077709198,
          0.6203423738479614,
          0.634441077709198,
          0.6435045599937439,
          0.6455186009407043,
          0.6283987760543823,
          0.6233635544776917,
          0.6253776550292969,
          0.635448157787323,
          0.6384692788124084,
          0.6253776550292969,
          0.6445115804672241,
          0.6384692788124084,
          0.616314172744751,
          0.6193353533744812,
          0.6243705749511719,
          0.6283987760543823,
          0.6283987760543823,
          0.6495468020439148,
          0.6243705749511719,
          0.6243705749511719,
          0.6364551782608032,
          0.6384692788124084,
          0.635448157787323,
          0.6435045599937439,
          0.6435045599937439,
          0.6203423738479614,
          0.6314199566841125,
          0.6455186009407043,
          0.6304128766059875,
          0.6535750031471252,
          0.6153071522712708,
          0.6233635544776917,
          0.6203423738479614,
          0.6485397815704346,
          0.6364551782608032,
          0.6334340572357178,
          0.6102719306945801,
          0.6283987760543823,
          0.6283987760543823,
          0.6304128766059875,
          0.6394763588905334,
          0.6042296290397644,
          0.6213494539260864,
          0.6243705749511719,
          0.6374622583389282,
          0.6424974799156189,
          0.6283987760543823,
          0.6414904594421387,
          0.6404833793640137,
          0.6203423738479614,
          0.6445115804672241,
          0.6143000721931458,
          0.6193353533744812,
          0.6364551782608032,
          0.6314199566841125,
          0.6374622583389282,
          0.6374622583389282,
          0.6233635544776917,
          0.617321252822876,
          0.6324269771575928,
          0.634441077709198,
          0.6294058561325073,
          0.6314199566841125,
          0.6384692788124084,
          0.6374622583389282,
          0.6384692788124084,
          0.6052366495132446,
          0.6314199566841125,
          0.634441077709198,
          0.6324269771575928,
          0.6424974799156189,
          0.6153071522712708,
          0.6122860312461853,
          0.635448157787323,
          0.6324269771575928,
          0.6404833793640137,
          0.6314199566841125,
          0.6314199566841125,
          0.6384692788124084,
          0.6263846755027771,
          0.6233635544776917,
          0.6253776550292969,
          0.6223564743995667,
          0.635448157787323,
          0.6334340572357178,
          0.6253776550292969,
          0.6294058561325073,
          0.635448157787323,
          0.6314199566841125,
          0.652567982673645,
          0.6364551782608032,
          0.6404833793640137,
          0.6304128766059875,
          0.6324269771575928,
          0.6435045599937439,
          0.65156090259552,
          0.6535750031471252,
          0.6294058561325073,
          0.635448157787323,
          0.6304128766059875,
          0.6374622583389282,
          0.6394763588905334,
          0.6424974799156189,
          0.6535750031471252,
          0.6253776550292969,
          0.6273917555809021,
          0.6253776550292969,
          0.6253776550292969,
          0.6273917555809021,
          0.6404833793640137,
          0.6223564743995667,
          0.6243705749511719,
          0.6394763588905334,
          0.6223564743995667,
          0.6203423738479614,
          0.6324269771575928,
          0.6435045599937439,
          0.6394763588905334,
          0.6213494539260864,
          0.6193353533744812,
          0.634441077709198,
          0.6283987760543823,
          0.6374622583389282,
          0.6414904594421387,
          0.6263846755027771,
          0.6394763588905334,
          0.6243705749511719,
          0.6424974799156189,
          0.634441077709198,
          0.6324269771575928,
          0.6193353533744812,
          0.6122860312461853,
          0.6404833793640137,
          0.6314199566841125,
          0.6374622583389282,
          0.6384692788124084,
          0.6263846755027771,
          0.6193353533744812,
          0.6213494539260864,
          0.6334340572357178,
          0.6576032042503357,
          0.6122860312461853,
          0.6273917555809021,
          0.6324269771575928,
          0.6223564743995667,
          0.6384692788124084,
          0.6243705749511719,
          0.6404833793640137,
          0.6143000721931458,
          0.6253776550292969,
          0.6475327014923096,
          0.6475327014923096,
          0.6263846755027771,
          0.6283987760543823,
          0.6283987760543823,
          0.6283987760543823,
          0.6243705749511719,
          0.6404833793640137,
          0.6364551782608032,
          0.6404833793640137,
          0.6294058561325073,
          0.6324269771575928,
          0.6183282732963562,
          0.6314199566841125,
          0.6273917555809021,
          0.6384692788124084,
          0.6253776550292969,
          0.6273917555809021,
          0.6455186009407043,
          0.635448157787323,
          0.6243705749511719,
          0.6253776550292969,
          0.634441077709198,
          0.6314199566841125,
          0.6505538821220398,
          0.6203423738479614,
          0.6374622583389282,
          0.6283987760543823,
          0.634441077709198,
          0.6455186009407043,
          0.6253776550292969,
          0.6384692788124084,
          0.6233635544776917,
          0.6153071522712708,
          0.6243705749511719,
          0.6304128766059875,
          0.6223564743995667,
          0.6384692788124084,
          0.6414904594421387,
          0.6283987760543823,
          0.6314199566841125,
          0.6223564743995667,
          0.6445115804672241,
          0.6435045599937439,
          0.652567982673645,
          0.635448157787323,
          0.6364551782608032,
          0.6263846755027771,
          0.6304128766059875,
          0.6243705749511719,
          0.6253776550292969,
          0.6324269771575928,
          0.6294058561325073,
          0.6394763588905334,
          0.6193353533744812,
          0.6404833793640137,
          0.6253776550292969,
          0.6364551782608032,
          0.634441077709198,
          0.6435045599937439,
          0.635448157787323,
          0.6263846755027771,
          0.6364551782608032,
          0.6112789511680603,
          0.6394763588905334,
          0.6294058561325073,
          0.6143000721931458,
          0.634441077709198,
          0.6445115804672241,
          0.6334340572357178,
          0.6364551782608032,
          0.6223564743995667,
          0.634441077709198,
          0.635448157787323,
          0.6505538821220398,
          0.65156090259552,
          0.6283987760543823,
          0.616314172744751,
          0.6445115804672241,
          0.6304128766059875,
          0.6455186009407043,
          0.6374622583389282,
          0.6283987760543823,
          0.6626384854316711,
          0.6314199566841125,
          0.6183282732963562,
          0.6102719306945801,
          0.6495468020439148,
          0.6394763588905334,
          0.6475327014923096,
          0.635448157787323,
          0.6374622583389282,
          0.6213494539260864,
          0.6213494539260864,
          0.6273917555809021,
          0.6455186009407043,
          0.6404833793640137,
          0.6404833793640137,
          0.6314199566841125,
          0.6223564743995667,
          0.6364551782608032,
          0.6314199566841125,
          0.6445115804672241,
          0.6404833793640137,
          0.6213494539260864,
          0.6455186009407043,
          0.6374622583389282,
          0.6414904594421387,
          0.6424974799156189,
          0.6414904594421387,
          0.6223564743995667,
          0.6424974799156189,
          0.6424974799156189,
          0.6283987760543823,
          0.635448157787323,
          0.6374622583389282,
          0.6213494539260864,
          0.6294058561325073,
          0.6384692788124084,
          0.6374622583389282,
          0.6485397815704346,
          0.634441077709198,
          0.6283987760543823,
          0.6364551782608032,
          0.6364551782608032,
          0.6404833793640137,
          0.6435045599937439,
          0.6545820832252502,
          0.6445115804672241,
          0.6414904594421387,
          0.634441077709198,
          0.617321252822876,
          0.6374622583389282,
          0.6294058561325073,
          0.6334340572357178,
          0.6314199566841125,
          0.6304128766059875,
          0.6132930517196655,
          0.6314199566841125,
          0.6435045599937439,
          0.6374622583389282,
          0.634441077709198
         ]
        },
        {
         "name": "Validation accuracy",
         "type": "scatter",
         "y": [
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.6272727251052856,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.48181816935539246,
          0.6454545259475708,
          0.6454545259475708,
          0.3636363744735718,
          0.3909091055393219,
          0.3545454442501068,
          0.3545454442501068,
          0.6454545259475708,
          0.37272727489471436,
          0.3545454442501068,
          0.6363636255264282,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.4636363685131073,
          0.40909090638160706,
          0.3636363744735718,
          0.5272727012634277,
          0.37272727489471436,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.5636363625526428,
          0.6272727251052856,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6454545259475708,
          0.6181818246841431,
          0.5727272629737854,
          0.37272727489471436,
          0.37272727489471436,
          0.3636363744735718,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.34545454382896423,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3636363744735718,
          0.3545454442501068,
          0.3545454442501068,
          0.3636363744735718,
          0.3545454442501068,
          0.3636363744735718,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3636363744735718,
          0.3636363744735718,
          0.37272727489471436,
          0.3545454442501068,
          0.6454545259475708,
          0.6272727251052856,
          0.6272727251052856,
          0.6454545259475708,
          0.6272727251052856,
          0.6272727251052856,
          0.6363636255264282,
          0.6272727251052856,
          0.6363636255264282,
          0.5909090638160706,
          0.4909090995788574,
          0.4363636374473572,
          0.4363636374473572,
          0.4000000059604645,
          0.4000000059604645,
          0.40909090638160706,
          0.38181817531585693,
          0.37272727489471436,
          0.3636363744735718,
          0.3636363744735718,
          0.37272727489471436,
          0.37272727489471436,
          0.3636363744735718,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3545454442501068,
          0.3636363744735718,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3545454442501068,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3545454442501068,
          0.3636363744735718,
          0.3636363744735718,
          0.3545454442501068,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.37272727489471436,
          0.38181817531585693,
          0.37272727489471436,
          0.3636363744735718,
          0.3636363744735718,
          0.3545454442501068,
          0.3545454442501068,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3545454442501068,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3545454442501068,
          0.3545454442501068,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3545454442501068,
          0.3545454442501068,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718,
          0.3636363744735718
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Accuracy CNN"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Accuracy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train F1 score",
         "type": "scatter",
         "y": [
          0.39600324630737305,
          0.3371089994907379,
          0.4370015859603882,
          0.5247769355773926,
          0.49873387813568115,
          0.48712921142578125,
          0.47165441513061523,
          0.5464937686920166,
          0.5318082571029663,
          0.5340795516967773,
          0.5479483604431152,
          0.5265986919403076,
          0.5527390241622925,
          0.4896039068698883,
          0.5105202198028564,
          0.5458189249038696,
          0.5620371103286743,
          0.5419613122940063,
          0.3679683804512024,
          0.5566964149475098,
          0.42712080478668213,
          0.4706781506538391,
          0.5341610312461853,
          0.5346387624740601,
          0.5138664245605469,
          0.5760917663574219,
          0.5495298504829407,
          0.5777468681335449,
          0.4293873906135559,
          0.33221250772476196,
          0.3377889394760132,
          0.352984756231308,
          0.3881220817565918,
          0.38377660512924194,
          0.3785080313682556,
          0.4753073453903198,
          0.42113226652145386,
          0.4753541946411133,
          0.5234450101852417,
          0.5176337361335754,
          0.46991950273513794,
          0.5387334823608398,
          0.539462685585022,
          0.4860292077064514,
          0.5478348731994629,
          0.5463857054710388,
          0.5486768484115601,
          0.5377174615859985,
          0.5367107391357422,
          0.5582308769226074,
          0.5523123741149902,
          0.49920254945755005,
          0.5262162685394287,
          0.549309492111206,
          0.5212482213973999,
          0.5394079685211182,
          0.5534111261367798,
          0.5661360025405884,
          0.5458041429519653,
          0.5628869533538818,
          0.56536865234375,
          0.5634043216705322,
          0.5772121548652649,
          0.574963390827179,
          0.6065129041671753,
          0.5811512470245361,
          0.5843762159347534,
          0.568664014339447,
          0.587039053440094,
          0.5964924097061157,
          0.6014079451560974,
          0.5961666107177734,
          0.5510097742080688,
          0.6012110114097595,
          0.5962358713150024,
          0.6030557155609131,
          0.6133967638015747,
          0.5883726477622986,
          0.5944820642471313,
          0.5835487842559814,
          0.5947908163070679,
          0.5987540483474731,
          0.5975745916366577,
          0.5874985456466675,
          0.6063013672828674,
          0.5925217866897583,
          0.6374366283416748,
          0.5977193117141724,
          0.5993773937225342,
          0.5934593081474304,
          0.5916736125946045,
          0.6098957657814026,
          0.6039243936538696,
          0.5966991186141968,
          0.62177574634552,
          0.6173912286758423,
          0.6183358430862427,
          0.5893639326095581,
          0.5775994062423706,
          0.5937456488609314,
          0.6122615933418274,
          0.6070230603218079,
          0.5976366996765137,
          0.6149044632911682,
          0.6160187721252441,
          0.6154645681381226,
          0.6055004596710205,
          0.5933773517608643,
          0.614341139793396,
          0.5782344937324524,
          0.43968743085861206,
          0.46370184421539307,
          0.5021790266036987,
          0.5431889295578003,
          0.5485821962356567,
          0.5874272584915161,
          0.5836693048477173,
          0.5674980878829956,
          0.5829664468765259,
          0.6143609881401062,
          0.6213972568511963,
          0.6085425615310669,
          0.620964527130127,
          0.6044047474861145,
          0.6226769685745239,
          0.6196346282958984,
          0.6148126125335693,
          0.6146502494812012,
          0.6007674932479858,
          0.6195955276489258,
          0.6301141381263733,
          0.6277109980583191,
          0.6078805923461914,
          0.6246738433837891,
          0.6124973297119141,
          0.6182222366333008,
          0.5966413617134094,
          0.6000094413757324,
          0.6200079917907715,
          0.6244101524353027,
          0.6159586906433105,
          0.6315474510192871,
          0.6178358793258667,
          0.6119030117988586,
          0.6031268835067749,
          0.6081310510635376,
          0.6278753280639648,
          0.6257204413414001,
          0.605779767036438,
          0.6225032806396484,
          0.6096239686012268,
          0.6067760586738586,
          0.6167604327201843,
          0.6176060438156128,
          0.615667998790741,
          0.626091718673706,
          0.6257204413414001,
          0.6277196407318115,
          0.620103657245636,
          0.6085839867591858,
          0.6147158145904541,
          0.6341134309768677,
          0.6494531631469727,
          0.623664379119873,
          0.6124829053878784,
          0.6341981887817383,
          0.6186577677726746,
          0.6159586906433105,
          0.621259331703186,
          0.6262092590332031,
          0.6093559265136719,
          0.6128615140914917,
          0.624753475189209,
          0.6255318522453308,
          0.627376914024353,
          0.6119055151939392,
          0.6204252243041992,
          0.6349387168884277,
          0.6230846643447876,
          0.6094915866851807,
          0.6147578954696655,
          0.6390889883041382,
          0.6305428743362427,
          0.6050082445144653,
          0.6294931173324585,
          0.6231343150138855,
          0.5975767374038696,
          0.6535809636116028,
          0.6400700211524963,
          0.6236509084701538,
          0.6408680081367493,
          0.6423970460891724,
          0.6239498853683472,
          0.6189903020858765,
          0.6236509084701538,
          0.6303294897079468,
          0.6313309669494629,
          0.616422176361084,
          0.621536135673523,
          0.6277109980583191,
          0.6243743300437927,
          0.6252540349960327,
          0.6270711421966553,
          0.63300621509552,
          0.6291902661323547,
          0.6286993026733398,
          0.619874119758606,
          0.63127601146698,
          0.6167604327201843,
          0.6316784620285034,
          0.6404852867126465,
          0.6436971426010132,
          0.625590443611145,
          0.6197350025177002,
          0.623664379119873,
          0.632757306098938,
          0.6361074447631836,
          0.6231114864349365,
          0.642632782459259,
          0.6359870433807373,
          0.6139321327209473,
          0.6167855262756348,
          0.6224945783615112,
          0.6262092590332031,
          0.626091718673706,
          0.6479440927505493,
          0.6231721639633179,
          0.6222729086875916,
          0.6343131065368652,
          0.6357370615005493,
          0.6319360136985779,
          0.6418741941452026,
          0.6419704556465149,
          0.6178621053695679,
          0.6296316385269165,
          0.6436971426010132,
          0.627351701259613,
          0.6512545347213745,
          0.6120490431785583,
          0.6200307011604309,
          0.616452693939209,
          0.6438456773757935,
          0.6328794360160828,
          0.6307283043861389,
          0.6065949201583862,
          0.6253209710121155,
          0.6263235807418823,
          0.628567099571228,
          0.637179970741272,
          0.6006510257720947,
          0.6179987788200378,
          0.6211183667182922,
          0.6346578598022461,
          0.639536440372467,
          0.6265429258346558,
          0.6389681100845337,
          0.6384756565093994,
          0.6173371076583862,
          0.6410151124000549,
          0.6115202307701111,
          0.6161113977432251,
          0.6331679821014404,
          0.6286993026733398,
          0.6343917846679688,
          0.6345263719558716,
          0.6205834150314331,
          0.6144965887069702,
          0.629903256893158,
          0.6308455467224121,
          0.6264047622680664,
          0.6267561912536621,
          0.6346139907836914,
          0.6325337886810303,
          0.6339777112007141,
          0.6009430885314941,
          0.6290722489356995,
          0.6303868293762207,
          0.6288116574287415,
          0.6389812231063843,
          0.6120490431785583,
          0.6078171730041504,
          0.6317882537841797,
          0.6288116574287415,
          0.6376375555992126,
          0.6282983422279358,
          0.6286993026733398,
          0.6357370615005493,
          0.6232901811599731,
          0.6207136511802673,
          0.6228682994842529,
          0.6201313138008118,
          0.6326282024383545,
          0.6312166452407837,
          0.6216166019439697,
          0.6264047622680664,
          0.6317882537841797,
          0.6274112462997437,
          0.6498178243637085,
          0.6327304840087891,
          0.6378922462463379,
          0.6284595727920532,
          0.6295174360275269,
          0.6412338018417358,
          0.64922696352005,
          0.6511377096176147,
          0.6255318522453308,
          0.632757306098938,
          0.6283489465713501,
          0.6339695453643799,
          0.637179970741272,
          0.6405010223388672,
          0.6499536633491516,
          0.6237655878067017,
          0.6245094537734985,
          0.6231114864349365,
          0.6229914426803589,
          0.6258858442306519,
          0.6383650302886963,
          0.6190868616104126,
          0.6228036880493164,
          0.6362861394882202,
          0.6193671226501465,
          0.6174731850624084,
          0.628959059715271,
          0.6415674686431885,
          0.6365567445755005,
          0.6173912286758423,
          0.6163907051086426,
          0.6327186822891235,
          0.6257204413414001,
          0.6353822350502014,
          0.6388441324234009,
          0.6245188117027283,
          0.6370614171028137,
          0.6211183667182922,
          0.639536440372467,
          0.6319311857223511,
          0.6296491026878357,
          0.6158190369606018,
          0.6097530722618103,
          0.6378922462463379,
          0.6278690695762634,
          0.6346578598022461,
          0.6358636021614075,
          0.6232901811599731,
          0.6169106364250183,
          0.6192919015884399,
          0.6317576169967651,
          0.6558437347412109,
          0.6093559265136719,
          0.6240944862365723,
          0.629903256893158,
          0.6184873580932617,
          0.6358636021614075,
          0.6206760406494141,
          0.6372326016426086,
          0.6113852262496948,
          0.622343897819519,
          0.6454005837440491,
          0.6457215547561646,
          0.6235611438751221,
          0.624591588973999,
          0.6259711384773254,
          0.6254572868347168,
          0.6221572756767273,
          0.6392526626586914,
          0.6341981887817383,
          0.6378922462463379,
          0.6270452737808228,
          0.6297777891159058,
          0.6145743131637573,
          0.6282983422279358,
          0.6245094537734985,
          0.6357370615005493,
          0.6227419376373291,
          0.6234967112541199,
          0.6434848308563232,
          0.6320806741714478,
          0.6220385432243347,
          0.622343897819519,
          0.6315474510192871,
          0.6286993026733398,
          0.6483843922615051,
          0.6173371076583862,
          0.6343917846679688,
          0.625847339630127,
          0.6315474510192871,
          0.6432607173919678,
          0.6232283115386963,
          0.6366637349128723,
          0.620964527130127,
          0.6128568053245544,
          0.6215318441390991,
          0.6284595727920532,
          0.6190868616104126,
          0.6353391408920288,
          0.6387172341346741,
          0.625847339630127,
          0.6285687685012817,
          0.6189417839050293,
          0.6416976451873779,
          0.6408730745315552,
          0.6501824855804443,
          0.6336793899536133,
          0.6330253481864929,
          0.6230064034461975,
          0.6277490854263306,
          0.6208266615867615,
          0.6228682994842529,
          0.6300256252288818,
          0.6269234418869019,
          0.6360030174255371,
          0.6167855262756348,
          0.6373707056045532,
          0.622343897819519,
          0.6346396207809448,
          0.6322870850563049,
          0.6406176090240479,
          0.6335748434066772,
          0.6234272718429565,
          0.6339591145515442,
          0.6073764562606812,
          0.6365567445755005,
          0.6262671947479248,
          0.6101870536804199,
          0.6321715116500854,
          0.6410151124000549,
          0.6296021342277527,
          0.6330253481864929,
          0.6196346282958984,
          0.6321715116500854,
          0.6323606371879578,
          0.6480352878570557,
          0.6480628252029419,
          0.6247438192367554,
          0.6134145259857178,
          0.6412972807884216,
          0.6288712024688721,
          0.6429020762443542,
          0.6350337862968445,
          0.626091718673706,
          0.6602069735527039,
          0.6282983422279358,
          0.616422176361084,
          0.607463002204895,
          0.6469600200653076,
          0.6364229917526245,
          0.6446783542633057,
          0.6326282024383545,
          0.6339695453643799,
          0.6175479888916016,
          0.6190589070320129,
          0.6239498853683472,
          0.6419598460197449,
          0.6369472742080688,
          0.6385833024978638,
          0.6284351348876953,
          0.6193671226501465,
          0.6337078213691711,
          0.6274112462997437,
          0.642526388168335,
          0.6378922462463379,
          0.6179987788200378,
          0.6423818469047546,
          0.6338225603103638,
          0.6385871171951294,
          0.6405010223388672,
          0.6387172341346741,
          0.6201313138008118,
          0.6403909921646118,
          0.6402779817581177,
          0.6244361400604248,
          0.6320806741714478,
          0.6347862482070923,
          0.618420422077179,
          0.6258354783058167,
          0.6358636021614075,
          0.6341134309768677,
          0.6461266875267029,
          0.6314133405685425,
          0.6257204413414001,
          0.6331679821014404,
          0.6335774660110474,
          0.6364960670471191,
          0.6412338018417358,
          0.6518478393554688,
          0.6418250799179077,
          0.6390889883041382,
          0.6318063735961914,
          0.6140801906585693,
          0.6345263719558716,
          0.6273921728134155,
          0.6297539472579956,
          0.6289510726928711,
          0.6277490854263306,
          0.6107027530670166,
          0.6284351348876953,
          0.6408730745315552,
          0.633362889289856,
          0.6326153874397278
         ]
        },
        {
         "name": "Validation F1 score",
         "type": "scatter",
         "y": [
          0.2617449462413788,
          0.2617449462413788,
          0.2617449462413788,
          0.2617449462413788,
          0.4273015856742859,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.47971123456954956,
          0.3922652006149292,
          0.3922652006149292,
          0.27740243077278137,
          0.3383606970310211,
          0.2617449462413788,
          0.2617449462413788,
          0.3922652006149292,
          0.30206894874572754,
          0.2617449462413788,
          0.3888888657093048,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.45051223039627075,
          0.3649524748325348,
          0.2873010039329529,
          0.4773391783237457,
          0.29270339012145996,
          0.2617449462413788,
          0.2617449462413788,
          0.2617449462413788,
          0.2617449462413788,
          0.2617449462413788,
          0.2617449462413788,
          0.4380587339401245,
          0.40743663907051086,
          0.3922652006149292,
          0.41545167565345764,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.3922652006149292,
          0.40340912342071533,
          0.5453346371650696,
          0.3106893002986908,
          0.30206894874572754,
          0.29641813039779663,
          0.2722020149230957,
          0.2722020149230957,
          0.2722020149230957,
          0.2722020149230957,
          0.2722020149230957,
          0.2722020149230957,
          0.2722020149230957,
          0.2617449462413788,
          0.2617449462413788,
          0.2722020149230957,
          0.2617449462413788,
          0.2617449462413788,
          0.2617449462413788,
          0.2722020149230957,
          0.2567567825317383,
          0.2617449462413788,
          0.2722020149230957,
          0.2722020149230957,
          0.2722020149230957,
          0.27740243077278137,
          0.2617449462413788,
          0.2617449462413788,
          0.27740243077278137,
          0.2617449462413788,
          0.27740243077278137,
          0.2617449462413788,
          0.2617449462413788,
          0.2617449462413788,
          0.2722020149230957,
          0.2617449462413788,
          0.2722020149230957,
          0.2722020149230957,
          0.2722020149230957,
          0.2617449462413788,
          0.2617449462413788,
          0.2617449462413788,
          0.2617449462413788,
          0.2617449462413788,
          0.2617449462413788,
          0.27740243077278137,
          0.27740243077278137,
          0.30206894874572754,
          0.2722020149230957,
          0.43634212017059326,
          0.4273015856742859,
          0.4273015856742859,
          0.43634212017059326,
          0.38547483086586,
          0.38547483086586,
          0.3888888657093048,
          0.38547483086586,
          0.48235294222831726,
          0.4672263264656067,
          0.44999998807907104,
          0.4356173276901245,
          0.4333665668964386,
          0.3834918439388275,
          0.3700104057788849,
          0.3649524748325348,
          0.31652048230171204,
          0.30206894874572754,
          0.2873010039329529,
          0.2873010039329529,
          0.30206894874572754,
          0.30206894874572754,
          0.2873010039329529,
          0.2722020149230957,
          0.2722020149230957,
          0.2722020149230957,
          0.2722020149230957,
          0.2722020149230957,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.2722020149230957,
          0.27740243077278137,
          0.2722020149230957,
          0.2722020149230957,
          0.2722020149230957,
          0.2722020149230957,
          0.2722020149230957,
          0.2722020149230957,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.2722020149230957,
          0.2873010039329529,
          0.2873010039329529,
          0.2722020149230957,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.30206894874572754,
          0.31652048230171204,
          0.30206894874572754,
          0.2873010039329529,
          0.2873010039329529,
          0.2722020149230957,
          0.2722020149230957,
          0.2873010039329529,
          0.2873010039329529,
          0.2873010039329529,
          0.2722020149230957,
          0.2873010039329529,
          0.2873010039329529,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.2873010039329529,
          0.2722020149230957,
          0.2722020149230957,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.2722020149230957,
          0.2722020149230957,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137,
          0.27740243077278137
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Accuracy CNN"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "F1 score"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'..\\\\..\\\\Outputs\\\\COVID\\\\LR\\\\NuovoTest_Not_Kfold\\\\GRAPHS\\\\training ML_df_U F1 score.html'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLOT HISTORY\n",
    "plt.clf()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['loss'],\n",
    "                    name='Train loss'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_loss'],\n",
    "                    name='Validation loss'))\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Loss CNN',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='Loss')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training ML_df_U loss.html\"))), auto_open=False)\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['categorical_accuracy'],\n",
    "                    name='Train accuracy'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_categorical_accuracy'],\n",
    "                    name='Validation accuracy'))\n",
    "\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Accuracy CNN',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='Accuracy')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training ML_df_U accuracy.html\"))), auto_open=False)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['f1_score'],\n",
    "                    name='Train F1 score'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_f1_score'],\n",
    "                    name='Validation F1 score'))\n",
    "\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Accuracy CNN',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='F1 score')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training ML_df_U F1 score.html\"))), auto_open=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "Ground truth of the validation\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 3.7962 - categorical_accuracy: 0.5727 - f1_score: 0.5453\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "[1 0 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0\n",
      " 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1\n",
      " 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0]\n",
      "[0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0\n",
      " 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1]\n",
      "accuracy:  0.5727272727272728\n",
      "log_loss:  8.579599094390868\n",
      "[[18 21]\n",
      " [26 45]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.40909   0.46154   0.43373        39\n",
      "           1    0.68182   0.63380   0.65693        71\n",
      "\n",
      "    accuracy                        0.57273       110\n",
      "   macro avg    0.54545   0.54767   0.54533       110\n",
      "weighted avg    0.58512   0.57273   0.57780       110\n",
      "\n",
      "{'loss': 3.796178102493286, 'categorical_accuracy': 0.5727272629737854, 'f1_score': 0.5453346371650696}\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "Ground truth of the validation\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 1.0939 - categorical_accuracy: 0.6455 - f1_score: 0.3923\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "4/4 [==============================] - 0s 24ms/step\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0\n",
      " 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1]\n",
      "accuracy:  0.6454545454545455\n",
      "log_loss:  0.6931471824645996\n",
      "[[ 0 39]\n",
      " [ 0 71]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.00000   0.00000   0.00000        39\n",
      "           1    0.64545   1.00000   0.78453        71\n",
      "\n",
      "    accuracy                        0.64545       110\n",
      "   macro avg    0.32273   0.50000   0.39227       110\n",
      "weighted avg    0.41661   0.64545   0.50638       110\n",
      "\n",
      "{'loss': 1.0939397811889648, 'categorical_accuracy': 0.6454545259475708, 'f1_score': 0.3922652006149292}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Michele\\anaconda3\\envs\\tfNew2023_2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\Michele\\anaconda3\\envs\\tfNew2023_2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\Michele\\anaconda3\\envs\\tfNew2023_2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## EVALUATION\n",
    "gc.collect()\n",
    "\n",
    "bestPt = checkpointPath / Path('ML_DF_U_Classification_F1')\n",
    "model = tf.keras.models.load_model(bestPt)\n",
    "\n",
    "print(\"Ground truth of the validation\")\n",
    "\n",
    "evaluation = model.evaluate(x = X_val_IMGS, y = Y_val)\n",
    "prediction = model.predict(X_val_IMGS)\n",
    "\n",
    "ypreds = np.argmax(prediction, axis=1)\n",
    "yground = np.argmax(Y_val.to_numpy(), axis=1)\n",
    "# yground = np.argmax(targets, axis=1)\n",
    "\n",
    "print(ypreds)\n",
    "print(yground)\n",
    "\n",
    "\n",
    "\n",
    "score = accuracy_score(yground, ypreds)\n",
    "print('accuracy: ', score)\n",
    "\n",
    "loss = log_loss(yground, prediction)\n",
    "print('log_loss: ', loss)\n",
    "\n",
    "# Print the confusion matrix\n",
    "cmatrix = confusion_matrix(yground, ypreds)\n",
    "print(cmatrix)\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "creport = classification_report(yground, ypreds, digits=5)\n",
    "print(creport)\n",
    "\n",
    "\n",
    "evaluation = dict(zip(model.metrics_names,evaluation))\n",
    "\n",
    "# FF_f1_values.append(evaluation['f1_score'])\n",
    "\n",
    "\n",
    "\n",
    "print(evaluation)\n",
    "with open(str(evalspath) + 'ML_DF_U_F1_EVALUATION.txt', 'w') as f:\n",
    "    f.write('CNN FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    for k, v in evaluation.items():\n",
    "        f.write(str(k) + ' >>> '+ str(v) + '\\n\\n')\n",
    "    f.write('CNN FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    f.write('SKLEARN METRICS:')\n",
    "    f.write(str('Loss') + ' >>> '+ str(loss) + '\\n\\n')\n",
    "    f.write(str('Accuracy') + ' >>> '+ str(score) + '\\n\\n')\n",
    "    f.write(str('Confusion Matrix') + ' >>> \\n'+ str(cmatrix) + '\\n\\n')\n",
    "    f.write(str('CLASSIFICATION REPORT WITH CLASSES') + ' >>> \\n'+ str(creport) + '\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bestPt = checkpointPath / Path('ML_DF_U_Classification_acc')\n",
    "model = tf.keras.models.load_model(bestPt)\n",
    "\n",
    "print(\"Ground truth of the validation\")\n",
    "\n",
    "evaluation = model.evaluate(x = X_val_IMGS, y = Y_val)\n",
    "prediction = model.predict(X_val_IMGS)\n",
    "\n",
    "ypreds = np.argmax(prediction, axis=1)\n",
    "yground = np.argmax(Y_val.to_numpy(), axis=1)\n",
    "# yground = np.argmax(targets, axis=1)\n",
    "\n",
    "print(ypreds)\n",
    "print(yground)\n",
    "\n",
    "\n",
    "\n",
    "score = accuracy_score(yground, ypreds)\n",
    "print('accuracy: ', score)\n",
    "\n",
    "loss = log_loss(yground, prediction)\n",
    "print('log_loss: ', loss)\n",
    "\n",
    "\n",
    "# Print the confusion matrix\n",
    "cmatrix = confusion_matrix(yground, ypreds)\n",
    "print(cmatrix)\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "creport = classification_report(yground, ypreds, digits=5)\n",
    "print(creport)\n",
    "\n",
    "\n",
    "# FF_accuracy_values.append(score)\n",
    "\n",
    "evaluation = dict(zip(model.metrics_names,evaluation))\n",
    "print(evaluation)\n",
    "with open(str(evalspath) + 'ML_DF_U_acc_EVALUATION.txt', 'w') as f:\n",
    "    f.write('CNN FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    for k, v in evaluation.items():\n",
    "        f.write(str(k) + ' >>> '+ str(v) + '\\n\\n')\n",
    "    f.write('CNN FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    f.write('SKLEARN METRICS:')\n",
    "    f.write(str('Loss') + ' >>> '+ str(loss) + '\\n\\n')\n",
    "    f.write(str('Accuracy') + ' >>> '+ str(score) + '\\n\\n')\n",
    "    f.write(str('Confusion Matrix') + ' >>> \\n'+ str(cmatrix) + '\\n\\n')\n",
    "    f.write(str('CLASSIFICATION REPORT WITH CLASSES') + ' >>> \\n'+ str(creport) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### addestramento EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficientnet.tfkeras as efn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficientnetCreate(inputShape):\n",
    "        # inputs = layers.Input(shape=(inputShape[0], inputShape[1], 3))\n",
    "        efficientnet = efn.EfficientNetB1(weights='imagenet', include_top=False, input_shape=(inputShape[0], inputShape[1], 3))\n",
    "        # efficientnet = tf.keras.applications.efficientnet_v2.EfficientNetV2L(\n",
    "        #     include_top=False,\n",
    "        #     input_tensor=inputs,\n",
    "        #     weights='imagenet',\n",
    "        # )\n",
    "\n",
    "        # Freeze the pretrained weights\n",
    "        efficientnet.trainable = False\n",
    "        # Rebuild top\n",
    "        x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(efficientnet.output)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        top_dropout_rate = 0.4\n",
    "        x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "        x = layers.Dense(512, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(top_dropout_rate)(x)\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        # x = layers.Dropout(top_dropout_rate)(x)\n",
    "        x = layers.Dense(16, activation=\"relu\")(x)\n",
    "        x = layers.Dense(4, activation=\"relu\")(x)\n",
    "        outputs = layers.Dense(2, activation=\"sigmoid\", name=\"pred\")(x)\n",
    "\n",
    "        efficientnet = tf.keras.Model(efficientnet.input, outputs, name=\"EfficientNet\")\n",
    "\n",
    "        return efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet\n",
      "Epoch 1/500\n",
      " 5/31 [===>..........................] - ETA: 0s - loss: 0.9666 - binary_accuracy: 0.4938 - f1_score: 0.3305WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0156s vs `on_train_batch_end` time: 0.0447s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0156s vs `on_train_batch_end` time: 0.0447s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - ETA: 0s - loss: 0.9770 - binary_accuracy: 0.4849 - f1_score: 0.3265"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 32s 974ms/step - loss: 0.9770 - binary_accuracy: 0.4849 - f1_score: 0.3265 - val_loss: 0.7038 - val_binary_accuracy: 0.4865 - val_f1_score: 0.3273\n",
      "Epoch 2/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.9067 - binary_accuracy: 0.4849 - f1_score: 0.3265 - val_loss: 0.6993 - val_binary_accuracy: 0.4865 - val_f1_score: 0.3273\n",
      "Epoch 3/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.8672 - binary_accuracy: 0.4859 - f1_score: 0.3287 - val_loss: 0.6983 - val_binary_accuracy: 0.4865 - val_f1_score: 0.3273\n",
      "Epoch 4/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.8270 - binary_accuracy: 0.4849 - f1_score: 0.3265 - val_loss: 0.6980 - val_binary_accuracy: 0.4865 - val_f1_score: 0.3273\n",
      "Epoch 5/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.8125 - binary_accuracy: 0.4859 - f1_score: 0.3287 - val_loss: 0.6979 - val_binary_accuracy: 0.4865 - val_f1_score: 0.3273\n",
      "Epoch 6/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.7988 - binary_accuracy: 0.4844 - f1_score: 0.3261 - val_loss: 0.6976 - val_binary_accuracy: 0.4865 - val_f1_score: 0.3273\n",
      "Epoch 7/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.7740 - binary_accuracy: 0.4879 - f1_score: 0.3330 - val_loss: 0.6970 - val_binary_accuracy: 0.4865 - val_f1_score: 0.3273\n",
      "Epoch 8/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.7747 - binary_accuracy: 0.4849 - f1_score: 0.3283 - val_loss: 0.6964 - val_binary_accuracy: 0.4865 - val_f1_score: 0.3273\n",
      "Epoch 9/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.7640 - binary_accuracy: 0.4864 - f1_score: 0.3309 - val_loss: 0.6956 - val_binary_accuracy: 0.4865 - val_f1_score: 0.3273\n",
      "Epoch 10/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.7592 - binary_accuracy: 0.4894 - f1_score: 0.3406 - val_loss: 0.6952 - val_binary_accuracy: 0.4865 - val_f1_score: 0.3273\n",
      "Epoch 11/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.7431 - binary_accuracy: 0.4869 - f1_score: 0.3309 - val_loss: 0.6944 - val_binary_accuracy: 0.4820 - val_f1_score: 0.3273\n",
      "Epoch 12/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.7360 - binary_accuracy: 0.4859 - f1_score: 0.3326 - val_loss: 0.6936 - val_binary_accuracy: 0.4685 - val_f1_score: 0.3190\n",
      "Epoch 13/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.7236 - binary_accuracy: 0.4945 - f1_score: 0.3543 - val_loss: 0.6921 - val_binary_accuracy: 0.4685 - val_f1_score: 0.3190\n",
      "Epoch 14/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.7349 - binary_accuracy: 0.4814 - f1_score: 0.3340 - val_loss: 0.6905 - val_binary_accuracy: 0.4640 - val_f1_score: 0.3148\n",
      "Epoch 15/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.7383 - binary_accuracy: 0.4869 - f1_score: 0.3371"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 483ms/step - loss: 0.7383 - binary_accuracy: 0.4869 - f1_score: 0.3371 - val_loss: 0.6892 - val_binary_accuracy: 0.4595 - val_f1_score: 0.3286\n",
      "Epoch 16/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.7333 - binary_accuracy: 0.4808 - f1_score: 0.3335"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 481ms/step - loss: 0.7333 - binary_accuracy: 0.4808 - f1_score: 0.3335 - val_loss: 0.6877 - val_binary_accuracy: 0.4775 - val_f1_score: 0.3633\n",
      "Epoch 17/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.7140 - binary_accuracy: 0.4904 - f1_score: 0.3471"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 29s 959ms/step - loss: 0.7140 - binary_accuracy: 0.4904 - f1_score: 0.3471 - val_loss: 0.6864 - val_binary_accuracy: 0.5045 - val_f1_score: 0.4217\n",
      "Epoch 18/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.7146 - binary_accuracy: 0.4904 - f1_score: 0.3502"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 29s 956ms/step - loss: 0.7146 - binary_accuracy: 0.4904 - f1_score: 0.3502 - val_loss: 0.6851 - val_binary_accuracy: 0.5180 - val_f1_score: 0.4367\n",
      "Epoch 19/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.7104 - binary_accuracy: 0.4970 - f1_score: 0.3703"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 30s 975ms/step - loss: 0.7104 - binary_accuracy: 0.4970 - f1_score: 0.3703 - val_loss: 0.6844 - val_binary_accuracy: 0.5405 - val_f1_score: 0.4795\n",
      "Epoch 20/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.7206 - binary_accuracy: 0.4869 - f1_score: 0.3611"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 30s 986ms/step - loss: 0.7206 - binary_accuracy: 0.4869 - f1_score: 0.3611 - val_loss: 0.6842 - val_binary_accuracy: 0.5495 - val_f1_score: 0.4998\n",
      "Epoch 21/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.7117 - binary_accuracy: 0.5005 - f1_score: 0.3719 - val_loss: 0.6837 - val_binary_accuracy: 0.5495 - val_f1_score: 0.4998\n",
      "Epoch 22/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.7156 - binary_accuracy: 0.4950 - f1_score: 0.3742"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 28s 936ms/step - loss: 0.7156 - binary_accuracy: 0.4950 - f1_score: 0.3742 - val_loss: 0.6836 - val_binary_accuracy: 0.5721 - val_f1_score: 0.5382\n",
      "Epoch 23/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.7074 - binary_accuracy: 0.5000 - f1_score: 0.3843 - val_loss: 0.6832 - val_binary_accuracy: 0.5586 - val_f1_score: 0.5310\n",
      "Epoch 24/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.7030 - binary_accuracy: 0.4980 - f1_score: 0.3826 - val_loss: 0.6825 - val_binary_accuracy: 0.5541 - val_f1_score: 0.5238\n",
      "Epoch 25/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6876 - binary_accuracy: 0.5035 - f1_score: 0.3855 - val_loss: 0.6820 - val_binary_accuracy: 0.5541 - val_f1_score: 0.5238\n",
      "Epoch 26/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6961 - binary_accuracy: 0.5045 - f1_score: 0.3982 - val_loss: 0.6817 - val_binary_accuracy: 0.5541 - val_f1_score: 0.5238\n",
      "Epoch 27/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6905 - binary_accuracy: 0.4985 - f1_score: 0.3858 - val_loss: 0.6811 - val_binary_accuracy: 0.5676 - val_f1_score: 0.5359\n",
      "Epoch 28/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6955 - binary_accuracy: 0.5005 - f1_score: 0.4000 - val_loss: 0.6806 - val_binary_accuracy: 0.5495 - val_f1_score: 0.5212\n",
      "Epoch 29/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6911 - binary_accuracy: 0.5202 - f1_score: 0.4246 - val_loss: 0.6803 - val_binary_accuracy: 0.5450 - val_f1_score: 0.5329\n",
      "Epoch 30/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6900 - binary_accuracy: 0.5111 - f1_score: 0.4161 - val_loss: 0.6801 - val_binary_accuracy: 0.5450 - val_f1_score: 0.5255\n",
      "Epoch 31/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6922 - binary_accuracy: 0.5307 - f1_score: 0.4501 - val_loss: 0.6796 - val_binary_accuracy: 0.5541 - val_f1_score: 0.5369\n",
      "Epoch 32/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6898 - binary_accuracy: 0.5222 - f1_score: 0.4413 - val_loss: 0.6790 - val_binary_accuracy: 0.5405 - val_f1_score: 0.5293\n",
      "Epoch 33/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6850 - binary_accuracy: 0.5302 - f1_score: 0.4498"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 487ms/step - loss: 0.6850 - binary_accuracy: 0.5302 - f1_score: 0.4498 - val_loss: 0.6784 - val_binary_accuracy: 0.5495 - val_f1_score: 0.5405\n",
      "Epoch 34/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6912 - binary_accuracy: 0.5186 - f1_score: 0.4322"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 483ms/step - loss: 0.6912 - binary_accuracy: 0.5186 - f1_score: 0.4322 - val_loss: 0.6781 - val_binary_accuracy: 0.5586 - val_f1_score: 0.5515\n",
      "Epoch 35/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6889 - binary_accuracy: 0.5257 - f1_score: 0.4518 - val_loss: 0.6780 - val_binary_accuracy: 0.5541 - val_f1_score: 0.5437\n",
      "Epoch 36/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6874 - binary_accuracy: 0.5242 - f1_score: 0.4457"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 483ms/step - loss: 0.6874 - binary_accuracy: 0.5242 - f1_score: 0.4457 - val_loss: 0.6776 - val_binary_accuracy: 0.5631 - val_f1_score: 0.5545\n",
      "Epoch 37/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6850 - binary_accuracy: 0.5267 - f1_score: 0.4639 - val_loss: 0.6769 - val_binary_accuracy: 0.5631 - val_f1_score: 0.5466\n",
      "Epoch 38/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6921 - binary_accuracy: 0.5227 - f1_score: 0.4508 - val_loss: 0.6765 - val_binary_accuracy: 0.5586 - val_f1_score: 0.5466\n",
      "Epoch 39/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6840 - binary_accuracy: 0.5348 - f1_score: 0.4699 - val_loss: 0.6760 - val_binary_accuracy: 0.5586 - val_f1_score: 0.5466\n",
      "Epoch 40/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6806 - binary_accuracy: 0.5600 - f1_score: 0.5073"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 500ms/step - loss: 0.6806 - binary_accuracy: 0.5600 - f1_score: 0.5073 - val_loss: 0.6749 - val_binary_accuracy: 0.5631 - val_f1_score: 0.5572\n",
      "Epoch 41/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6922 - binary_accuracy: 0.5504 - f1_score: 0.4898 - val_loss: 0.6744 - val_binary_accuracy: 0.5676 - val_f1_score: 0.5572\n",
      "Epoch 42/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6844 - binary_accuracy: 0.5499 - f1_score: 0.4937"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 28s 932ms/step - loss: 0.6844 - binary_accuracy: 0.5499 - f1_score: 0.4937 - val_loss: 0.6738 - val_binary_accuracy: 0.5766 - val_f1_score: 0.5676\n",
      "Epoch 43/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6833 - binary_accuracy: 0.5469 - f1_score: 0.4966"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 497ms/step - loss: 0.6833 - binary_accuracy: 0.5469 - f1_score: 0.4966 - val_loss: 0.6731 - val_binary_accuracy: 0.5811 - val_f1_score: 0.5676\n",
      "Epoch 44/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6786 - binary_accuracy: 0.5469 - f1_score: 0.4944"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 488ms/step - loss: 0.6786 - binary_accuracy: 0.5469 - f1_score: 0.4944 - val_loss: 0.6723 - val_binary_accuracy: 0.5811 - val_f1_score: 0.5697\n",
      "Epoch 45/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6853 - binary_accuracy: 0.5428 - f1_score: 0.4976"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 29s 955ms/step - loss: 0.6853 - binary_accuracy: 0.5428 - f1_score: 0.4976 - val_loss: 0.6714 - val_binary_accuracy: 0.5901 - val_f1_score: 0.5798\n",
      "Epoch 46/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6799 - binary_accuracy: 0.5570 - f1_score: 0.5111"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 29s 970ms/step - loss: 0.6799 - binary_accuracy: 0.5570 - f1_score: 0.5111 - val_loss: 0.6709 - val_binary_accuracy: 0.5991 - val_f1_score: 0.5898\n",
      "Epoch 47/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6746 - binary_accuracy: 0.5711 - f1_score: 0.5286"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 29s 953ms/step - loss: 0.6746 - binary_accuracy: 0.5711 - f1_score: 0.5286 - val_loss: 0.6707 - val_binary_accuracy: 0.6126 - val_f1_score: 0.6094\n",
      "Epoch 48/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6760 - binary_accuracy: 0.5489 - f1_score: 0.5048 - val_loss: 0.6700 - val_binary_accuracy: 0.6126 - val_f1_score: 0.6094\n",
      "Epoch 49/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6828 - binary_accuracy: 0.5454 - f1_score: 0.4941 - val_loss: 0.6695 - val_binary_accuracy: 0.6126 - val_f1_score: 0.6094\n",
      "Epoch 50/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6740 - binary_accuracy: 0.5585 - f1_score: 0.5206 - val_loss: 0.6689 - val_binary_accuracy: 0.6126 - val_f1_score: 0.6094\n",
      "Epoch 51/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6741 - binary_accuracy: 0.5640 - f1_score: 0.5319 - val_loss: 0.6685 - val_binary_accuracy: 0.6126 - val_f1_score: 0.6094\n",
      "Epoch 52/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6766 - binary_accuracy: 0.5570 - f1_score: 0.5178 - val_loss: 0.6678 - val_binary_accuracy: 0.6126 - val_f1_score: 0.6094\n",
      "Epoch 53/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6807 - binary_accuracy: 0.5675 - f1_score: 0.5288"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 491ms/step - loss: 0.6807 - binary_accuracy: 0.5675 - f1_score: 0.5288 - val_loss: 0.6670 - val_binary_accuracy: 0.6171 - val_f1_score: 0.6094\n",
      "Epoch 54/500\n",
      "30/31 [============================>.] - ETA: 0s - loss: 0.6809 - binary_accuracy: 0.5641 - f1_score: 0.5295"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 28s 936ms/step - loss: 0.6812 - binary_accuracy: 0.5630 - f1_score: 0.5272 - val_loss: 0.6664 - val_binary_accuracy: 0.6306 - val_f1_score: 0.6382\n",
      "Epoch 55/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6665 - binary_accuracy: 0.5781 - f1_score: 0.5435 - val_loss: 0.6656 - val_binary_accuracy: 0.6306 - val_f1_score: 0.6382\n",
      "Epoch 56/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6732 - binary_accuracy: 0.5595 - f1_score: 0.5254"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 14s 482ms/step - loss: 0.6732 - binary_accuracy: 0.5595 - f1_score: 0.5254 - val_loss: 0.6647 - val_binary_accuracy: 0.6396 - val_f1_score: 0.6382\n",
      "Epoch 57/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6789 - binary_accuracy: 0.5721 - f1_score: 0.5416 - val_loss: 0.6640 - val_binary_accuracy: 0.6396 - val_f1_score: 0.6382\n",
      "Epoch 58/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6713 - binary_accuracy: 0.5811 - f1_score: 0.5534 - val_loss: 0.6638 - val_binary_accuracy: 0.6396 - val_f1_score: 0.6382\n",
      "Epoch 59/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6712 - binary_accuracy: 0.5590 - f1_score: 0.5302"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 29s 955ms/step - loss: 0.6712 - binary_accuracy: 0.5590 - f1_score: 0.5302 - val_loss: 0.6632 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6476\n",
      "Epoch 60/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6773 - binary_accuracy: 0.5615 - f1_score: 0.5330"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 29s 939ms/step - loss: 0.6773 - binary_accuracy: 0.5615 - f1_score: 0.5330 - val_loss: 0.6625 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 61/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6663 - binary_accuracy: 0.5872 - f1_score: 0.5601 - val_loss: 0.6622 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 62/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6701 - binary_accuracy: 0.5610 - f1_score: 0.5364"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 489ms/step - loss: 0.6701 - binary_accuracy: 0.5610 - f1_score: 0.5364 - val_loss: 0.6619 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 63/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6607 - binary_accuracy: 0.5811 - f1_score: 0.5590"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 29s 960ms/step - loss: 0.6607 - binary_accuracy: 0.5811 - f1_score: 0.5590 - val_loss: 0.6615 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6666\n",
      "Epoch 64/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6635 - binary_accuracy: 0.6023 - f1_score: 0.5790 - val_loss: 0.6607 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6574\n",
      "Epoch 65/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6636 - binary_accuracy: 0.5847 - f1_score: 0.5635 - val_loss: 0.6604 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6666\n",
      "Epoch 66/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6691 - binary_accuracy: 0.5897 - f1_score: 0.5732 - val_loss: 0.6599 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6666\n",
      "Epoch 67/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6637 - binary_accuracy: 0.5998 - f1_score: 0.5842"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 494ms/step - loss: 0.6637 - binary_accuracy: 0.5998 - f1_score: 0.5842 - val_loss: 0.6596 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6756\n",
      "Epoch 68/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6595 - binary_accuracy: 0.5907 - f1_score: 0.5707 - val_loss: 0.6588 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6666\n",
      "Epoch 69/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6554 - binary_accuracy: 0.5892 - f1_score: 0.5657"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 513ms/step - loss: 0.6554 - binary_accuracy: 0.5892 - f1_score: 0.5657 - val_loss: 0.6582 - val_binary_accuracy: 0.6712 - val_f1_score: 0.6756\n",
      "Epoch 70/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6555 - binary_accuracy: 0.6235 - f1_score: 0.6122 - val_loss: 0.6573 - val_binary_accuracy: 0.6712 - val_f1_score: 0.6754\n",
      "Epoch 71/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6546 - binary_accuracy: 0.5791 - f1_score: 0.5589"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 499ms/step - loss: 0.6546 - binary_accuracy: 0.5791 - f1_score: 0.5589 - val_loss: 0.6563 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6754\n",
      "Epoch 72/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6545 - binary_accuracy: 0.6220 - f1_score: 0.6072 - val_loss: 0.6557 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6754\n",
      "Epoch 73/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6579 - binary_accuracy: 0.5842 - f1_score: 0.5639 - val_loss: 0.6553 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6754\n",
      "Epoch 74/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6493 - binary_accuracy: 0.6290 - f1_score: 0.6112"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 504ms/step - loss: 0.6493 - binary_accuracy: 0.6290 - f1_score: 0.6112 - val_loss: 0.6541 - val_binary_accuracy: 0.6847 - val_f1_score: 0.6750\n",
      "Epoch 75/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6553 - binary_accuracy: 0.6129 - f1_score: 0.5976"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 495ms/step - loss: 0.6553 - binary_accuracy: 0.6129 - f1_score: 0.5976 - val_loss: 0.6534 - val_binary_accuracy: 0.6847 - val_f1_score: 0.6843\n",
      "Epoch 76/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6561 - binary_accuracy: 0.6265 - f1_score: 0.6099 - val_loss: 0.6530 - val_binary_accuracy: 0.6802 - val_f1_score: 0.6843\n",
      "Epoch 77/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6637 - binary_accuracy: 0.6084 - f1_score: 0.5966 - val_loss: 0.6522 - val_binary_accuracy: 0.6802 - val_f1_score: 0.6754\n",
      "Epoch 78/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6553 - binary_accuracy: 0.6169 - f1_score: 0.6048 - val_loss: 0.6516 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6754\n",
      "Epoch 79/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6592 - binary_accuracy: 0.5887 - f1_score: 0.5749 - val_loss: 0.6513 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6756\n",
      "Epoch 80/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6537 - binary_accuracy: 0.6240 - f1_score: 0.6093"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 494ms/step - loss: 0.6537 - binary_accuracy: 0.6240 - f1_score: 0.6093 - val_loss: 0.6518 - val_binary_accuracy: 0.6802 - val_f1_score: 0.6846\n",
      "Epoch 81/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6522 - binary_accuracy: 0.6129 - f1_score: 0.6059"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 115). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\efficientnet_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 29s 954ms/step - loss: 0.6522 - binary_accuracy: 0.6129 - f1_score: 0.6059 - val_loss: 0.6513 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7027\n",
      "Epoch 82/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6589 - binary_accuracy: 0.6099 - f1_score: 0.5959 - val_loss: 0.6515 - val_binary_accuracy: 0.6937 - val_f1_score: 0.6847\n",
      "Epoch 83/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6429 - binary_accuracy: 0.6411 - f1_score: 0.6375 - val_loss: 0.6514 - val_binary_accuracy: 0.6847 - val_f1_score: 0.6846\n",
      "Epoch 84/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6554 - binary_accuracy: 0.6094 - f1_score: 0.5989 - val_loss: 0.6510 - val_binary_accuracy: 0.6847 - val_f1_score: 0.6846\n",
      "Epoch 85/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6494 - binary_accuracy: 0.6124 - f1_score: 0.6011 - val_loss: 0.6508 - val_binary_accuracy: 0.6712 - val_f1_score: 0.6756\n",
      "Epoch 86/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6619 - binary_accuracy: 0.6164 - f1_score: 0.6109 - val_loss: 0.6508 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6756\n",
      "Epoch 87/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6510 - binary_accuracy: 0.6230 - f1_score: 0.6146 - val_loss: 0.6503 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6756\n",
      "Epoch 88/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6476 - binary_accuracy: 0.6144 - f1_score: 0.6073 - val_loss: 0.6496 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6756\n",
      "Epoch 89/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6487 - binary_accuracy: 0.6290 - f1_score: 0.6229 - val_loss: 0.6492 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6756\n",
      "Epoch 90/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6469 - binary_accuracy: 0.6381 - f1_score: 0.6297 - val_loss: 0.6487 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6756\n",
      "Epoch 91/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6474 - binary_accuracy: 0.6457 - f1_score: 0.6401 - val_loss: 0.6481 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6756\n",
      "Epoch 92/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6552 - binary_accuracy: 0.6275 - f1_score: 0.6171 - val_loss: 0.6475 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6576\n",
      "Epoch 93/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6442 - binary_accuracy: 0.6371 - f1_score: 0.6302 - val_loss: 0.6475 - val_binary_accuracy: 0.6712 - val_f1_score: 0.6667\n",
      "Epoch 94/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6529 - binary_accuracy: 0.6436 - f1_score: 0.6371 - val_loss: 0.6475 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6576\n",
      "Epoch 95/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6516 - binary_accuracy: 0.6321 - f1_score: 0.6291 - val_loss: 0.6480 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6666\n",
      "Epoch 96/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6466 - binary_accuracy: 0.6406 - f1_score: 0.6365 - val_loss: 0.6480 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6666\n",
      "Epoch 97/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6471 - binary_accuracy: 0.6220 - f1_score: 0.6135 - val_loss: 0.6478 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6666\n",
      "Epoch 98/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6438 - binary_accuracy: 0.6305 - f1_score: 0.6227 - val_loss: 0.6474 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6666\n",
      "Epoch 99/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6535 - binary_accuracy: 0.6391 - f1_score: 0.6345 - val_loss: 0.6473 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6666\n",
      "Epoch 100/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6479 - binary_accuracy: 0.6436 - f1_score: 0.6341 - val_loss: 0.6476 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6754\n",
      "Epoch 101/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6364 - binary_accuracy: 0.6452 - f1_score: 0.6384 - val_loss: 0.6468 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6754\n",
      "Epoch 102/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6399 - binary_accuracy: 0.6532 - f1_score: 0.6467 - val_loss: 0.6471 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6666\n",
      "Epoch 103/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6483 - binary_accuracy: 0.6421 - f1_score: 0.6367 - val_loss: 0.6477 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 104/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6397 - binary_accuracy: 0.6336 - f1_score: 0.6265 - val_loss: 0.6478 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6662\n",
      "Epoch 105/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6449 - binary_accuracy: 0.6547 - f1_score: 0.6520 - val_loss: 0.6480 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6574\n",
      "Epoch 106/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6447 - binary_accuracy: 0.6295 - f1_score: 0.6247 - val_loss: 0.6483 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 107/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6337 - binary_accuracy: 0.6643 - f1_score: 0.6635 - val_loss: 0.6484 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6574\n",
      "Epoch 108/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6409 - binary_accuracy: 0.6613 - f1_score: 0.6563 - val_loss: 0.6484 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6574\n",
      "Epoch 109/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6400 - binary_accuracy: 0.6643 - f1_score: 0.6613 - val_loss: 0.6484 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6485\n",
      "Epoch 110/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6403 - binary_accuracy: 0.6557 - f1_score: 0.6505 - val_loss: 0.6481 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6485\n",
      "Epoch 111/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6351 - binary_accuracy: 0.6542 - f1_score: 0.6522 - val_loss: 0.6478 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6485\n",
      "Epoch 112/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.6379 - binary_accuracy: 0.6648 - f1_score: 0.6594 - val_loss: 0.6483 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6485\n",
      "Epoch 113/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6350 - binary_accuracy: 0.6578 - f1_score: 0.6551 - val_loss: 0.6486 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6485\n",
      "Epoch 114/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6374 - binary_accuracy: 0.6477 - f1_score: 0.6425 - val_loss: 0.6486 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6574\n",
      "Epoch 115/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6361 - binary_accuracy: 0.6381 - f1_score: 0.6323 - val_loss: 0.6481 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6574\n",
      "Epoch 116/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6367 - binary_accuracy: 0.6421 - f1_score: 0.6390 - val_loss: 0.6478 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6574\n",
      "Epoch 117/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6295 - binary_accuracy: 0.6497 - f1_score: 0.6444 - val_loss: 0.6475 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6574\n",
      "Epoch 118/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6356 - binary_accuracy: 0.6618 - f1_score: 0.6611 - val_loss: 0.6474 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6574\n",
      "Epoch 119/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6474 - binary_accuracy: 0.6436 - f1_score: 0.6385 - val_loss: 0.6478 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6574\n",
      "Epoch 120/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6343 - binary_accuracy: 0.6507 - f1_score: 0.6425 - val_loss: 0.6481 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6574\n",
      "Epoch 121/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6310 - binary_accuracy: 0.6603 - f1_score: 0.6567 - val_loss: 0.6480 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6574\n",
      "Epoch 122/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6332 - binary_accuracy: 0.6588 - f1_score: 0.6524 - val_loss: 0.6481 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6574\n",
      "Epoch 123/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6310 - binary_accuracy: 0.6643 - f1_score: 0.6597 - val_loss: 0.6480 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6574\n",
      "Epoch 124/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6408 - binary_accuracy: 0.6593 - f1_score: 0.6585 - val_loss: 0.6483 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6574\n",
      "Epoch 125/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6409 - binary_accuracy: 0.6689 - f1_score: 0.6635 - val_loss: 0.6486 - val_binary_accuracy: 0.6712 - val_f1_score: 0.6662\n",
      "Epoch 126/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6349 - binary_accuracy: 0.6512 - f1_score: 0.6481 - val_loss: 0.6490 - val_binary_accuracy: 0.6712 - val_f1_score: 0.6662\n",
      "Epoch 127/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6409 - binary_accuracy: 0.6678 - f1_score: 0.6709 - val_loss: 0.6490 - val_binary_accuracy: 0.6712 - val_f1_score: 0.6750\n",
      "Epoch 128/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6401 - binary_accuracy: 0.6487 - f1_score: 0.6509 - val_loss: 0.6491 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6750\n",
      "Epoch 129/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6307 - binary_accuracy: 0.6799 - f1_score: 0.6786 - val_loss: 0.6496 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 130/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6293 - binary_accuracy: 0.6699 - f1_score: 0.6671 - val_loss: 0.6497 - val_binary_accuracy: 0.6712 - val_f1_score: 0.6657\n",
      "Epoch 131/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6334 - binary_accuracy: 0.6583 - f1_score: 0.6532 - val_loss: 0.6494 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 132/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6363 - binary_accuracy: 0.6744 - f1_score: 0.6727 - val_loss: 0.6490 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 133/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6284 - binary_accuracy: 0.6552 - f1_score: 0.6511 - val_loss: 0.6490 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 134/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6270 - binary_accuracy: 0.6557 - f1_score: 0.6522 - val_loss: 0.6490 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 135/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6314 - binary_accuracy: 0.6699 - f1_score: 0.6684 - val_loss: 0.6490 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 136/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6272 - binary_accuracy: 0.6835 - f1_score: 0.6838 - val_loss: 0.6494 - val_binary_accuracy: 0.6712 - val_f1_score: 0.6657\n",
      "Epoch 137/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6246 - binary_accuracy: 0.6643 - f1_score: 0.6602 - val_loss: 0.6491 - val_binary_accuracy: 0.6712 - val_f1_score: 0.6657\n",
      "Epoch 138/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6224 - binary_accuracy: 0.6890 - f1_score: 0.6835 - val_loss: 0.6487 - val_binary_accuracy: 0.6712 - val_f1_score: 0.6657\n",
      "Epoch 139/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.6300 - binary_accuracy: 0.6578 - f1_score: 0.6564 - val_loss: 0.6491 - val_binary_accuracy: 0.6712 - val_f1_score: 0.6744\n",
      "Epoch 140/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6251 - binary_accuracy: 0.6825 - f1_score: 0.6804 - val_loss: 0.6488 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 141/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6187 - binary_accuracy: 0.6779 - f1_score: 0.6786 - val_loss: 0.6494 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 142/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6215 - binary_accuracy: 0.6724 - f1_score: 0.6739 - val_loss: 0.6494 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 143/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6324 - binary_accuracy: 0.6527 - f1_score: 0.6514 - val_loss: 0.6493 - val_binary_accuracy: 0.6712 - val_f1_score: 0.6744\n",
      "Epoch 144/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6291 - binary_accuracy: 0.6754 - f1_score: 0.6730 - val_loss: 0.6491 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6657\n",
      "Epoch 145/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6260 - binary_accuracy: 0.6739 - f1_score: 0.6726 - val_loss: 0.6490 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6838\n",
      "Epoch 146/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6299 - binary_accuracy: 0.6784 - f1_score: 0.6759 - val_loss: 0.6492 - val_binary_accuracy: 0.6802 - val_f1_score: 0.6750\n",
      "Epoch 147/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6327 - binary_accuracy: 0.6603 - f1_score: 0.6601 - val_loss: 0.6494 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6838\n",
      "Epoch 148/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6270 - binary_accuracy: 0.6905 - f1_score: 0.6888 - val_loss: 0.6497 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 149/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6349 - binary_accuracy: 0.6452 - f1_score: 0.6422 - val_loss: 0.6497 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 150/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6227 - binary_accuracy: 0.6789 - f1_score: 0.6771 - val_loss: 0.6502 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 151/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6276 - binary_accuracy: 0.6694 - f1_score: 0.6684 - val_loss: 0.6502 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 152/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6190 - binary_accuracy: 0.6885 - f1_score: 0.6883 - val_loss: 0.6506 - val_binary_accuracy: 0.6802 - val_f1_score: 0.6744\n",
      "Epoch 153/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6313 - binary_accuracy: 0.6678 - f1_score: 0.6660 - val_loss: 0.6506 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 154/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6303 - binary_accuracy: 0.6638 - f1_score: 0.6610 - val_loss: 0.6510 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 155/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6234 - binary_accuracy: 0.6804 - f1_score: 0.6770 - val_loss: 0.6511 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 156/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6162 - binary_accuracy: 0.6804 - f1_score: 0.6793 - val_loss: 0.6506 - val_binary_accuracy: 0.6802 - val_f1_score: 0.6744\n",
      "Epoch 157/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6258 - binary_accuracy: 0.6744 - f1_score: 0.6742 - val_loss: 0.6512 - val_binary_accuracy: 0.6802 - val_f1_score: 0.6744\n",
      "Epoch 158/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6332 - binary_accuracy: 0.6764 - f1_score: 0.6792 - val_loss: 0.6509 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 159/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.6226 - binary_accuracy: 0.6941 - f1_score: 0.6924 - val_loss: 0.6504 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 160/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6262 - binary_accuracy: 0.6759 - f1_score: 0.6723 - val_loss: 0.6505 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 161/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6215 - binary_accuracy: 0.6815 - f1_score: 0.6784 - val_loss: 0.6504 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 162/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.6274 - binary_accuracy: 0.6754 - f1_score: 0.6712 - val_loss: 0.6505 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 163/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.6291 - binary_accuracy: 0.6734 - f1_score: 0.6713 - val_loss: 0.6508 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 164/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.6196 - binary_accuracy: 0.6915 - f1_score: 0.6935 - val_loss: 0.6512 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 165/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6168 - binary_accuracy: 0.6850 - f1_score: 0.6819 - val_loss: 0.6516 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 166/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6184 - binary_accuracy: 0.6860 - f1_score: 0.6852 - val_loss: 0.6523 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 167/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.6192 - binary_accuracy: 0.6764 - f1_score: 0.6774 - val_loss: 0.6523 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 168/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6109 - binary_accuracy: 0.6714 - f1_score: 0.6658 - val_loss: 0.6523 - val_binary_accuracy: 0.6757 - val_f1_score: 0.6744\n",
      "Epoch 169/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6146 - binary_accuracy: 0.6638 - f1_score: 0.6669 - val_loss: 0.6530 - val_binary_accuracy: 0.6712 - val_f1_score: 0.6649\n",
      "Epoch 170/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6316 - binary_accuracy: 0.6830 - f1_score: 0.6824 - val_loss: 0.6529 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6649\n",
      "Epoch 171/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.6183 - binary_accuracy: 0.6835 - f1_score: 0.6852 - val_loss: 0.6532 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6649\n",
      "Epoch 172/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6235 - binary_accuracy: 0.6890 - f1_score: 0.6864 - val_loss: 0.6529 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6649\n",
      "Epoch 173/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6124 - binary_accuracy: 0.6885 - f1_score: 0.6893 - val_loss: 0.6526 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6649\n",
      "Epoch 174/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6185 - binary_accuracy: 0.6678 - f1_score: 0.6662 - val_loss: 0.6532 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6649\n",
      "Epoch 175/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6167 - binary_accuracy: 0.7011 - f1_score: 0.7005 - val_loss: 0.6536 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6649\n",
      "Epoch 176/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6165 - binary_accuracy: 0.6860 - f1_score: 0.6814 - val_loss: 0.6538 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6649\n",
      "Epoch 177/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6283 - binary_accuracy: 0.6794 - f1_score: 0.6793 - val_loss: 0.6544 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6649\n",
      "Epoch 178/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6179 - binary_accuracy: 0.6870 - f1_score: 0.6872 - val_loss: 0.6545 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6649\n",
      "Epoch 179/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6231 - binary_accuracy: 0.6986 - f1_score: 0.6996 - val_loss: 0.6549 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6649\n",
      "Epoch 180/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6253 - binary_accuracy: 0.6820 - f1_score: 0.6823 - val_loss: 0.6551 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6649\n",
      "Epoch 181/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6098 - binary_accuracy: 0.6794 - f1_score: 0.6783 - val_loss: 0.6551 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6649\n",
      "Epoch 182/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6178 - binary_accuracy: 0.6951 - f1_score: 0.6965 - val_loss: 0.6552 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6649\n",
      "Epoch 183/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6216 - binary_accuracy: 0.6754 - f1_score: 0.6713 - val_loss: 0.6554 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6563\n",
      "Epoch 184/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6180 - binary_accuracy: 0.6956 - f1_score: 0.6986 - val_loss: 0.6551 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6563\n",
      "Epoch 185/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6107 - binary_accuracy: 0.6774 - f1_score: 0.6754 - val_loss: 0.6548 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6563\n",
      "Epoch 186/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6207 - binary_accuracy: 0.7056 - f1_score: 0.7036 - val_loss: 0.6548 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6563\n",
      "Epoch 187/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6126 - binary_accuracy: 0.6900 - f1_score: 0.6895 - val_loss: 0.6548 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6563\n",
      "Epoch 188/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6196 - binary_accuracy: 0.6920 - f1_score: 0.6915 - val_loss: 0.6548 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6563\n",
      "Epoch 189/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.6182 - binary_accuracy: 0.6961 - f1_score: 0.6955 - val_loss: 0.6551 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6563\n",
      "Epoch 190/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6264 - binary_accuracy: 0.6694 - f1_score: 0.6663 - val_loss: 0.6550 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6563\n",
      "Epoch 191/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6197 - binary_accuracy: 0.6809 - f1_score: 0.6825 - val_loss: 0.6552 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6563\n",
      "Epoch 192/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6116 - binary_accuracy: 0.7107 - f1_score: 0.7125 - val_loss: 0.6552 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6563\n",
      "Epoch 193/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6106 - binary_accuracy: 0.6925 - f1_score: 0.6925 - val_loss: 0.6558 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6563\n",
      "Epoch 194/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6128 - binary_accuracy: 0.7147 - f1_score: 0.7156 - val_loss: 0.6555 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6563\n",
      "Epoch 195/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6090 - binary_accuracy: 0.6900 - f1_score: 0.6855 - val_loss: 0.6555 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 196/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6138 - binary_accuracy: 0.7117 - f1_score: 0.7126 - val_loss: 0.6555 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 197/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6131 - binary_accuracy: 0.7067 - f1_score: 0.7066 - val_loss: 0.6561 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 198/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6193 - binary_accuracy: 0.6951 - f1_score: 0.6935 - val_loss: 0.6562 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6657\n",
      "Epoch 199/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6196 - binary_accuracy: 0.6875 - f1_score: 0.6855 - val_loss: 0.6562 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6657\n",
      "Epoch 200/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6083 - binary_accuracy: 0.7122 - f1_score: 0.7137 - val_loss: 0.6569 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6657\n",
      "Epoch 201/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6110 - binary_accuracy: 0.6875 - f1_score: 0.6892 - val_loss: 0.6567 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6657\n",
      "Epoch 202/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6085 - binary_accuracy: 0.7117 - f1_score: 0.7106 - val_loss: 0.6568 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6657\n",
      "Epoch 203/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6063 - binary_accuracy: 0.7072 - f1_score: 0.7126 - val_loss: 0.6576 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6657\n",
      "Epoch 204/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6103 - binary_accuracy: 0.6996 - f1_score: 0.6976 - val_loss: 0.6583 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6657\n",
      "Epoch 205/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6093 - binary_accuracy: 0.7077 - f1_score: 0.7077 - val_loss: 0.6588 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6657\n",
      "Epoch 206/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6221 - binary_accuracy: 0.6663 - f1_score: 0.6662 - val_loss: 0.6588 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6657\n",
      "Epoch 207/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6101 - binary_accuracy: 0.6956 - f1_score: 0.6966 - val_loss: 0.6591 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6563\n",
      "Epoch 208/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6225 - binary_accuracy: 0.6663 - f1_score: 0.6653 - val_loss: 0.6591 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6563\n",
      "Epoch 209/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6131 - binary_accuracy: 0.6930 - f1_score: 0.6935 - val_loss: 0.6591 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6563\n",
      "Epoch 210/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6052 - binary_accuracy: 0.6946 - f1_score: 0.6944 - val_loss: 0.6586 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6563\n",
      "Epoch 211/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6096 - binary_accuracy: 0.6986 - f1_score: 0.6956 - val_loss: 0.6583 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6563\n",
      "Epoch 212/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6053 - binary_accuracy: 0.7072 - f1_score: 0.7067 - val_loss: 0.6594 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 213/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6166 - binary_accuracy: 0.7077 - f1_score: 0.7067 - val_loss: 0.6599 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6657\n",
      "Epoch 214/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6021 - binary_accuracy: 0.7001 - f1_score: 0.6996 - val_loss: 0.6593 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6657\n",
      "Epoch 215/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6022 - binary_accuracy: 0.7006 - f1_score: 0.7006 - val_loss: 0.6593 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6657\n",
      "Epoch 216/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6079 - binary_accuracy: 0.7011 - f1_score: 0.7016 - val_loss: 0.6593 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 217/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6044 - binary_accuracy: 0.6946 - f1_score: 0.6955 - val_loss: 0.6601 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6563\n",
      "Epoch 218/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6090 - binary_accuracy: 0.7072 - f1_score: 0.7067 - val_loss: 0.6602 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6563\n",
      "Epoch 219/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6070 - binary_accuracy: 0.7213 - f1_score: 0.7237 - val_loss: 0.6605 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6563\n",
      "Epoch 220/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6033 - binary_accuracy: 0.7001 - f1_score: 0.6984 - val_loss: 0.6607 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6563\n",
      "Epoch 221/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6109 - binary_accuracy: 0.6890 - f1_score: 0.6884 - val_loss: 0.6614 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6563\n",
      "Epoch 222/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6015 - binary_accuracy: 0.7137 - f1_score: 0.7127 - val_loss: 0.6613 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6563\n",
      "Epoch 223/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6049 - binary_accuracy: 0.7102 - f1_score: 0.7065 - val_loss: 0.6621 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6563\n",
      "Epoch 224/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6125 - binary_accuracy: 0.7031 - f1_score: 0.7026 - val_loss: 0.6627 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6563\n",
      "Epoch 225/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6075 - binary_accuracy: 0.6804 - f1_score: 0.6784 - val_loss: 0.6627 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6657\n",
      "Epoch 226/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6072 - binary_accuracy: 0.7067 - f1_score: 0.7046 - val_loss: 0.6622 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6563\n",
      "Epoch 227/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6090 - binary_accuracy: 0.7208 - f1_score: 0.7187 - val_loss: 0.6624 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 228/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6074 - binary_accuracy: 0.6850 - f1_score: 0.6814 - val_loss: 0.6622 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 229/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5976 - binary_accuracy: 0.6900 - f1_score: 0.6904 - val_loss: 0.6628 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 230/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6040 - binary_accuracy: 0.7056 - f1_score: 0.7066 - val_loss: 0.6632 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6563\n",
      "Epoch 231/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6029 - binary_accuracy: 0.7031 - f1_score: 0.7015 - val_loss: 0.6639 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6563\n",
      "Epoch 232/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5966 - binary_accuracy: 0.7112 - f1_score: 0.7105 - val_loss: 0.6641 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 233/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6087 - binary_accuracy: 0.6930 - f1_score: 0.6925 - val_loss: 0.6642 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6657\n",
      "Epoch 234/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6160 - binary_accuracy: 0.7001 - f1_score: 0.6976 - val_loss: 0.6645 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 235/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6064 - binary_accuracy: 0.6910 - f1_score: 0.6885 - val_loss: 0.6645 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 236/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6149 - binary_accuracy: 0.6976 - f1_score: 0.6915 - val_loss: 0.6638 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 237/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6103 - binary_accuracy: 0.6991 - f1_score: 0.6986 - val_loss: 0.6636 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6563\n",
      "Epoch 238/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6098 - binary_accuracy: 0.6981 - f1_score: 0.6975 - val_loss: 0.6636 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6563\n",
      "Epoch 239/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6118 - binary_accuracy: 0.7056 - f1_score: 0.7036 - val_loss: 0.6628 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6468\n",
      "Epoch 240/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6094 - binary_accuracy: 0.7213 - f1_score: 0.7197 - val_loss: 0.6628 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6563\n",
      "Epoch 241/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6100 - binary_accuracy: 0.7001 - f1_score: 0.6976 - val_loss: 0.6625 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 242/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6044 - binary_accuracy: 0.7243 - f1_score: 0.7258 - val_loss: 0.6618 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6563\n",
      "Epoch 243/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6049 - binary_accuracy: 0.7006 - f1_score: 0.7026 - val_loss: 0.6617 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 244/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6009 - binary_accuracy: 0.7107 - f1_score: 0.7147 - val_loss: 0.6624 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6657\n",
      "Epoch 245/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6017 - binary_accuracy: 0.7056 - f1_score: 0.7085 - val_loss: 0.6619 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6657\n",
      "Epoch 246/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.5979 - binary_accuracy: 0.7107 - f1_score: 0.7106 - val_loss: 0.6621 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 247/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6080 - binary_accuracy: 0.7102 - f1_score: 0.7117 - val_loss: 0.6621 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 248/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5984 - binary_accuracy: 0.7046 - f1_score: 0.7046 - val_loss: 0.6626 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6657\n",
      "Epoch 249/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5989 - binary_accuracy: 0.7208 - f1_score: 0.7177 - val_loss: 0.6626 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6570\n",
      "Epoch 250/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6053 - binary_accuracy: 0.7172 - f1_score: 0.7186 - val_loss: 0.6623 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6570\n",
      "Epoch 251/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6058 - binary_accuracy: 0.6991 - f1_score: 0.7022 - val_loss: 0.6615 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6570\n",
      "Epoch 252/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6033 - binary_accuracy: 0.6996 - f1_score: 0.6984 - val_loss: 0.6611 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 253/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5986 - binary_accuracy: 0.7117 - f1_score: 0.7095 - val_loss: 0.6611 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6657\n",
      "Epoch 254/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6067 - binary_accuracy: 0.7102 - f1_score: 0.7082 - val_loss: 0.6610 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6570\n",
      "Epoch 255/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6014 - binary_accuracy: 0.6870 - f1_score: 0.6843 - val_loss: 0.6610 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6570\n",
      "Epoch 256/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6131 - binary_accuracy: 0.7067 - f1_score: 0.7066 - val_loss: 0.6611 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 257/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6074 - binary_accuracy: 0.6870 - f1_score: 0.6834 - val_loss: 0.6602 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6657\n",
      "Epoch 258/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6063 - binary_accuracy: 0.6991 - f1_score: 0.6963 - val_loss: 0.6603 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6563\n",
      "Epoch 259/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6025 - binary_accuracy: 0.7127 - f1_score: 0.7136 - val_loss: 0.6604 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 260/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5974 - binary_accuracy: 0.7188 - f1_score: 0.7164 - val_loss: 0.6602 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6476\n",
      "Epoch 261/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6076 - binary_accuracy: 0.6961 - f1_score: 0.6955 - val_loss: 0.6605 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6476\n",
      "Epoch 262/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6024 - binary_accuracy: 0.7182 - f1_score: 0.7144 - val_loss: 0.6601 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 263/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6073 - binary_accuracy: 0.7172 - f1_score: 0.7185 - val_loss: 0.6596 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 264/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6029 - binary_accuracy: 0.6946 - f1_score: 0.6914 - val_loss: 0.6598 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6570\n",
      "Epoch 265/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.6023 - binary_accuracy: 0.7092 - f1_score: 0.7091 - val_loss: 0.6599 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6570\n",
      "Epoch 266/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6030 - binary_accuracy: 0.7177 - f1_score: 0.7174 - val_loss: 0.6597 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6570\n",
      "Epoch 267/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6077 - binary_accuracy: 0.7087 - f1_score: 0.7063 - val_loss: 0.6596 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 268/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6051 - binary_accuracy: 0.7157 - f1_score: 0.7172 - val_loss: 0.6608 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 269/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6024 - binary_accuracy: 0.7122 - f1_score: 0.7096 - val_loss: 0.6607 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 270/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6094 - binary_accuracy: 0.7072 - f1_score: 0.7060 - val_loss: 0.6601 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 271/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.5994 - binary_accuracy: 0.6991 - f1_score: 0.6975 - val_loss: 0.6604 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 272/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.5960 - binary_accuracy: 0.7218 - f1_score: 0.7203 - val_loss: 0.6606 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 273/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6036 - binary_accuracy: 0.7087 - f1_score: 0.7054 - val_loss: 0.6611 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 274/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5940 - binary_accuracy: 0.7364 - f1_score: 0.7361 - val_loss: 0.6617 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 275/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6063 - binary_accuracy: 0.7213 - f1_score: 0.7194 - val_loss: 0.6622 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 276/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5991 - binary_accuracy: 0.7288 - f1_score: 0.7316 - val_loss: 0.6632 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6476\n",
      "Epoch 277/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5876 - binary_accuracy: 0.7152 - f1_score: 0.7147 - val_loss: 0.6636 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6476\n",
      "Epoch 278/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6019 - binary_accuracy: 0.7329 - f1_score: 0.7318 - val_loss: 0.6640 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 279/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5998 - binary_accuracy: 0.6951 - f1_score: 0.6934 - val_loss: 0.6647 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 280/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5954 - binary_accuracy: 0.7001 - f1_score: 0.7004 - val_loss: 0.6647 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 281/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6031 - binary_accuracy: 0.7147 - f1_score: 0.7167 - val_loss: 0.6645 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 282/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5988 - binary_accuracy: 0.7107 - f1_score: 0.7096 - val_loss: 0.6643 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 283/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6041 - binary_accuracy: 0.7122 - f1_score: 0.7097 - val_loss: 0.6641 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 284/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.5970 - binary_accuracy: 0.7177 - f1_score: 0.7184 - val_loss: 0.6646 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 285/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6003 - binary_accuracy: 0.7016 - f1_score: 0.7003 - val_loss: 0.6652 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 286/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.5936 - binary_accuracy: 0.7122 - f1_score: 0.7134 - val_loss: 0.6655 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 287/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.5932 - binary_accuracy: 0.7127 - f1_score: 0.7117 - val_loss: 0.6656 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 288/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.6072 - binary_accuracy: 0.7061 - f1_score: 0.7056 - val_loss: 0.6651 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 289/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5932 - binary_accuracy: 0.7182 - f1_score: 0.7183 - val_loss: 0.6652 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 290/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.6045 - binary_accuracy: 0.7006 - f1_score: 0.7004 - val_loss: 0.6649 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 291/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.6004 - binary_accuracy: 0.7087 - f1_score: 0.7104 - val_loss: 0.6648 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 292/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.5937 - binary_accuracy: 0.7203 - f1_score: 0.7173 - val_loss: 0.6651 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 293/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5996 - binary_accuracy: 0.7056 - f1_score: 0.7026 - val_loss: 0.6648 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 294/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.6022 - binary_accuracy: 0.6996 - f1_score: 0.6985 - val_loss: 0.6645 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 295/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5864 - binary_accuracy: 0.7238 - f1_score: 0.7256 - val_loss: 0.6649 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 296/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.6007 - binary_accuracy: 0.7132 - f1_score: 0.7123 - val_loss: 0.6651 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 297/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5944 - binary_accuracy: 0.7162 - f1_score: 0.7174 - val_loss: 0.6654 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 298/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6024 - binary_accuracy: 0.7127 - f1_score: 0.7125 - val_loss: 0.6649 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 299/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.5940 - binary_accuracy: 0.7152 - f1_score: 0.7154 - val_loss: 0.6644 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 300/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.5989 - binary_accuracy: 0.7142 - f1_score: 0.7132 - val_loss: 0.6648 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 301/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.6027 - binary_accuracy: 0.7167 - f1_score: 0.7130 - val_loss: 0.6647 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 302/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5959 - binary_accuracy: 0.7188 - f1_score: 0.7210 - val_loss: 0.6654 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 303/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5956 - binary_accuracy: 0.7177 - f1_score: 0.7164 - val_loss: 0.6657 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 304/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5936 - binary_accuracy: 0.7273 - f1_score: 0.7268 - val_loss: 0.6658 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 305/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.5875 - binary_accuracy: 0.7218 - f1_score: 0.7204 - val_loss: 0.6665 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 306/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.5876 - binary_accuracy: 0.7440 - f1_score: 0.7442 - val_loss: 0.6662 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 307/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5956 - binary_accuracy: 0.7006 - f1_score: 0.6974 - val_loss: 0.6663 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 308/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5933 - binary_accuracy: 0.7263 - f1_score: 0.7237 - val_loss: 0.6658 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 309/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5974 - binary_accuracy: 0.7278 - f1_score: 0.7255 - val_loss: 0.6650 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 310/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.5933 - binary_accuracy: 0.7233 - f1_score: 0.7214 - val_loss: 0.6653 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 311/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5898 - binary_accuracy: 0.7248 - f1_score: 0.7222 - val_loss: 0.6653 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 312/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5996 - binary_accuracy: 0.7142 - f1_score: 0.7133 - val_loss: 0.6654 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 313/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.5902 - binary_accuracy: 0.7228 - f1_score: 0.7206 - val_loss: 0.6652 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 314/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5843 - binary_accuracy: 0.7288 - f1_score: 0.7266 - val_loss: 0.6667 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 315/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.5957 - binary_accuracy: 0.7228 - f1_score: 0.7236 - val_loss: 0.6669 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6662\n",
      "Epoch 316/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.5959 - binary_accuracy: 0.7157 - f1_score: 0.7145 - val_loss: 0.6679 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 317/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5883 - binary_accuracy: 0.7314 - f1_score: 0.7297 - val_loss: 0.6683 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6570\n",
      "Epoch 318/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.5950 - binary_accuracy: 0.7172 - f1_score: 0.7175 - val_loss: 0.6684 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 319/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.5963 - binary_accuracy: 0.7193 - f1_score: 0.7176 - val_loss: 0.6675 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 320/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5918 - binary_accuracy: 0.7329 - f1_score: 0.7364 - val_loss: 0.6680 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 321/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5986 - binary_accuracy: 0.7288 - f1_score: 0.7312 - val_loss: 0.6675 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6570\n",
      "Epoch 322/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5930 - binary_accuracy: 0.7203 - f1_score: 0.7196 - val_loss: 0.6673 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 323/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5880 - binary_accuracy: 0.7233 - f1_score: 0.7245 - val_loss: 0.6681 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 324/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.5871 - binary_accuracy: 0.7208 - f1_score: 0.7181 - val_loss: 0.6679 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 325/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.5861 - binary_accuracy: 0.7188 - f1_score: 0.7193 - val_loss: 0.6682 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 326/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5898 - binary_accuracy: 0.7238 - f1_score: 0.7194 - val_loss: 0.6685 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 327/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5987 - binary_accuracy: 0.7208 - f1_score: 0.7205 - val_loss: 0.6686 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 328/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5848 - binary_accuracy: 0.7349 - f1_score: 0.7336 - val_loss: 0.6687 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 329/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5906 - binary_accuracy: 0.7112 - f1_score: 0.7092 - val_loss: 0.6693 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 330/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5879 - binary_accuracy: 0.7319 - f1_score: 0.7314 - val_loss: 0.6691 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 331/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5931 - binary_accuracy: 0.7278 - f1_score: 0.7271 - val_loss: 0.6705 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6570\n",
      "Epoch 332/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5947 - binary_accuracy: 0.7329 - f1_score: 0.7324 - val_loss: 0.6711 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6570\n",
      "Epoch 333/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5948 - binary_accuracy: 0.7193 - f1_score: 0.7167 - val_loss: 0.6711 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6570\n",
      "Epoch 334/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5869 - binary_accuracy: 0.7389 - f1_score: 0.7368 - val_loss: 0.6722 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6570\n",
      "Epoch 335/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5873 - binary_accuracy: 0.7384 - f1_score: 0.7406 - val_loss: 0.6722 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 336/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5970 - binary_accuracy: 0.7293 - f1_score: 0.7264 - val_loss: 0.6717 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6662\n",
      "Epoch 337/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5922 - binary_accuracy: 0.7223 - f1_score: 0.7225 - val_loss: 0.6720 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 338/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.5919 - binary_accuracy: 0.7218 - f1_score: 0.7222 - val_loss: 0.6716 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 339/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5893 - binary_accuracy: 0.7344 - f1_score: 0.7335 - val_loss: 0.6711 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 340/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5860 - binary_accuracy: 0.7278 - f1_score: 0.7216 - val_loss: 0.6714 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6662\n",
      "Epoch 341/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5906 - binary_accuracy: 0.7278 - f1_score: 0.7277 - val_loss: 0.6717 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 342/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5775 - binary_accuracy: 0.7167 - f1_score: 0.7155 - val_loss: 0.6714 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 343/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5823 - binary_accuracy: 0.7399 - f1_score: 0.7405 - val_loss: 0.6717 - val_binary_accuracy: 0.6667 - val_f1_score: 0.6662\n",
      "Epoch 344/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5962 - binary_accuracy: 0.6996 - f1_score: 0.6975 - val_loss: 0.6716 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 345/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5975 - binary_accuracy: 0.6966 - f1_score: 0.6961 - val_loss: 0.6723 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 346/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5845 - binary_accuracy: 0.7142 - f1_score: 0.7144 - val_loss: 0.6726 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 347/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5979 - binary_accuracy: 0.7137 - f1_score: 0.7143 - val_loss: 0.6729 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 348/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5909 - binary_accuracy: 0.7177 - f1_score: 0.7185 - val_loss: 0.6735 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 349/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.5880 - binary_accuracy: 0.7258 - f1_score: 0.7216 - val_loss: 0.6730 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 350/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5847 - binary_accuracy: 0.7344 - f1_score: 0.7350 - val_loss: 0.6738 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 351/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5790 - binary_accuracy: 0.7344 - f1_score: 0.7324 - val_loss: 0.6746 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 352/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5836 - binary_accuracy: 0.7253 - f1_score: 0.7243 - val_loss: 0.6752 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 353/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5798 - binary_accuracy: 0.7324 - f1_score: 0.7314 - val_loss: 0.6758 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 354/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.5917 - binary_accuracy: 0.7238 - f1_score: 0.7226 - val_loss: 0.6741 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 355/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5800 - binary_accuracy: 0.7308 - f1_score: 0.7285 - val_loss: 0.6753 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6570\n",
      "Epoch 356/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5935 - binary_accuracy: 0.7253 - f1_score: 0.7241 - val_loss: 0.6762 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6570\n",
      "Epoch 357/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5890 - binary_accuracy: 0.7278 - f1_score: 0.7251 - val_loss: 0.6763 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6570\n",
      "Epoch 358/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5939 - binary_accuracy: 0.7374 - f1_score: 0.7363 - val_loss: 0.6761 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6570\n",
      "Epoch 359/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5860 - binary_accuracy: 0.7162 - f1_score: 0.7180 - val_loss: 0.6760 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 360/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5855 - binary_accuracy: 0.7349 - f1_score: 0.7323 - val_loss: 0.6755 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 361/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.5834 - binary_accuracy: 0.7308 - f1_score: 0.7283 - val_loss: 0.6758 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 362/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5847 - binary_accuracy: 0.7177 - f1_score: 0.7162 - val_loss: 0.6756 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 363/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5885 - binary_accuracy: 0.7283 - f1_score: 0.7273 - val_loss: 0.6757 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 364/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5862 - binary_accuracy: 0.7142 - f1_score: 0.7141 - val_loss: 0.6761 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 365/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5758 - binary_accuracy: 0.7490 - f1_score: 0.7465 - val_loss: 0.6769 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 366/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.5827 - binary_accuracy: 0.7298 - f1_score: 0.7294 - val_loss: 0.6763 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 367/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.5842 - binary_accuracy: 0.7258 - f1_score: 0.7230 - val_loss: 0.6768 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 368/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5811 - binary_accuracy: 0.7414 - f1_score: 0.7385 - val_loss: 0.6768 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6476\n",
      "Epoch 369/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5763 - binary_accuracy: 0.7515 - f1_score: 0.7505 - val_loss: 0.6772 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 370/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5847 - binary_accuracy: 0.7203 - f1_score: 0.7173 - val_loss: 0.6775 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 371/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5806 - binary_accuracy: 0.7314 - f1_score: 0.7294 - val_loss: 0.6777 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 372/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5897 - binary_accuracy: 0.7203 - f1_score: 0.7174 - val_loss: 0.6784 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 373/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5811 - binary_accuracy: 0.7424 - f1_score: 0.7404 - val_loss: 0.6794 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 374/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5882 - binary_accuracy: 0.7510 - f1_score: 0.7528 - val_loss: 0.6795 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 375/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5846 - binary_accuracy: 0.7334 - f1_score: 0.7316 - val_loss: 0.6796 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 376/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5842 - binary_accuracy: 0.7298 - f1_score: 0.7281 - val_loss: 0.6789 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 377/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5881 - binary_accuracy: 0.7288 - f1_score: 0.7293 - val_loss: 0.6794 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 378/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5792 - binary_accuracy: 0.7238 - f1_score: 0.7253 - val_loss: 0.6793 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 379/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5863 - binary_accuracy: 0.7283 - f1_score: 0.7271 - val_loss: 0.6785 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 380/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5865 - binary_accuracy: 0.7409 - f1_score: 0.7405 - val_loss: 0.6790 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 381/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5744 - binary_accuracy: 0.7339 - f1_score: 0.7325 - val_loss: 0.6790 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6570\n",
      "Epoch 382/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5881 - binary_accuracy: 0.7248 - f1_score: 0.7188 - val_loss: 0.6790 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 383/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5757 - binary_accuracy: 0.7389 - f1_score: 0.7395 - val_loss: 0.6789 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 384/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5708 - binary_accuracy: 0.7293 - f1_score: 0.7284 - val_loss: 0.6791 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 385/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5736 - binary_accuracy: 0.7419 - f1_score: 0.7414 - val_loss: 0.6791 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 386/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5831 - binary_accuracy: 0.7384 - f1_score: 0.7375 - val_loss: 0.6807 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 387/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5749 - binary_accuracy: 0.7515 - f1_score: 0.7489 - val_loss: 0.6820 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 388/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5893 - binary_accuracy: 0.7419 - f1_score: 0.7414 - val_loss: 0.6824 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 389/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5793 - binary_accuracy: 0.7419 - f1_score: 0.7416 - val_loss: 0.6816 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 390/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5797 - binary_accuracy: 0.7288 - f1_score: 0.7263 - val_loss: 0.6817 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 391/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5810 - binary_accuracy: 0.7510 - f1_score: 0.7483 - val_loss: 0.6825 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6570\n",
      "Epoch 392/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5831 - binary_accuracy: 0.7424 - f1_score: 0.7445 - val_loss: 0.6825 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 393/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5867 - binary_accuracy: 0.7177 - f1_score: 0.7146 - val_loss: 0.6818 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6570\n",
      "Epoch 394/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5962 - binary_accuracy: 0.7278 - f1_score: 0.7266 - val_loss: 0.6813 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6476\n",
      "Epoch 395/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5688 - binary_accuracy: 0.7389 - f1_score: 0.7363 - val_loss: 0.6811 - val_binary_accuracy: 0.6396 - val_f1_score: 0.6476\n",
      "Epoch 396/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5867 - binary_accuracy: 0.7334 - f1_score: 0.7301 - val_loss: 0.6814 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6476\n",
      "Epoch 397/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5792 - binary_accuracy: 0.7263 - f1_score: 0.7259 - val_loss: 0.6812 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 398/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5751 - binary_accuracy: 0.7631 - f1_score: 0.7597 - val_loss: 0.6810 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 399/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5867 - binary_accuracy: 0.7480 - f1_score: 0.7478 - val_loss: 0.6809 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 400/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5860 - binary_accuracy: 0.7349 - f1_score: 0.7323 - val_loss: 0.6812 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 401/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5706 - binary_accuracy: 0.7450 - f1_score: 0.7407 - val_loss: 0.6810 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 402/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5839 - binary_accuracy: 0.7450 - f1_score: 0.7447 - val_loss: 0.6818 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 403/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5843 - binary_accuracy: 0.7127 - f1_score: 0.7134 - val_loss: 0.6816 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6662\n",
      "Epoch 404/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5673 - binary_accuracy: 0.7434 - f1_score: 0.7444 - val_loss: 0.6814 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6662\n",
      "Epoch 405/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5911 - binary_accuracy: 0.7424 - f1_score: 0.7393 - val_loss: 0.6822 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6662\n",
      "Epoch 406/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5817 - binary_accuracy: 0.7218 - f1_score: 0.7210 - val_loss: 0.6824 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6662\n",
      "Epoch 407/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5828 - binary_accuracy: 0.7298 - f1_score: 0.7275 - val_loss: 0.6820 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 408/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5700 - binary_accuracy: 0.7490 - f1_score: 0.7465 - val_loss: 0.6825 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6662\n",
      "Epoch 409/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5690 - binary_accuracy: 0.7434 - f1_score: 0.7477 - val_loss: 0.6837 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 410/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5776 - binary_accuracy: 0.7404 - f1_score: 0.7403 - val_loss: 0.6836 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 411/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5760 - binary_accuracy: 0.7404 - f1_score: 0.7389 - val_loss: 0.6837 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 412/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5823 - binary_accuracy: 0.7329 - f1_score: 0.7287 - val_loss: 0.6837 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 413/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5709 - binary_accuracy: 0.7298 - f1_score: 0.7294 - val_loss: 0.6833 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 414/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5756 - binary_accuracy: 0.7293 - f1_score: 0.7285 - val_loss: 0.6824 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 415/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5732 - binary_accuracy: 0.7374 - f1_score: 0.7387 - val_loss: 0.6821 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 416/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5871 - binary_accuracy: 0.7213 - f1_score: 0.7212 - val_loss: 0.6818 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 417/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5645 - binary_accuracy: 0.7409 - f1_score: 0.7394 - val_loss: 0.6822 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 418/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5846 - binary_accuracy: 0.7344 - f1_score: 0.7335 - val_loss: 0.6825 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 419/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5773 - binary_accuracy: 0.7359 - f1_score: 0.7335 - val_loss: 0.6822 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 420/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5781 - binary_accuracy: 0.7434 - f1_score: 0.7404 - val_loss: 0.6821 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 421/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5791 - binary_accuracy: 0.7329 - f1_score: 0.7300 - val_loss: 0.6825 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6570\n",
      "Epoch 422/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5789 - binary_accuracy: 0.7263 - f1_score: 0.7272 - val_loss: 0.6824 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6382\n",
      "Epoch 423/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5798 - binary_accuracy: 0.7369 - f1_score: 0.7365 - val_loss: 0.6814 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6382\n",
      "Epoch 424/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5760 - binary_accuracy: 0.7379 - f1_score: 0.7344 - val_loss: 0.6816 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6382\n",
      "Epoch 425/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5731 - binary_accuracy: 0.7429 - f1_score: 0.7420 - val_loss: 0.6828 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 426/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5700 - binary_accuracy: 0.7419 - f1_score: 0.7376 - val_loss: 0.6828 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 427/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5897 - binary_accuracy: 0.7238 - f1_score: 0.7242 - val_loss: 0.6818 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6476\n",
      "Epoch 428/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5696 - binary_accuracy: 0.7369 - f1_score: 0.7353 - val_loss: 0.6818 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6476\n",
      "Epoch 429/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5697 - binary_accuracy: 0.7641 - f1_score: 0.7644 - val_loss: 0.6817 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 430/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5614 - binary_accuracy: 0.7344 - f1_score: 0.7363 - val_loss: 0.6828 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6476\n",
      "Epoch 431/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5754 - binary_accuracy: 0.7218 - f1_score: 0.7194 - val_loss: 0.6829 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 432/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5747 - binary_accuracy: 0.7344 - f1_score: 0.7311 - val_loss: 0.6831 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6476\n",
      "Epoch 433/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5732 - binary_accuracy: 0.7349 - f1_score: 0.7305 - val_loss: 0.6831 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6476\n",
      "Epoch 434/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5778 - binary_accuracy: 0.7324 - f1_score: 0.7315 - val_loss: 0.6830 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 435/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5742 - binary_accuracy: 0.7515 - f1_score: 0.7486 - val_loss: 0.6833 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 436/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5701 - binary_accuracy: 0.7581 - f1_score: 0.7567 - val_loss: 0.6836 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 437/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5755 - binary_accuracy: 0.7369 - f1_score: 0.7385 - val_loss: 0.6845 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6476\n",
      "Epoch 438/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5822 - binary_accuracy: 0.7455 - f1_score: 0.7443 - val_loss: 0.6840 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 439/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5722 - binary_accuracy: 0.7344 - f1_score: 0.7341 - val_loss: 0.6842 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 440/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5867 - binary_accuracy: 0.7394 - f1_score: 0.7377 - val_loss: 0.6839 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 441/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5688 - binary_accuracy: 0.7445 - f1_score: 0.7444 - val_loss: 0.6838 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 442/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5787 - binary_accuracy: 0.7450 - f1_score: 0.7432 - val_loss: 0.6831 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6476\n",
      "Epoch 443/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5676 - binary_accuracy: 0.7450 - f1_score: 0.7458 - val_loss: 0.6827 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6476\n",
      "Epoch 444/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5817 - binary_accuracy: 0.7067 - f1_score: 0.7048 - val_loss: 0.6818 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6476\n",
      "Epoch 445/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5766 - binary_accuracy: 0.7485 - f1_score: 0.7487 - val_loss: 0.6823 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 446/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5558 - binary_accuracy: 0.7576 - f1_score: 0.7534 - val_loss: 0.6834 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 447/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5809 - binary_accuracy: 0.7137 - f1_score: 0.7100 - val_loss: 0.6840 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 448/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5642 - binary_accuracy: 0.7440 - f1_score: 0.7442 - val_loss: 0.6850 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 449/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5818 - binary_accuracy: 0.7545 - f1_score: 0.7538 - val_loss: 0.6849 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 450/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5775 - binary_accuracy: 0.7263 - f1_score: 0.7263 - val_loss: 0.6850 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 451/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5758 - binary_accuracy: 0.7500 - f1_score: 0.7453 - val_loss: 0.6852 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 452/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5766 - binary_accuracy: 0.7319 - f1_score: 0.7304 - val_loss: 0.6857 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6476\n",
      "Epoch 453/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5653 - binary_accuracy: 0.7601 - f1_score: 0.7623 - val_loss: 0.6861 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 454/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5619 - binary_accuracy: 0.7520 - f1_score: 0.7531 - val_loss: 0.6864 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 455/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5812 - binary_accuracy: 0.7258 - f1_score: 0.7254 - val_loss: 0.6871 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 456/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5754 - binary_accuracy: 0.7389 - f1_score: 0.7385 - val_loss: 0.6869 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6662\n",
      "Epoch 457/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5688 - binary_accuracy: 0.7349 - f1_score: 0.7301 - val_loss: 0.6870 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 458/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5623 - binary_accuracy: 0.7450 - f1_score: 0.7442 - val_loss: 0.6882 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 459/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5789 - binary_accuracy: 0.7465 - f1_score: 0.7435 - val_loss: 0.6877 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 460/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5683 - binary_accuracy: 0.7429 - f1_score: 0.7396 - val_loss: 0.6871 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6476\n",
      "Epoch 461/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5798 - binary_accuracy: 0.7167 - f1_score: 0.7193 - val_loss: 0.6872 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 462/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5792 - binary_accuracy: 0.7550 - f1_score: 0.7517 - val_loss: 0.6873 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 463/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5640 - binary_accuracy: 0.7399 - f1_score: 0.7364 - val_loss: 0.6892 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 464/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5775 - binary_accuracy: 0.7495 - f1_score: 0.7476 - val_loss: 0.6888 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 465/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5699 - binary_accuracy: 0.7550 - f1_score: 0.7547 - val_loss: 0.6887 - val_binary_accuracy: 0.6577 - val_f1_score: 0.6570\n",
      "Epoch 466/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5904 - binary_accuracy: 0.7308 - f1_score: 0.7294 - val_loss: 0.6874 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 467/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5627 - binary_accuracy: 0.7510 - f1_score: 0.7480 - val_loss: 0.6883 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 468/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5690 - binary_accuracy: 0.7424 - f1_score: 0.7422 - val_loss: 0.6885 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 469/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5744 - binary_accuracy: 0.7490 - f1_score: 0.7475 - val_loss: 0.6884 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 470/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5723 - binary_accuracy: 0.7293 - f1_score: 0.7261 - val_loss: 0.6891 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 471/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5732 - binary_accuracy: 0.7409 - f1_score: 0.7433 - val_loss: 0.6884 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 472/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5683 - binary_accuracy: 0.7495 - f1_score: 0.7492 - val_loss: 0.6888 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 473/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5872 - binary_accuracy: 0.7581 - f1_score: 0.7549 - val_loss: 0.6885 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 474/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5710 - binary_accuracy: 0.7409 - f1_score: 0.7394 - val_loss: 0.6882 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 475/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5665 - binary_accuracy: 0.7374 - f1_score: 0.7330 - val_loss: 0.6879 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 476/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5660 - binary_accuracy: 0.7374 - f1_score: 0.7364 - val_loss: 0.6879 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 477/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5577 - binary_accuracy: 0.7626 - f1_score: 0.7569 - val_loss: 0.6882 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 478/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5746 - binary_accuracy: 0.7404 - f1_score: 0.7395 - val_loss: 0.6877 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 479/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5726 - binary_accuracy: 0.7314 - f1_score: 0.7283 - val_loss: 0.6876 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 480/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5684 - binary_accuracy: 0.7319 - f1_score: 0.7300 - val_loss: 0.6874 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 481/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5792 - binary_accuracy: 0.7314 - f1_score: 0.7296 - val_loss: 0.6879 - val_binary_accuracy: 0.6532 - val_f1_score: 0.6476\n",
      "Epoch 482/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5627 - binary_accuracy: 0.7480 - f1_score: 0.7459 - val_loss: 0.6880 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 483/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5692 - binary_accuracy: 0.7455 - f1_score: 0.7439 - val_loss: 0.6888 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 484/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5723 - binary_accuracy: 0.7475 - f1_score: 0.7463 - val_loss: 0.6889 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 485/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5814 - binary_accuracy: 0.7314 - f1_score: 0.7281 - val_loss: 0.6881 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6476\n",
      "Epoch 486/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5666 - binary_accuracy: 0.7460 - f1_score: 0.7448 - val_loss: 0.6883 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6476\n",
      "Epoch 487/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5622 - binary_accuracy: 0.7480 - f1_score: 0.7507 - val_loss: 0.6897 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 488/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5680 - binary_accuracy: 0.7424 - f1_score: 0.7427 - val_loss: 0.6899 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 489/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5744 - binary_accuracy: 0.7480 - f1_score: 0.7495 - val_loss: 0.6892 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 490/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5779 - binary_accuracy: 0.7374 - f1_score: 0.7353 - val_loss: 0.6892 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6476\n",
      "Epoch 491/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5641 - binary_accuracy: 0.7470 - f1_score: 0.7442 - val_loss: 0.6888 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6476\n",
      "Epoch 492/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5619 - binary_accuracy: 0.7596 - f1_score: 0.7573 - val_loss: 0.6891 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 493/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5690 - binary_accuracy: 0.7576 - f1_score: 0.7564 - val_loss: 0.6897 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 494/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5720 - binary_accuracy: 0.7440 - f1_score: 0.7434 - val_loss: 0.6900 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 495/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5751 - binary_accuracy: 0.7485 - f1_score: 0.7484 - val_loss: 0.6907 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 496/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5619 - binary_accuracy: 0.7520 - f1_score: 0.7482 - val_loss: 0.6899 - val_binary_accuracy: 0.6441 - val_f1_score: 0.6476\n",
      "Epoch 497/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5716 - binary_accuracy: 0.7470 - f1_score: 0.7476 - val_loss: 0.6901 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 498/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5561 - binary_accuracy: 0.7540 - f1_score: 0.7495 - val_loss: 0.6905 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 499/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.5701 - binary_accuracy: 0.7379 - f1_score: 0.7350 - val_loss: 0.6906 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n",
      "Epoch 500/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.5614 - binary_accuracy: 0.7485 - f1_score: 0.7493 - val_loss: 0.6899 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6476\n"
     ]
    }
   ],
   "source": [
    "efficientnet = efficientnetCreate(IMAGE_SIZE)\n",
    "\n",
    "plot_model(efficientnet, to_file=plotpath / Path('efficientnet.png'), show_shapes=True, show_layer_names=True)\n",
    "\n",
    "\n",
    "\n",
    "# initial_learning_rate = 1.5e-5      # NON VA BENE\n",
    "# initial_learning_rate = 1.5e-4      #acc 0.66\n",
    "initial_learning_rate = 1e-5  # migliore come accuracy massima  0.69\n",
    "\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=100, decay_rate=0.98, staircase=False)\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=100, decay_rate=0.95, staircase=False)\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=200, decay_rate=0.99, staircase=False)\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=np.ceil(X_train_IMGS.shape[0]/batch_size), decay_rate=0.99, staircase=False)\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=np.ceil((X_train_IMGS.shape[0]/batch_size)/2), decay_rate=0.99, staircase=True)\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=np.ceil((X_train_IMGS.shape[0]/batch_size)), decay_rate=0.99, staircase=True)\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=300, decay_rate=0.99, staircase=False)\n",
    "    \n",
    "\n",
    "efficientnet.compile(\n",
    "        # optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-2, decay=1e-4),   #FUSION 2022\n",
    "        optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "        # optimizer=tf.keras.optimizers.Adam(0.0011),\n",
    "        # optimizer = tf.keras.optimizers.Adam(1e-2),\n",
    "        # optimizer=tf.keras.optimizers.Adam(1.1e-3),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        # loss=\"categorical_crossentropy\",\n",
    "        # metrics=[\"accuracy\",dice_coef,\"categorical_accuracy\"],\n",
    "        # metrics=[\"categorical_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=4)],\n",
    "        metrics=[\"binary_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=2)],\n",
    "        # metrics=[\"categorical_accuracy\"],\n",
    ")\n",
    "# CREATE CALLBACKS\n",
    "checkpointClassification = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        checkpointPath / Path('efficientnet_Classification_acc'),\n",
    "                        monitor='val_binary_accuracy', verbose=0, \n",
    "                        # monitor='val_dm_pignistic_16_categorical_accuracy', verbose=0, \n",
    "                        save_best_only=True, mode='max')\n",
    "                                                # CREATE CALLBACKS\n",
    "\n",
    "\n",
    "checkpointClassificationF1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        checkpointPath / Path('efficientnet_Classification_F1'), \n",
    "                        monitor='val_f1_score', verbose=0, \n",
    "                        save_best_only=True, mode='max')\n",
    "                                                # CREATE CALLBACKS\n",
    "\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "\n",
    "\n",
    "    \n",
    "lr_finder = LRFinder(min_lr=1e-3,\n",
    "                     max_lr=1e-3*9,\n",
    "                             steps_per_epoch=np.ceil(X_train_IMGS.shape[0]/batch_size), \n",
    "                             epochs=10)\n",
    "\n",
    "\n",
    "schedule_lr = SGDRScheduler(min_lr=0.0009,\n",
    "                                max_lr=0.01,\n",
    "                                steps_per_epoch=np.ceil(X_train_IMGS.shape[0]/batch_size),\n",
    "                                lr_decay=0.8,\n",
    "                                cycle_length=10,\n",
    "                                mult_factor=3.5)\n",
    "\n",
    "\n",
    "    \n",
    "# lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "\n",
    "cb = TimingCallback()\n",
    "\n",
    "print(checkpointPath / Path('efficientnet'))\n",
    "\n",
    "# callbacks_list = [checkpointClassification]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb, schedule_lr,lrate]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb, schedule_lr]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, cb, schedule_lr]\n",
    "callbacks_list = [checkpointClassification, checkpointClassificationF1, cb]\n",
    "# callbacks_list = [checkpointClassification, cb]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb]\n",
    "# callbacks_list = [checkpointClassificationF1, earlyStopping]\n",
    "\n",
    "history = efficientnet.fit(\n",
    "        # x = X_train_IMGS,\n",
    "        # trainset,\n",
    "        x = X_train_IMGS,\n",
    "        # y = [Y_train,Y_tr_unc],\n",
    "        y = Y_train,\n",
    "        # epochs = 1,\n",
    "        epochs = 500,\n",
    "        validation_data = [X_val_IMGS,Y_val],\n",
    "        # validation_data = [X_val_IMGS,[Y_val,Y_val_unc]],\n",
    "        # validation_data = validset,\n",
    "        callbacks = callbacks_list,    #some problems with DS layers and callbacks\n",
    "        batch_size=32,\n",
    "        # batch_size=1,\n",
    "        # class_weight=c_w,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train loss",
         "type": "scatter",
         "y": [
          0.9769638180732727,
          0.9066765904426575,
          0.8672434687614441,
          0.826961100101471,
          0.8125013709068298,
          0.7988059520721436,
          0.7739643454551697,
          0.7746745347976685,
          0.7640342116355896,
          0.7591609358787537,
          0.7431185841560364,
          0.7360103130340576,
          0.7235609292984009,
          0.7348844408988953,
          0.7382780909538269,
          0.7333031892776489,
          0.7139852643013,
          0.7145570516586304,
          0.7104490399360657,
          0.7206447124481201,
          0.7116873860359192,
          0.715649425983429,
          0.7074103951454163,
          0.7030133605003357,
          0.6876200437545776,
          0.696061909198761,
          0.6905370950698853,
          0.6955403089523315,
          0.6910789608955383,
          0.6900070309638977,
          0.69223552942276,
          0.6897599101066589,
          0.685036838054657,
          0.6911585330963135,
          0.6889405846595764,
          0.6873699426651001,
          0.6849827170372009,
          0.6921257972717285,
          0.6840041875839233,
          0.6805689930915833,
          0.6921513080596924,
          0.6844434142112732,
          0.6832740902900696,
          0.6786497831344604,
          0.6853370070457458,
          0.6799448132514954,
          0.6746293902397156,
          0.6760474443435669,
          0.6827813982963562,
          0.6740161180496216,
          0.6740719079971313,
          0.676582396030426,
          0.6806856393814087,
          0.6812427639961243,
          0.6664899587631226,
          0.6731923222541809,
          0.6789402961730957,
          0.6712737083435059,
          0.6712419986724854,
          0.6772595047950745,
          0.6662985682487488,
          0.6701352000236511,
          0.6606972217559814,
          0.6635221242904663,
          0.6636217832565308,
          0.6690590977668762,
          0.6636708974838257,
          0.6594995260238647,
          0.6553524732589722,
          0.6554676294326782,
          0.6545737981796265,
          0.6545347571372986,
          0.6578709483146667,
          0.649257481098175,
          0.6552913784980774,
          0.6561349630355835,
          0.66371089220047,
          0.655309796333313,
          0.6592431664466858,
          0.6537129878997803,
          0.6522097587585449,
          0.6589205861091614,
          0.6428748369216919,
          0.6554375290870667,
          0.6494208574295044,
          0.6618660688400269,
          0.6509832143783569,
          0.6476410031318665,
          0.6487171649932861,
          0.6468876600265503,
          0.6473995447158813,
          0.6551991105079651,
          0.6441883444786072,
          0.6529212594032288,
          0.6515859961509705,
          0.6465561985969543,
          0.6471056938171387,
          0.643832266330719,
          0.6535255312919617,
          0.6479498147964478,
          0.6363504528999329,
          0.6399410367012024,
          0.6483474373817444,
          0.6397149562835693,
          0.644861102104187,
          0.6447018384933472,
          0.6336911916732788,
          0.6408960223197937,
          0.6400253772735596,
          0.6403223276138306,
          0.6350648999214172,
          0.6378508806228638,
          0.6350312232971191,
          0.6374075412750244,
          0.6361255049705505,
          0.6367334127426147,
          0.6295101046562195,
          0.6356160044670105,
          0.647377610206604,
          0.6343488097190857,
          0.6309665441513062,
          0.6332473158836365,
          0.6309584379196167,
          0.6407955884933472,
          0.6408707499504089,
          0.6349323391914368,
          0.6408840417861938,
          0.6401307582855225,
          0.6307462453842163,
          0.6292981505393982,
          0.6334080696105957,
          0.636276364326477,
          0.6283527612686157,
          0.6269901990890503,
          0.631445586681366,
          0.6271740794181824,
          0.6245734095573425,
          0.6223618388175964,
          0.6299944519996643,
          0.6250702142715454,
          0.6186964511871338,
          0.6215100288391113,
          0.6323971748352051,
          0.6290923357009888,
          0.6259698271751404,
          0.6299315690994263,
          0.6327071785926819,
          0.6270477175712585,
          0.6348831057548523,
          0.6226713061332703,
          0.6276335716247559,
          0.6190435886383057,
          0.6313211917877197,
          0.630310595035553,
          0.6233657002449036,
          0.6161964535713196,
          0.6258302330970764,
          0.6331623792648315,
          0.622624933719635,
          0.6261960864067078,
          0.6215120553970337,
          0.6273582577705383,
          0.6291083693504333,
          0.6196382641792297,
          0.6167800426483154,
          0.6184310913085938,
          0.6191510558128357,
          0.6109184622764587,
          0.6146419048309326,
          0.6315789818763733,
          0.618263840675354,
          0.6234891414642334,
          0.6123554706573486,
          0.6185243725776672,
          0.6166775226593018,
          0.6164849400520325,
          0.6282639503479004,
          0.6178765892982483,
          0.6231215596199036,
          0.6253226399421692,
          0.609773576259613,
          0.6177567839622498,
          0.6215769648551941,
          0.6179607510566711,
          0.6106898784637451,
          0.6206991672515869,
          0.6126307845115662,
          0.6195980310440063,
          0.6182182431221008,
          0.626433789730072,
          0.6196982264518738,
          0.6115970015525818,
          0.610587477684021,
          0.6128086447715759,
          0.6090145707130432,
          0.613767683506012,
          0.6130965948104858,
          0.6193147301673889,
          0.619598925113678,
          0.6083452105522156,
          0.6109904646873474,
          0.6084656119346619,
          0.6062926650047302,
          0.6103053689002991,
          0.6092942953109741,
          0.6220946311950684,
          0.6100854277610779,
          0.6224851012229919,
          0.6131036877632141,
          0.6052068471908569,
          0.6096426844596863,
          0.6053222417831421,
          0.616619348526001,
          0.6020801067352295,
          0.6021941900253296,
          0.6079292893409729,
          0.6043900847434998,
          0.6090472340583801,
          0.6070474982261658,
          0.6032720804214478,
          0.6109145879745483,
          0.6015181541442871,
          0.6048802137374878,
          0.6125098466873169,
          0.6074573993682861,
          0.6072267293930054,
          0.6090350151062012,
          0.6073576807975769,
          0.597550094127655,
          0.6039531826972961,
          0.6028507351875305,
          0.5966466665267944,
          0.6087385416030884,
          0.6160120368003845,
          0.6064297556877136,
          0.6148545145988464,
          0.610310435295105,
          0.6097510457038879,
          0.6117951273918152,
          0.6093997359275818,
          0.6099997758865356,
          0.6043784618377686,
          0.6048662662506104,
          0.600907027721405,
          0.6016547083854675,
          0.5979402661323547,
          0.608025848865509,
          0.5983652472496033,
          0.5988669395446777,
          0.6052542328834534,
          0.6057679057121277,
          0.6033336520195007,
          0.5986343026161194,
          0.606743335723877,
          0.6014302372932434,
          0.6131479740142822,
          0.6074051856994629,
          0.6062980890274048,
          0.6025410294532776,
          0.5973694324493408,
          0.607560396194458,
          0.602361261844635,
          0.6072964072227478,
          0.6028584837913513,
          0.6023266911506653,
          0.603028416633606,
          0.6076685786247253,
          0.6051203608512878,
          0.6023968458175659,
          0.6094272136688232,
          0.5994135737419128,
          0.596030056476593,
          0.6036052703857422,
          0.5939878225326538,
          0.60626620054245,
          0.5990645289421082,
          0.5876227617263794,
          0.6019331216812134,
          0.5997693538665771,
          0.5954304337501526,
          0.6030787825584412,
          0.5987679362297058,
          0.6040940880775452,
          0.5969997048377991,
          0.6002674698829651,
          0.5936050415039062,
          0.593244194984436,
          0.6072024703025818,
          0.593217134475708,
          0.6045414209365845,
          0.6003832817077637,
          0.5936554074287415,
          0.5996261239051819,
          0.6022447943687439,
          0.5864312052726746,
          0.6007343530654907,
          0.5944134593009949,
          0.6023821830749512,
          0.5939834713935852,
          0.5988677740097046,
          0.6027387976646423,
          0.5959365367889404,
          0.595629096031189,
          0.5935914516448975,
          0.5874682068824768,
          0.5876054167747498,
          0.5956441760063171,
          0.5932856798171997,
          0.5973728895187378,
          0.5932875275611877,
          0.5897688865661621,
          0.5996472835540771,
          0.5902410745620728,
          0.5842683911323547,
          0.5956711769104004,
          0.5959315896034241,
          0.588336706161499,
          0.5949665904045105,
          0.5963088274002075,
          0.5918363928794861,
          0.5986449718475342,
          0.5929856896400452,
          0.5879546999931335,
          0.587100088596344,
          0.5861148238182068,
          0.5898497700691223,
          0.5987138152122498,
          0.5847891569137573,
          0.5905853509902954,
          0.5879484415054321,
          0.5931478142738342,
          0.5946895480155945,
          0.5948424339294434,
          0.5869201421737671,
          0.5872988104820251,
          0.5969670414924622,
          0.5921611189842224,
          0.5918874144554138,
          0.5892747044563293,
          0.5859972834587097,
          0.5906208753585815,
          0.577486515045166,
          0.5822680592536926,
          0.5961557030677795,
          0.597451388835907,
          0.5844954252243042,
          0.5978976488113403,
          0.5908693075180054,
          0.5880014896392822,
          0.5846986174583435,
          0.5789756178855896,
          0.5836480259895325,
          0.5798248648643494,
          0.5917077660560608,
          0.5799724459648132,
          0.593478262424469,
          0.5890139937400818,
          0.5939278602600098,
          0.5860175490379333,
          0.58547443151474,
          0.5833629965782166,
          0.5846943855285645,
          0.5885056257247925,
          0.5861895084381104,
          0.5758033990859985,
          0.5827078223228455,
          0.584162175655365,
          0.5810825824737549,
          0.5763447284698486,
          0.5846832394599915,
          0.5805790424346924,
          0.589701771736145,
          0.5811387300491333,
          0.5882137417793274,
          0.5846055150032043,
          0.5841597318649292,
          0.5881054997444153,
          0.5791581869125366,
          0.5862665772438049,
          0.5864579677581787,
          0.5744096636772156,
          0.5881262421607971,
          0.5756651759147644,
          0.5707860589027405,
          0.5736268758773804,
          0.5831290483474731,
          0.5748679041862488,
          0.5893371105194092,
          0.5793358087539673,
          0.5797091722488403,
          0.5809699296951294,
          0.5830747485160828,
          0.58668452501297,
          0.5961960554122925,
          0.5688181519508362,
          0.586720883846283,
          0.5791893601417542,
          0.5751132965087891,
          0.5867336988449097,
          0.5859808921813965,
          0.5705885887145996,
          0.5839253067970276,
          0.5843151807785034,
          0.567314088344574,
          0.5910606980323792,
          0.5817039608955383,
          0.5827532410621643,
          0.5700116753578186,
          0.5690030455589294,
          0.5775742530822754,
          0.5759538412094116,
          0.5822691917419434,
          0.5708757638931274,
          0.5756422281265259,
          0.5731737613677979,
          0.587114691734314,
          0.5644616484642029,
          0.5845670104026794,
          0.5773330330848694,
          0.5781025290489197,
          0.5790500044822693,
          0.5789270997047424,
          0.5798056125640869,
          0.575996994972229,
          0.5730617642402649,
          0.5700296759605408,
          0.5896750688552856,
          0.569643497467041,
          0.5697445869445801,
          0.5614235401153564,
          0.5754470229148865,
          0.5747076272964478,
          0.5731572508811951,
          0.5777938365936279,
          0.5741723775863647,
          0.5701291561126709,
          0.575529158115387,
          0.58216792345047,
          0.5721519589424133,
          0.5867361426353455,
          0.5688278079032898,
          0.5787465572357178,
          0.5676088929176331,
          0.5817215442657471,
          0.576553225517273,
          0.5558106899261475,
          0.5808574557304382,
          0.5641646385192871,
          0.581828236579895,
          0.577452540397644,
          0.5758234858512878,
          0.5766109228134155,
          0.56528240442276,
          0.5619418621063232,
          0.5812340974807739,
          0.5754272937774658,
          0.5687716007232666,
          0.5622684955596924,
          0.5788857936859131,
          0.568286120891571,
          0.5798439383506775,
          0.5792170763015747,
          0.5639839768409729,
          0.5775371193885803,
          0.5699102282524109,
          0.5903700590133667,
          0.5627267956733704,
          0.5689628720283508,
          0.5744155049324036,
          0.5723426938056946,
          0.5732374787330627,
          0.5682741403579712,
          0.587232768535614,
          0.5710368156433105,
          0.5665494203567505,
          0.5659992098808289,
          0.5577455759048462,
          0.5745752453804016,
          0.5725597143173218,
          0.5684128999710083,
          0.5792011022567749,
          0.5627042651176453,
          0.5692142844200134,
          0.5722586512565613,
          0.5814001560211182,
          0.5665613412857056,
          0.5621998906135559,
          0.5679972171783447,
          0.574400782585144,
          0.5778807997703552,
          0.5640565752983093,
          0.5619167685508728,
          0.5689996480941772,
          0.5719583034515381,
          0.57511967420578,
          0.5619251728057861,
          0.571569561958313,
          0.5560852885246277,
          0.5701051950454712,
          0.5613879561424255
         ]
        },
        {
         "name": "Validation loss",
         "type": "scatter",
         "y": [
          0.70383220911026,
          0.6993272304534912,
          0.698340117931366,
          0.6979849934577942,
          0.6978557705879211,
          0.6976317167282104,
          0.6970247626304626,
          0.6964415311813354,
          0.6955859065055847,
          0.6951791048049927,
          0.6943536996841431,
          0.6936013102531433,
          0.6921318769454956,
          0.6904938220977783,
          0.6892297863960266,
          0.6877011060714722,
          0.6864243745803833,
          0.6851471662521362,
          0.6843992471694946,
          0.6841874122619629,
          0.6837009787559509,
          0.6835983991622925,
          0.683189868927002,
          0.6825196146965027,
          0.6820204257965088,
          0.681690514087677,
          0.6811094284057617,
          0.6806478500366211,
          0.680274486541748,
          0.6801231503486633,
          0.6796144247055054,
          0.6789565682411194,
          0.6783531308174133,
          0.6780737638473511,
          0.6780180931091309,
          0.677574634552002,
          0.6768779754638672,
          0.6765185594558716,
          0.6760167479515076,
          0.6749267578125,
          0.6743751764297485,
          0.6737604141235352,
          0.6731319427490234,
          0.6722806096076965,
          0.6714445948600769,
          0.6709442734718323,
          0.6706916689872742,
          0.6700080633163452,
          0.669464111328125,
          0.6688721179962158,
          0.6684842705726624,
          0.6678406596183777,
          0.6670302748680115,
          0.6664304733276367,
          0.6655834317207336,
          0.6646842360496521,
          0.6639997959136963,
          0.6637706756591797,
          0.6631799936294556,
          0.662548840045929,
          0.6621565818786621,
          0.6618931293487549,
          0.6615009903907776,
          0.6606810688972473,
          0.6604077219963074,
          0.6598741412162781,
          0.6596238017082214,
          0.6587618589401245,
          0.658157467842102,
          0.6572650074958801,
          0.6562921404838562,
          0.6557304859161377,
          0.6552554368972778,
          0.6541408896446228,
          0.6533780694007874,
          0.6530019640922546,
          0.6521938443183899,
          0.6515591740608215,
          0.6513147950172424,
          0.651828408241272,
          0.6512680053710938,
          0.6515039205551147,
          0.6513864398002625,
          0.6509935259819031,
          0.6507529020309448,
          0.6507833003997803,
          0.6502630114555359,
          0.6496280431747437,
          0.6491504311561584,
          0.6486839652061462,
          0.6481108665466309,
          0.647546648979187,
          0.6475440859794617,
          0.6474892497062683,
          0.6479864716529846,
          0.6479547619819641,
          0.647770345211029,
          0.6473501920700073,
          0.6473338603973389,
          0.6475759744644165,
          0.6468489170074463,
          0.6470814347267151,
          0.6477410197257996,
          0.6477884650230408,
          0.6480054259300232,
          0.6482771039009094,
          0.6484149694442749,
          0.6483723521232605,
          0.6484329700469971,
          0.6480998992919922,
          0.6477930545806885,
          0.6482754349708557,
          0.648622453212738,
          0.6485623717308044,
          0.6481067538261414,
          0.6478279232978821,
          0.6475307941436768,
          0.6474428772926331,
          0.6478139758110046,
          0.6481099128723145,
          0.6480124592781067,
          0.6481272578239441,
          0.6479764580726624,
          0.6482557654380798,
          0.6485564112663269,
          0.6489809155464172,
          0.649025022983551,
          0.649135172367096,
          0.6495899558067322,
          0.6497470140457153,
          0.6493661403656006,
          0.6489537358283997,
          0.6489540934562683,
          0.6490335464477539,
          0.6490382552146912,
          0.6494341492652893,
          0.6490753293037415,
          0.6486983895301819,
          0.6490644216537476,
          0.6487755179405212,
          0.6493688821792603,
          0.6493513584136963,
          0.6492655277252197,
          0.6490504741668701,
          0.6489872336387634,
          0.6492310762405396,
          0.649384617805481,
          0.6497195363044739,
          0.6496673226356506,
          0.6501541137695312,
          0.6502429246902466,
          0.6505641341209412,
          0.6505720019340515,
          0.650982141494751,
          0.6511298418045044,
          0.6505985856056213,
          0.6512147784233093,
          0.6509075164794922,
          0.6503952741622925,
          0.6505482792854309,
          0.650439977645874,
          0.6505401730537415,
          0.6508142948150635,
          0.6512077450752258,
          0.6516417860984802,
          0.65226149559021,
          0.6523436903953552,
          0.652325451374054,
          0.6530206203460693,
          0.6529374122619629,
          0.6532067060470581,
          0.6528830528259277,
          0.6525592803955078,
          0.6532161831855774,
          0.653605580329895,
          0.6538498401641846,
          0.6543947458267212,
          0.6545469760894775,
          0.6549046635627747,
          0.6551001071929932,
          0.6550706624984741,
          0.655231773853302,
          0.6553586721420288,
          0.6551045775413513,
          0.6548205018043518,
          0.6547675728797913,
          0.6548325419425964,
          0.6548115015029907,
          0.6551297307014465,
          0.6550215482711792,
          0.6551922559738159,
          0.6551929712295532,
          0.6558123826980591,
          0.6555099487304688,
          0.6555163860321045,
          0.6555159091949463,
          0.6560564041137695,
          0.6562048196792603,
          0.6562499403953552,
          0.6568769812583923,
          0.6566715240478516,
          0.6567822098731995,
          0.6575592756271362,
          0.6583356261253357,
          0.6587675213813782,
          0.6587931513786316,
          0.6591014266014099,
          0.6590690016746521,
          0.659066915512085,
          0.6586176156997681,
          0.6583356261253357,
          0.6594091057777405,
          0.6599310636520386,
          0.6592893600463867,
          0.6592680215835571,
          0.6592733263969421,
          0.6601354479789734,
          0.6602236032485962,
          0.6604543924331665,
          0.6606841087341309,
          0.6614165306091309,
          0.6612575054168701,
          0.6621271967887878,
          0.6626709699630737,
          0.6627189517021179,
          0.6621847152709961,
          0.6623745560646057,
          0.6622331142425537,
          0.6627746224403381,
          0.6632018089294434,
          0.6638777852058411,
          0.6641054749488831,
          0.6642120480537415,
          0.6644693613052368,
          0.6644673943519592,
          0.6637751460075378,
          0.6636471152305603,
          0.6635631918907166,
          0.6628198623657227,
          0.6628457307815552,
          0.6625154614448547,
          0.6618160605430603,
          0.6616722345352173,
          0.662378191947937,
          0.6618898510932922,
          0.6621042490005493,
          0.6620998382568359,
          0.6625576615333557,
          0.6626214385032654,
          0.6623039841651917,
          0.6614544987678528,
          0.661126434803009,
          0.6610735058784485,
          0.6610353589057922,
          0.6610422730445862,
          0.661078929901123,
          0.6601888537406921,
          0.6602980494499207,
          0.6603935360908508,
          0.6601541638374329,
          0.660472571849823,
          0.6600774526596069,
          0.6595554947853088,
          0.6597716808319092,
          0.6598789691925049,
          0.6596748232841492,
          0.6596041917800903,
          0.6607852578163147,
          0.6606892347335815,
          0.6601019501686096,
          0.6603950262069702,
          0.6605521440505981,
          0.6611123085021973,
          0.661736786365509,
          0.6622188687324524,
          0.6631944179534912,
          0.6635984182357788,
          0.6640217304229736,
          0.6647278666496277,
          0.664674699306488,
          0.6644955277442932,
          0.6642796397209167,
          0.6640856862068176,
          0.6646245121955872,
          0.6651790738105774,
          0.6654521226882935,
          0.6655542850494385,
          0.6650833487510681,
          0.6652175188064575,
          0.6649366021156311,
          0.6647925972938538,
          0.6650797128677368,
          0.6648271679878235,
          0.664527952671051,
          0.6649467349052429,
          0.6650831699371338,
          0.6653827428817749,
          0.6648955345153809,
          0.6643941402435303,
          0.6648356318473816,
          0.6647127270698547,
          0.665381669998169,
          0.6657410860061646,
          0.6658198833465576,
          0.6664778590202332,
          0.6662333011627197,
          0.6662620902061462,
          0.6658229827880859,
          0.6650175452232361,
          0.6652674078941345,
          0.6653147339820862,
          0.6654394865036011,
          0.665214478969574,
          0.6666576862335205,
          0.6669012308120728,
          0.6678704619407654,
          0.6682590246200562,
          0.6684104204177856,
          0.6675089001655579,
          0.6679529547691345,
          0.6675487756729126,
          0.6673133969306946,
          0.6680812835693359,
          0.6678631901741028,
          0.6681969165802002,
          0.66852867603302,
          0.6685982346534729,
          0.6686761379241943,
          0.6692854166030884,
          0.6690506935119629,
          0.6705471873283386,
          0.6710946559906006,
          0.6710981726646423,
          0.672173798084259,
          0.6721640825271606,
          0.671711266040802,
          0.6720346212387085,
          0.6715753674507141,
          0.6711316704750061,
          0.6713981032371521,
          0.6716638803482056,
          0.6714445352554321,
          0.6716784238815308,
          0.6715872883796692,
          0.6723157167434692,
          0.6726350784301758,
          0.6728620529174805,
          0.6734763383865356,
          0.6729991436004639,
          0.6738146543502808,
          0.6746302247047424,
          0.6752166152000427,
          0.675785481929779,
          0.6740718483924866,
          0.6753004789352417,
          0.6761775016784668,
          0.6763380765914917,
          0.676071047782898,
          0.6760289669036865,
          0.6755285859107971,
          0.6757852435112,
          0.6756036877632141,
          0.6757230758666992,
          0.6760551929473877,
          0.6769067049026489,
          0.6763245463371277,
          0.6768433451652527,
          0.6768336892127991,
          0.677204966545105,
          0.6774670481681824,
          0.677715539932251,
          0.6783707737922668,
          0.6794014573097229,
          0.6794840693473816,
          0.6795757412910461,
          0.6789061427116394,
          0.6794356107711792,
          0.679261326789856,
          0.6784626841545105,
          0.6789675354957581,
          0.6789976358413696,
          0.6789568066596985,
          0.6788963675498962,
          0.679111897945404,
          0.6791080832481384,
          0.6807460188865662,
          0.6819528341293335,
          0.682355523109436,
          0.6815821528434753,
          0.681682288646698,
          0.6824922561645508,
          0.6824559569358826,
          0.6818033456802368,
          0.6813254356384277,
          0.6811338663101196,
          0.6813941597938538,
          0.6812273263931274,
          0.681030809879303,
          0.6808753609657288,
          0.6811666488647461,
          0.6809832453727722,
          0.6817594170570374,
          0.6815976500511169,
          0.6814113855361938,
          0.6821745038032532,
          0.682421088218689,
          0.6820321083068848,
          0.6825480461120605,
          0.6836562156677246,
          0.6835707426071167,
          0.6836758852005005,
          0.6836851239204407,
          0.6833263635635376,
          0.6824122667312622,
          0.6820955276489258,
          0.6817522644996643,
          0.6821702122688293,
          0.6824614405632019,
          0.6821539402008057,
          0.6821180582046509,
          0.6825387477874756,
          0.682386040687561,
          0.6813724040985107,
          0.6816288828849792,
          0.6827871203422546,
          0.6828291416168213,
          0.6818494200706482,
          0.6818450093269348,
          0.6817455291748047,
          0.6827912330627441,
          0.6828761100769043,
          0.6830533146858215,
          0.6830903887748718,
          0.6829935312271118,
          0.683341383934021,
          0.6835838556289673,
          0.6844759583473206,
          0.684027910232544,
          0.6841884255409241,
          0.6838934421539307,
          0.6837930083274841,
          0.6831377744674683,
          0.6827418804168701,
          0.6818166375160217,
          0.6823391318321228,
          0.6834295988082886,
          0.6839573383331299,
          0.6849633455276489,
          0.6849420070648193,
          0.6849967241287231,
          0.6851931810379028,
          0.6856638789176941,
          0.6861279010772705,
          0.6864079833030701,
          0.6870964169502258,
          0.6869223713874817,
          0.6869519352912903,
          0.6882005333900452,
          0.687726616859436,
          0.6870889663696289,
          0.6871881484985352,
          0.6873272061347961,
          0.6891530752182007,
          0.6887823343276978,
          0.6887325048446655,
          0.6874158978462219,
          0.6882606148719788,
          0.6885323524475098,
          0.688361406326294,
          0.6890935301780701,
          0.6883748173713684,
          0.6887791156768799,
          0.6884797215461731,
          0.6882201433181763,
          0.6878819465637207,
          0.6879220008850098,
          0.688167154788971,
          0.6876949071884155,
          0.6876265406608582,
          0.6873875260353088,
          0.6879159808158875,
          0.6879737377166748,
          0.6887509226799011,
          0.6888716816902161,
          0.6881334185600281,
          0.6882755756378174,
          0.6897492408752441,
          0.6898641586303711,
          0.6891929507255554,
          0.6891504526138306,
          0.6888425946235657,
          0.6891224980354309,
          0.6897070407867432,
          0.6899757981300354,
          0.6906756162643433,
          0.6898841261863708,
          0.6901495456695557,
          0.6905401945114136,
          0.690636157989502,
          0.6899339556694031
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Loss EFFICIENTNET"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train accuracy",
         "type": "scatter",
         "y": [
          0.4848790466785431,
          0.4848790466785431,
          0.4858871102333069,
          0.4848790466785431,
          0.4858871102333069,
          0.484375,
          0.4879032373428345,
          0.4848790466785431,
          0.4863911271095276,
          0.48941531777381897,
          0.4868951737880707,
          0.4858871102333069,
          0.49445563554763794,
          0.4813508093357086,
          0.4868951737880707,
          0.4808467626571655,
          0.49042338132858276,
          0.49042338132858276,
          0.4969758093357086,
          0.4868951737880707,
          0.5005040168762207,
          0.49495968222618103,
          0.5,
          0.4979838728904724,
          0.5035282373428345,
          0.5045362710952759,
          0.4984878897666931,
          0.5005040168762207,
          0.5201612710952759,
          0.5110887289047241,
          0.5307459831237793,
          0.5221773982048035,
          0.5302419066429138,
          0.5186492204666138,
          0.5257056355476379,
          0.524193525314331,
          0.5267137289047241,
          0.522681474685669,
          0.5347782373428345,
          0.5599798560142517,
          0.5504032373428345,
          0.5498992204666138,
          0.546875,
          0.546875,
          0.5428427457809448,
          0.5569556355476379,
          0.571068525314331,
          0.5488911271095276,
          0.5453628897666931,
          0.5584677457809448,
          0.5640121102333069,
          0.5569556355476379,
          0.5675403475761414,
          0.5630040168762207,
          0.578125,
          0.5594757795333862,
          0.5720766186714172,
          0.5811492204666138,
          0.5589717626571655,
          0.5614919066429138,
          0.5871976017951965,
          0.5609878897666931,
          0.5811492204666138,
          0.602318525314331,
          0.5846773982048035,
          0.5897177457809448,
          0.5997983813285828,
          0.5907257795333862,
          0.5892137289047241,
          0.6234878897666931,
          0.5791330933570862,
          0.6219757795333862,
          0.5841733813285828,
          0.6290322542190552,
          0.6129032373428345,
          0.6265121102333069,
          0.6083669066429138,
          0.6169354915618896,
          0.5887096524238586,
          0.6239919066429138,
          0.6129032373428345,
          0.6098790168762207,
          0.6411290168762207,
          0.609375,
          0.6123992204666138,
          0.616431474685669,
          0.6229838728904724,
          0.6144153475761414,
          0.6290322542190552,
          0.6381048560142517,
          0.6456653475761414,
          0.6275201439857483,
          0.6370967626571655,
          0.6436492204666138,
          0.632056474685669,
          0.640625,
          0.6219757795333862,
          0.6305443644523621,
          0.6391128897666931,
          0.6436492204666138,
          0.6451612710952759,
          0.6532257795333862,
          0.6421371102333069,
          0.633568525314331,
          0.6547378897666931,
          0.6295362710952759,
          0.6643145084381104,
          0.6612903475761414,
          0.6643145084381104,
          0.6557459831237793,
          0.6542338728904724,
          0.664818525314331,
          0.6577621102333069,
          0.647681474685669,
          0.6381048560142517,
          0.6421371102333069,
          0.6496976017951965,
          0.6617943644523621,
          0.6436492204666138,
          0.6507056355476379,
          0.6602822542190552,
          0.6587701439857483,
          0.6643145084381104,
          0.6592742204666138,
          0.6688507795333862,
          0.6512096524238586,
          0.6678427457809448,
          0.6486895084381104,
          0.6799395084381104,
          0.6698588728904724,
          0.6582661271095276,
          0.6743951439857483,
          0.6552419066429138,
          0.6557459831237793,
          0.6698588728904724,
          0.6834677457809448,
          0.6643145084381104,
          0.6890121102333069,
          0.6577621102333069,
          0.6824596524238586,
          0.6779233813285828,
          0.6723790168762207,
          0.6527217626571655,
          0.6754032373428345,
          0.6738911271095276,
          0.6784273982048035,
          0.6602822542190552,
          0.6905242204666138,
          0.6451612710952759,
          0.678931474685669,
          0.6693548560142517,
          0.6885080933570862,
          0.6678427457809448,
          0.6638104915618896,
          0.680443525314331,
          0.680443525314331,
          0.6743951439857483,
          0.6764112710952759,
          0.6940523982048035,
          0.6759072542190552,
          0.6814516186714172,
          0.6754032373428345,
          0.6733871102333069,
          0.6915322542190552,
          0.6849798560142517,
          0.6859878897666931,
          0.6764112710952759,
          0.6713709831237793,
          0.6638104915618896,
          0.6829637289047241,
          0.6834677457809448,
          0.6890121102333069,
          0.6885080933570862,
          0.6678427457809448,
          0.7011088728904724,
          0.6859878897666931,
          0.6794354915618896,
          0.6869959831237793,
          0.6985887289047241,
          0.6819556355476379,
          0.6794354915618896,
          0.6950604915618896,
          0.6754032373428345,
          0.6955645084381104,
          0.6774193644523621,
          0.7056451439857483,
          0.6900201439857483,
          0.6920362710952759,
          0.696068525314331,
          0.6693548560142517,
          0.6809476017951965,
          0.7106854915618896,
          0.6925403475761414,
          0.7147177457809448,
          0.6900201439857483,
          0.711693525314331,
          0.7066532373428345,
          0.6950604915618896,
          0.6875,
          0.7121976017951965,
          0.6875,
          0.711693525314331,
          0.7071572542190552,
          0.6995967626571655,
          0.7076612710952759,
          0.6663306355476379,
          0.6955645084381104,
          0.6663306355476379,
          0.6930443644523621,
          0.694556474685669,
          0.6985887289047241,
          0.7071572542190552,
          0.7076612710952759,
          0.7001007795333862,
          0.7006048560142517,
          0.7011088728904724,
          0.694556474685669,
          0.7071572542190552,
          0.7212701439857483,
          0.7001007795333862,
          0.6890121102333069,
          0.7137096524238586,
          0.710181474685669,
          0.703125,
          0.680443525314331,
          0.7066532373428345,
          0.7207661271095276,
          0.6849798560142517,
          0.6900201439857483,
          0.7056451439857483,
          0.703125,
          0.7111895084381104,
          0.6930443644523621,
          0.7001007795333862,
          0.6910282373428345,
          0.6975806355476379,
          0.6990927457809448,
          0.6980846524238586,
          0.7056451439857483,
          0.7212701439857483,
          0.7001007795333862,
          0.7242943644523621,
          0.7006048560142517,
          0.7106854915618896,
          0.7056451439857483,
          0.7106854915618896,
          0.710181474685669,
          0.7046371102333069,
          0.7207661271095276,
          0.7172378897666931,
          0.6990927457809448,
          0.6995967626571655,
          0.711693525314331,
          0.710181474685669,
          0.6869959831237793,
          0.7066532373428345,
          0.6869959831237793,
          0.6990927457809448,
          0.7127016186714172,
          0.71875,
          0.696068525314331,
          0.7182459831237793,
          0.7172378897666931,
          0.694556474685669,
          0.7091733813285828,
          0.7177419066429138,
          0.7086693644523621,
          0.7157257795333862,
          0.7121976017951965,
          0.7071572542190552,
          0.6990927457809448,
          0.7217742204666138,
          0.7086693644523621,
          0.7363911271095276,
          0.7212701439857483,
          0.7288306355476379,
          0.7152217626571655,
          0.7328628897666931,
          0.6950604915618896,
          0.7001007795333862,
          0.7147177457809448,
          0.7106854915618896,
          0.7121976017951965,
          0.7177419066429138,
          0.7016128897666931,
          0.7121976017951965,
          0.7127016186714172,
          0.7061492204666138,
          0.7182459831237793,
          0.7006048560142517,
          0.7086693644523621,
          0.7202621102333069,
          0.7056451439857483,
          0.6995967626571655,
          0.7237903475761414,
          0.7132056355476379,
          0.7162298560142517,
          0.7127016186714172,
          0.7152217626571655,
          0.7142137289047241,
          0.7167338728904724,
          0.71875,
          0.7177419066429138,
          0.727318525314331,
          0.7217742204666138,
          0.7439516186714172,
          0.7006048560142517,
          0.7263104915618896,
          0.7278226017951965,
          0.7232862710952759,
          0.7247983813285828,
          0.7142137289047241,
          0.7227822542190552,
          0.7288306355476379,
          0.7227822542190552,
          0.7157257795333862,
          0.7313507795333862,
          0.7172378897666931,
          0.7192540168762207,
          0.7328628897666931,
          0.7288306355476379,
          0.7202621102333069,
          0.7232862710952759,
          0.7207661271095276,
          0.71875,
          0.7237903475761414,
          0.7207661271095276,
          0.7348790168762207,
          0.7111895084381104,
          0.7318548560142517,
          0.7278226017951965,
          0.7328628897666931,
          0.7192540168762207,
          0.7389112710952759,
          0.7384072542190552,
          0.7293346524238586,
          0.7222782373428345,
          0.7217742204666138,
          0.734375,
          0.7278226017951965,
          0.7278226017951965,
          0.7167338728904724,
          0.7399193644523621,
          0.6995967626571655,
          0.6965726017951965,
          0.7142137289047241,
          0.7137096524238586,
          0.7177419066429138,
          0.725806474685669,
          0.734375,
          0.734375,
          0.7253023982048035,
          0.7323588728904724,
          0.7237903475761414,
          0.7308467626571655,
          0.7253023982048035,
          0.7278226017951965,
          0.7373992204666138,
          0.7162298560142517,
          0.7348790168762207,
          0.7308467626571655,
          0.7177419066429138,
          0.7283266186714172,
          0.7142137289047241,
          0.7489919066429138,
          0.7298387289047241,
          0.725806474685669,
          0.741431474685669,
          0.7515121102333069,
          0.7202621102333069,
          0.7313507795333862,
          0.7202621102333069,
          0.7424395084381104,
          0.7510080933570862,
          0.7333669066429138,
          0.7298387289047241,
          0.7288306355476379,
          0.7237903475761414,
          0.7283266186714172,
          0.7409273982048035,
          0.7338709831237793,
          0.7247983813285828,
          0.7389112710952759,
          0.7293346524238586,
          0.7419354915618896,
          0.7384072542190552,
          0.7515121102333069,
          0.7419354915618896,
          0.7419354915618896,
          0.7288306355476379,
          0.7510080933570862,
          0.7424395084381104,
          0.7177419066429138,
          0.7278226017951965,
          0.7389112710952759,
          0.7333669066429138,
          0.7263104915618896,
          0.7631048560142517,
          0.7479838728904724,
          0.7348790168762207,
          0.7449596524238586,
          0.7449596524238586,
          0.7127016186714172,
          0.7434476017951965,
          0.7424395084381104,
          0.7217742204666138,
          0.7298387289047241,
          0.7489919066429138,
          0.7434476017951965,
          0.7404233813285828,
          0.7404233813285828,
          0.7328628897666931,
          0.7298387289047241,
          0.7293346524238586,
          0.7373992204666138,
          0.7212701439857483,
          0.7409273982048035,
          0.734375,
          0.7358871102333069,
          0.7434476017951965,
          0.7328628897666931,
          0.7263104915618896,
          0.7368951439857483,
          0.7379032373428345,
          0.742943525314331,
          0.7419354915618896,
          0.7237903475761414,
          0.7368951439857483,
          0.7641128897666931,
          0.734375,
          0.7217742204666138,
          0.734375,
          0.7348790168762207,
          0.7323588728904724,
          0.7515121102333069,
          0.7580645084381104,
          0.7368951439857483,
          0.7454637289047241,
          0.734375,
          0.7394153475761414,
          0.7444556355476379,
          0.7449596524238586,
          0.7449596524238586,
          0.7066532373428345,
          0.7484878897666931,
          0.7575604915618896,
          0.7137096524238586,
          0.7439516186714172,
          0.7545362710952759,
          0.7263104915618896,
          0.75,
          0.7318548560142517,
          0.7600806355476379,
          0.7520161271095276,
          0.725806474685669,
          0.7389112710952759,
          0.7348790168762207,
          0.7449596524238586,
          0.7464717626571655,
          0.742943525314331,
          0.7167338728904724,
          0.7550403475761414,
          0.7399193644523621,
          0.7494959831237793,
          0.7550403475761414,
          0.7308467626571655,
          0.7510080933570862,
          0.7424395084381104,
          0.7489919066429138,
          0.7293346524238586,
          0.7409273982048035,
          0.7494959831237793,
          0.7580645084381104,
          0.7409273982048035,
          0.7373992204666138,
          0.7373992204666138,
          0.7626007795333862,
          0.7404233813285828,
          0.7313507795333862,
          0.7318548560142517,
          0.7313507795333862,
          0.7479838728904724,
          0.7454637289047241,
          0.7474798560142517,
          0.7313507795333862,
          0.7459677457809448,
          0.7479838728904724,
          0.7424395084381104,
          0.7479838728904724,
          0.7373992204666138,
          0.7469757795333862,
          0.7595766186714172,
          0.7575604915618896,
          0.7439516186714172,
          0.7484878897666931,
          0.7520161271095276,
          0.7469757795333862,
          0.7540322542190552,
          0.7379032373428345,
          0.7484878897666931
         ]
        },
        {
         "name": "Validation accuracy",
         "type": "scatter",
         "y": [
          0.4864864945411682,
          0.4864864945411682,
          0.4864864945411682,
          0.4864864945411682,
          0.4864864945411682,
          0.4864864945411682,
          0.4864864945411682,
          0.4864864945411682,
          0.4864864945411682,
          0.4864864945411682,
          0.4819819927215576,
          0.46846845746040344,
          0.46846845746040344,
          0.46396395564079285,
          0.45945945382118225,
          0.477477490901947,
          0.5045045018196106,
          0.5180180072784424,
          0.5405405163764954,
          0.5495495200157166,
          0.5495495200157166,
          0.5720720887184143,
          0.5585585832595825,
          0.5540540814399719,
          0.5540540814399719,
          0.5540540814399719,
          0.5675675868988037,
          0.5495495200157166,
          0.545045018196106,
          0.545045018196106,
          0.5540540814399719,
          0.5405405163764954,
          0.5495495200157166,
          0.5585585832595825,
          0.5540540814399719,
          0.5630630850791931,
          0.5630630850791931,
          0.5585585832595825,
          0.5585585832595825,
          0.5630630850791931,
          0.5675675868988037,
          0.5765765905380249,
          0.5810810923576355,
          0.5810810923576355,
          0.5900900959968567,
          0.5990990996360779,
          0.6126126050949097,
          0.6126126050949097,
          0.6126126050949097,
          0.6126126050949097,
          0.6126126050949097,
          0.6126126050949097,
          0.6171171069145203,
          0.630630612373352,
          0.630630612373352,
          0.6396396160125732,
          0.6396396160125732,
          0.6396396160125732,
          0.6441441178321838,
          0.6531531810760498,
          0.6531531810760498,
          0.6576576828956604,
          0.6666666865348816,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.6666666865348816,
          0.6666666865348816,
          0.6711711883544922,
          0.6711711883544922,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.684684693813324,
          0.684684693813324,
          0.6801801919937134,
          0.6801801919937134,
          0.6666666865348816,
          0.6756756901741028,
          0.6801801919937134,
          0.7027027010917664,
          0.6936936974525452,
          0.684684693813324,
          0.684684693813324,
          0.6711711883544922,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6666666865348816,
          0.6711711883544922,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.662162184715271,
          0.662162184715271,
          0.6666666865348816,
          0.662162184715271,
          0.662162184715271,
          0.6576576828956604,
          0.6486486196517944,
          0.6531531810760498,
          0.6486486196517944,
          0.6486486196517944,
          0.6531531810760498,
          0.6486486196517944,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6666666865348816,
          0.6711711883544922,
          0.6711711883544922,
          0.6711711883544922,
          0.662162184715271,
          0.662162184715271,
          0.6711711883544922,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6711711883544922,
          0.6711711883544922,
          0.6711711883544922,
          0.6711711883544922,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6711711883544922,
          0.6756756901741028,
          0.6756756901741028,
          0.6801801919937134,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6801801919937134,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6801801919937134,
          0.6801801919937134,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6756756901741028,
          0.6711711883544922,
          0.6666666865348816,
          0.662162184715271,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.662162184715271,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.662162184715271,
          0.662162184715271,
          0.6576576828956604,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.6666666865348816,
          0.6666666865348816,
          0.6666666865348816,
          0.662162184715271,
          0.6576576828956604,
          0.662162184715271,
          0.6531531810760498,
          0.6576576828956604,
          0.6576576828956604,
          0.6531531810760498,
          0.6576576828956604,
          0.662162184715271,
          0.6576576828956604,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.6666666865348816,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.6666666865348816,
          0.6666666865348816,
          0.662162184715271,
          0.662162184715271,
          0.6576576828956604,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.6576576828956604,
          0.6576576828956604,
          0.6531531810760498,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6531531810760498,
          0.6576576828956604,
          0.6576576828956604,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6531531810760498,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6576576828956604,
          0.6666666865348816,
          0.6576576828956604,
          0.662162184715271,
          0.662162184715271,
          0.6576576828956604,
          0.662162184715271,
          0.662162184715271,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6576576828956604,
          0.6576576828956604,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.6666666865348816,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.6666666865348816,
          0.662162184715271,
          0.662162184715271,
          0.6666666865348816,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6441441178321838,
          0.662162184715271,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.662162184715271,
          0.6576576828956604,
          0.6576576828956604,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.662162184715271,
          0.6576576828956604,
          0.6531531810760498,
          0.6441441178321838,
          0.6576576828956604,
          0.6441441178321838,
          0.6441441178321838,
          0.6396396160125732,
          0.6441441178321838,
          0.6531531810760498,
          0.6531531810760498,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.662162184715271,
          0.662162184715271,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6576576828956604,
          0.6576576828956604,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6486486196517944,
          0.6441441178321838,
          0.6441441178321838,
          0.6486486196517944,
          0.6441441178321838,
          0.6486486196517944,
          0.6441441178321838,
          0.6441441178321838,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6576576828956604,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6486486196517944,
          0.6441441178321838,
          0.6441441178321838,
          0.6441441178321838,
          0.6486486196517944,
          0.6531531810760498,
          0.6531531810760498,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6576576828956604,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6576576828956604,
          0.6486486196517944,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6531531810760498,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6441441178321838,
          0.6441441178321838,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6441441178321838,
          0.6441441178321838,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6441441178321838,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944,
          0.6486486196517944
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Accuracy EFFICIENTNET"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Accuracy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train F1 score",
         "type": "scatter",
         "y": [
          0.32654449343681335,
          0.32654449343681335,
          0.32871943712234497,
          0.32654449343681335,
          0.32871943712234497,
          0.32608693838119507,
          0.333047479391098,
          0.3282579481601715,
          0.3308870792388916,
          0.34064731001853943,
          0.3308870792388916,
          0.3325781524181366,
          0.3543001115322113,
          0.3339955508708954,
          0.3370756506919861,
          0.33351457118988037,
          0.3470696210861206,
          0.35019364953041077,
          0.37025007605552673,
          0.3610548973083496,
          0.371895432472229,
          0.37416237592697144,
          0.3843165934085846,
          0.3825789988040924,
          0.3854735493659973,
          0.39820075035095215,
          0.3857722580432892,
          0.40001100301742554,
          0.4245883524417877,
          0.41608893871307373,
          0.4501390755176544,
          0.4413348436355591,
          0.4497627913951874,
          0.4321708083152771,
          0.45179545879364014,
          0.4457310140132904,
          0.4638659358024597,
          0.4508408308029175,
          0.4698891043663025,
          0.5072705745697021,
          0.48975664377212524,
          0.49366796016693115,
          0.4965716600418091,
          0.49438032507896423,
          0.49760836362838745,
          0.5110534429550171,
          0.5285928845405579,
          0.5047627687454224,
          0.49413567781448364,
          0.5206193923950195,
          0.5318607687950134,
          0.5178495645523071,
          0.5287826061248779,
          0.5271713733673096,
          0.5435192584991455,
          0.5253588557243347,
          0.5415951013565063,
          0.5533806085586548,
          0.530189037322998,
          0.5330067276954651,
          0.5600782036781311,
          0.5364227294921875,
          0.5589520335197449,
          0.5789849162101746,
          0.5635472536087036,
          0.5732268691062927,
          0.5842431783676147,
          0.5707495212554932,
          0.5657206177711487,
          0.6122329235076904,
          0.5588597059249878,
          0.6071691513061523,
          0.5638532638549805,
          0.6112127304077148,
          0.5975956320762634,
          0.6098513007164001,
          0.5966218113899231,
          0.6047759056091309,
          0.5749342441558838,
          0.6092531681060791,
          0.6059256792068481,
          0.595853865146637,
          0.6374807357788086,
          0.598865270614624,
          0.6011200547218323,
          0.6108551025390625,
          0.6145502924919128,
          0.6072700619697571,
          0.6229492425918579,
          0.6297218203544617,
          0.6400830149650574,
          0.61713707447052,
          0.6302173137664795,
          0.6371031999588013,
          0.6290878057479858,
          0.6365005970001221,
          0.6134878396987915,
          0.622708261013031,
          0.6344531774520874,
          0.6341097354888916,
          0.6384118795394897,
          0.6466826796531677,
          0.6367127299308777,
          0.6264553070068359,
          0.6519826650619507,
          0.624747633934021,
          0.6635044813156128,
          0.6562603712081909,
          0.6613379716873169,
          0.6504656076431274,
          0.6522461175918579,
          0.6594176888465881,
          0.6551258563995361,
          0.6424630880355835,
          0.6322767734527588,
          0.6390101909637451,
          0.6443513035774231,
          0.6611024141311646,
          0.6385372877120972,
          0.6424741744995117,
          0.6566939949989319,
          0.6523805856704712,
          0.6596808433532715,
          0.6585330963134766,
          0.6634989976882935,
          0.6480813026428223,
          0.6708776950836182,
          0.6508976817131042,
          0.678606390953064,
          0.6670880317687988,
          0.6532064080238342,
          0.6726832389831543,
          0.6510685682296753,
          0.652195155620575,
          0.6684437394142151,
          0.6838251948356628,
          0.6602075099945068,
          0.6835029125213623,
          0.6563851237297058,
          0.6804332137107849,
          0.6785522103309631,
          0.6739416122436523,
          0.6513670682907104,
          0.6729563474655151,
          0.6726206541061401,
          0.6759103536605835,
          0.6601284742355347,
          0.6887874603271484,
          0.6422362327575684,
          0.6770833134651184,
          0.6684437394142151,
          0.6882771849632263,
          0.665960967540741,
          0.660980224609375,
          0.6769939661026001,
          0.6793298721313477,
          0.6742491722106934,
          0.679215133190155,
          0.6924024820327759,
          0.6723428964614868,
          0.6783539056777954,
          0.6712373495101929,
          0.6713055372238159,
          0.6935172080993652,
          0.681916356086731,
          0.6851562261581421,
          0.6773720979690552,
          0.6658140420913696,
          0.6668999791145325,
          0.6823663711547852,
          0.685196042060852,
          0.6863513588905334,
          0.6893342733383179,
          0.6661511659622192,
          0.7005363702774048,
          0.6814464330673218,
          0.6793298721313477,
          0.6871744990348816,
          0.69959557056427,
          0.6822888851165771,
          0.6783329248428345,
          0.6965031623840332,
          0.6713228821754456,
          0.6985859870910645,
          0.6754019260406494,
          0.7035518884658813,
          0.6894959211349487,
          0.6915310621261597,
          0.6955447196960449,
          0.6662895679473877,
          0.6824593544006348,
          0.7125189304351807,
          0.6925250291824341,
          0.7155859470367432,
          0.6854826211929321,
          0.71263587474823,
          0.7066290974617004,
          0.6935372352600098,
          0.6854839324951172,
          0.713699221611023,
          0.6892319321632385,
          0.7106357216835022,
          0.71263587474823,
          0.6975757479667664,
          0.7076601386070251,
          0.6662326455116272,
          0.6965698003768921,
          0.6653171181678772,
          0.69351726770401,
          0.6944442987442017,
          0.695559561252594,
          0.7066505551338196,
          0.7066505551338196,
          0.6995857954025269,
          0.7006045579910278,
          0.7015825510025024,
          0.6955447196960449,
          0.7066505551338196,
          0.7236992716789246,
          0.698397159576416,
          0.6884368658065796,
          0.7126522064208984,
          0.7065216898918152,
          0.7025529742240906,
          0.6784192323684692,
          0.7045695781707764,
          0.7187268733978271,
          0.6814192533493042,
          0.6904106140136719,
          0.7066457271575928,
          0.7015352249145508,
          0.7105015516281128,
          0.6925325393676758,
          0.6975609660148621,
          0.6884925365447998,
          0.6915009021759033,
          0.6985639333724976,
          0.6975020170211792,
          0.7035989165306091,
          0.7196657657623291,
          0.6975794434547424,
          0.7257785797119141,
          0.702569842338562,
          0.714652419090271,
          0.7085126042366028,
          0.7105793356895447,
          0.7116830348968506,
          0.7046007513999939,
          0.7177132368087769,
          0.718646764755249,
          0.702160656452179,
          0.6983652114868164,
          0.7095073461532593,
          0.7081708908081055,
          0.6842752695083618,
          0.7066385746002197,
          0.6834046840667725,
          0.6962760090827942,
          0.7135688066482544,
          0.71642005443573,
          0.6955039501190186,
          0.7143621444702148,
          0.7185415029525757,
          0.6914306879043579,
          0.7091051340103149,
          0.7173696756362915,
          0.7062876224517822,
          0.7171854972839355,
          0.7096478939056396,
          0.7060483694076538,
          0.6974576711654663,
          0.7203339338302612,
          0.7054427862167358,
          0.7360838651657104,
          0.719388484954834,
          0.7315754890441895,
          0.714652419090271,
          0.7317850589752197,
          0.6934474110603333,
          0.7003829479217529,
          0.7167312502861023,
          0.7096478939056396,
          0.7096586227416992,
          0.7183994054794312,
          0.7002731561660767,
          0.7133729457855225,
          0.7116748094558716,
          0.7055685520172119,
          0.7183146476745605,
          0.7004145979881287,
          0.7103649377822876,
          0.717282235622406,
          0.7026134133338928,
          0.6985001564025879,
          0.7255554795265198,
          0.712256908416748,
          0.7174479961395264,
          0.7124558687210083,
          0.7154297232627869,
          0.7131955623626709,
          0.7129805088043213,
          0.7210075855255127,
          0.716380774974823,
          0.7267520427703857,
          0.7204180955886841,
          0.7442295551300049,
          0.6974035501480103,
          0.7236779928207397,
          0.7254839539527893,
          0.7213653326034546,
          0.7222105860710144,
          0.7133321762084961,
          0.7205591201782227,
          0.7266120314598083,
          0.7235701084136963,
          0.7145376205444336,
          0.7297288179397583,
          0.7175478935241699,
          0.7176490426063538,
          0.7364449501037598,
          0.7312256097793579,
          0.7195653915405273,
          0.7244935035705566,
          0.7180620431900024,
          0.7193016409873962,
          0.719388484954834,
          0.7204568386077881,
          0.7336273789405823,
          0.7092046737670898,
          0.7313733100891113,
          0.7270726561546326,
          0.7323599457740784,
          0.7167105674743652,
          0.7367772459983826,
          0.7405664920806885,
          0.7264339327812195,
          0.7225451469421387,
          0.7222105860710144,
          0.733519971370697,
          0.7215830087661743,
          0.7277329564094543,
          0.7154991626739502,
          0.7404841184616089,
          0.6974576711654663,
          0.6960533857345581,
          0.7143621444702148,
          0.7143203020095825,
          0.7184751033782959,
          0.7215523719787598,
          0.7349811792373657,
          0.7324494123458862,
          0.7242803573608398,
          0.7313733100891113,
          0.7226060032844543,
          0.7284529209136963,
          0.7241252660751343,
          0.7251081466674805,
          0.7362516522407532,
          0.7180047035217285,
          0.7323120832443237,
          0.728271484375,
          0.7162491679191589,
          0.7272860407829285,
          0.7141295075416565,
          0.7465428113937378,
          0.7293535470962524,
          0.7230292558670044,
          0.7384645938873291,
          0.7504945993423462,
          0.7173271775245667,
          0.72944176197052,
          0.7174099683761597,
          0.7404397130012512,
          0.7527505159378052,
          0.7316410541534424,
          0.728111982345581,
          0.7292565107345581,
          0.7252659201622009,
          0.727129340171814,
          0.7405263781547546,
          0.7324907183647156,
          0.7187967896461487,
          0.7394523024559021,
          0.7283666729927063,
          0.7414267659187317,
          0.7375180125236511,
          0.7488884329795837,
          0.7413793802261353,
          0.7416319847106934,
          0.7263002395629883,
          0.7483267188072205,
          0.744523286819458,
          0.7146130800247192,
          0.7265808582305908,
          0.7363032102584839,
          0.730076253414154,
          0.7259095907211304,
          0.7597280740737915,
          0.7477532029151917,
          0.7322618961334229,
          0.7407058477401733,
          0.7447103261947632,
          0.7134114503860474,
          0.744385838508606,
          0.7392569184303284,
          0.7210075855255127,
          0.7274636626243591,
          0.746454119682312,
          0.7476874589920044,
          0.7403445243835449,
          0.7388982772827148,
          0.7286583185195923,
          0.72944176197052,
          0.7284529209136963,
          0.7386879920959473,
          0.7212257385253906,
          0.7394067049026489,
          0.733519971370697,
          0.733519971370697,
          0.7404397130012512,
          0.730016827583313,
          0.7272360920906067,
          0.7365286350250244,
          0.7344253659248352,
          0.7419678568840027,
          0.7375575304031372,
          0.7241792678833008,
          0.7352673411369324,
          0.7643967270851135,
          0.7362516522407532,
          0.719388484954834,
          0.73105788230896,
          0.7304718494415283,
          0.7315394878387451,
          0.7486422657966614,
          0.7566803693771362,
          0.7385070323944092,
          0.7442837953567505,
          0.7341201305389404,
          0.7376632690429688,
          0.744385838508606,
          0.7431906461715698,
          0.7458163499832153,
          0.7048341035842896,
          0.7487488985061646,
          0.7534058094024658,
          0.7100345492362976,
          0.7442295551300049,
          0.7538493871688843,
          0.7263002395629883,
          0.7452677488327026,
          0.7304301261901855,
          0.7623201012611389,
          0.7531291246414185,
          0.7254448533058167,
          0.7384645938873291,
          0.730076253414154,
          0.7442295551300049,
          0.7434918880462646,
          0.7396484613418579,
          0.7193016409873962,
          0.7516517639160156,
          0.7363526821136475,
          0.7476135492324829,
          0.7546991109848022,
          0.72944176197052,
          0.7479754686355591,
          0.7422076463699341,
          0.7475312948226929,
          0.7260905504226685,
          0.7432994842529297,
          0.749200701713562,
          0.7548966407775879,
          0.7393589019775391,
          0.7330201864242554,
          0.7363526821136475,
          0.7569222450256348,
          0.7395371794700623,
          0.728271484375,
          0.730016827583313,
          0.7296133637428284,
          0.7458847761154175,
          0.7439268827438354,
          0.7463052868843079,
          0.728111982345581,
          0.7448382377624512,
          0.7506977319717407,
          0.7426922917366028,
          0.74946129322052,
          0.7353179454803467,
          0.7441732883453369,
          0.7573454976081848,
          0.7563610076904297,
          0.7433998584747314,
          0.7483779191970825,
          0.7482179403305054,
          0.7475733757019043,
          0.74946129322052,
          0.7349811792373657,
          0.7493111491203308
         ]
        },
        {
         "name": "Validation F1 score",
         "type": "scatter",
         "y": [
          0.3272727429866791,
          0.3272727429866791,
          0.3272727429866791,
          0.3272727429866791,
          0.3272727429866791,
          0.3272727429866791,
          0.3272727429866791,
          0.3272727429866791,
          0.3272727429866791,
          0.3272727429866791,
          0.3272727429866791,
          0.3190184235572815,
          0.3190184235572815,
          0.31481480598449707,
          0.3286290168762207,
          0.3633306920528412,
          0.421710729598999,
          0.43665412068367004,
          0.4795439839363098,
          0.4998197555541992,
          0.4998197555541992,
          0.5381959676742554,
          0.5309858918190002,
          0.5237720012664795,
          0.5237720012664795,
          0.5237720012664795,
          0.5358884930610657,
          0.5212215185165405,
          0.5329325795173645,
          0.525478720664978,
          0.5369093418121338,
          0.5293419361114502,
          0.5405085682868958,
          0.5515151619911194,
          0.5437463521957397,
          0.5545150637626648,
          0.5466366410255432,
          0.5466366410255432,
          0.5466366410255432,
          0.5571808815002441,
          0.5571808815002441,
          0.567592203617096,
          0.567592203617096,
          0.5697319507598877,
          0.5798222422599792,
          0.5898004174232483,
          0.6094428300857544,
          0.6094428300857544,
          0.6094428300857544,
          0.6094428300857544,
          0.6094428300857544,
          0.6094428300857544,
          0.6094428300857544,
          0.6382007598876953,
          0.6382007598876953,
          0.6382007598876953,
          0.6382007598876953,
          0.6382007598876953,
          0.6476190686225891,
          0.6569615602493286,
          0.6476190686225891,
          0.6569615602493286,
          0.6665583848953247,
          0.6574074029922485,
          0.6665583848953247,
          0.6665583848953247,
          0.6756494045257568,
          0.6665583848953247,
          0.6756494045257568,
          0.675438642501831,
          0.675438642501831,
          0.675438642501831,
          0.675438642501831,
          0.6750162839889526,
          0.6842746734619141,
          0.6842746734619141,
          0.675438642501831,
          0.675438642501831,
          0.6756494045257568,
          0.6845822930335999,
          0.7027027010917664,
          0.684684693813324,
          0.6845822930335999,
          0.6845822930335999,
          0.6756493449211121,
          0.6756493449211121,
          0.6756493449211121,
          0.6756493449211121,
          0.6756493449211121,
          0.6756493449211121,
          0.6756493449211121,
          0.6576298475265503,
          0.6666666865348816,
          0.6576298475265503,
          0.6665583848953247,
          0.6665583848953247,
          0.6665583848953247,
          0.6665583848953247,
          0.6665583848953247,
          0.675438642501831,
          0.675438642501831,
          0.6665583848953247,
          0.6662331819534302,
          0.6662331819534302,
          0.6574074029922485,
          0.6662331819534302,
          0.6574074029922485,
          0.6574074029922485,
          0.6485345363616943,
          0.6485345363616943,
          0.6485345363616943,
          0.6485345363616943,
          0.6485345363616943,
          0.6574074029922485,
          0.6574074029922485,
          0.6574074029922485,
          0.6574074029922485,
          0.6574074029922485,
          0.6574074029922485,
          0.6574074029922485,
          0.6574074029922485,
          0.6574074029922485,
          0.6574074029922485,
          0.6574074029922485,
          0.6662331819534302,
          0.6662331819534302,
          0.6750162839889526,
          0.6750162839889526,
          0.6656898260116577,
          0.6656898260116577,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6656898260116577,
          0.6837607026100159,
          0.6750162839889526,
          0.6837607026100159,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6743806600570679,
          0.6649261713027954,
          0.6649261713027954,
          0.6649261713027954,
          0.6649261713027954,
          0.6649261713027954,
          0.6649261713027954,
          0.6649261713027954,
          0.6649261713027954,
          0.6649261713027954,
          0.6649261713027954,
          0.6649261713027954,
          0.6649261713027954,
          0.6649261713027954,
          0.6649261713027954,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6562907695770264,
          0.6656898260116577,
          0.6562907695770264,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6562907695770264,
          0.6562907695770264,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6562907695770264,
          0.6562907695770264,
          0.6468140482902527,
          0.6562907695770264,
          0.6656898260116577,
          0.6562907695770264,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6656898260116577,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6656898260116577,
          0.6656898260116577,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6656898260116577,
          0.6562907695770264,
          0.6569616198539734,
          0.6476190686225891,
          0.6476190686225891,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6476190686225891,
          0.6476190686225891,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6476190686225891,
          0.6569616198539734,
          0.6569616198539734,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6662331819534302,
          0.6662331819534302,
          0.6569616198539734,
          0.6662331819534302,
          0.6569616198539734,
          0.6662331819534302,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6476190686225891,
          0.6569616198539734,
          0.6662331819534302,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6569616198539734,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6662331819534302,
          0.6569616198539734,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6569616198539734,
          0.6569616198539734,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6569616198539734,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6662331819534302,
          0.6569616198539734,
          0.6476190686225891,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6382008194923401,
          0.6382008194923401,
          0.6382008194923401,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6662331819534302,
          0.6476190686225891,
          0.6662331819534302,
          0.6662331819534302,
          0.6569616198539734,
          0.6662331819534302,
          0.6569616198539734,
          0.6569616198539734,
          0.6569616198539734,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6569616198539734,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891,
          0.6476190686225891
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Accuracy EFFICIENTNET"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "F1 score"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'..\\\\..\\\\Outputs\\\\COVID\\\\LR\\\\NuovoTest_Not_Kfold\\\\GRAPHS\\\\training Efficientnet F1 score.html'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLOT HISTORY\n",
    "plt.clf()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['loss'],\n",
    "                    name='Train loss'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_loss'],\n",
    "                    name='Validation loss'))\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Loss EFFICIENTNET',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='Loss')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training Efficientnet loss.html\"))), auto_open=False)\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['binary_accuracy'],\n",
    "                    name='Train accuracy'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_binary_accuracy'],\n",
    "                    name='Validation accuracy'))\n",
    "\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Accuracy EFFICIENTNET',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='Accuracy')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training Efficientnet accuracy.html\"))), auto_open=False)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['f1_score'],\n",
    "                    name='Train F1 score'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_f1_score'],\n",
    "                    name='Validation F1 score'))\n",
    "\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Accuracy EFFICIENTNET',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='F1 score')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training Efficientnet F1 score.html\"))), auto_open=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth of the validation\n",
      "4/4 [==============================] - 1s 30ms/step - loss: 0.6513 - binary_accuracy: 0.7027 - f1_score: 0.7027\n",
      "4/4 [==============================] - 1s 27ms/step\n",
      "[0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1\n",
      " 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0]\n",
      "[0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1]\n",
      "accuracy:  0.7027027027027027\n",
      "log_loss:  0.649209439150385\n",
      "[[39 15]\n",
      " [18 39]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.68421   0.72222   0.70270        54\n",
      "           1    0.72222   0.68421   0.70270        57\n",
      "\n",
      "    accuracy                        0.70270       111\n",
      "   macro avg    0.70322   0.70322   0.70270       111\n",
      "weighted avg    0.70373   0.70270   0.70270       111\n",
      "\n",
      "{'loss': 0.6512680053710938, 'binary_accuracy': 0.7027027010917664, 'f1_score': 0.7027027010917664}\n",
      "Ground truth of the validation\n",
      "4/4 [==============================] - 1s 28ms/step - loss: 0.6513 - binary_accuracy: 0.7027 - f1_score: 0.7027\n",
      "4/4 [==============================] - 1s 27ms/step\n",
      "[0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1\n",
      " 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0]\n",
      "[0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1]\n",
      "accuracy:  0.7027027027027027\n",
      "log_loss:  0.649209439150385\n",
      "[[39 15]\n",
      " [18 39]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.68421   0.72222   0.70270        54\n",
      "           1    0.72222   0.68421   0.70270        57\n",
      "\n",
      "    accuracy                        0.70270       111\n",
      "   macro avg    0.70322   0.70322   0.70270       111\n",
      "weighted avg    0.70373   0.70270   0.70270       111\n",
      "\n",
      "{'loss': 0.6512680053710938, 'binary_accuracy': 0.7027027010917664, 'f1_score': 0.7027027010917664}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## EVALUATION\n",
    "gc.collect()\n",
    "\n",
    "bestPt = checkpointPath / Path('efficientnet_Classification_F1')\n",
    "model = tf.keras.models.load_model(bestPt)\n",
    "\n",
    "print(\"Ground truth of the validation\")\n",
    "\n",
    "evaluation = model.evaluate(x = X_val_IMGS, y = Y_val)\n",
    "prediction = model.predict(X_val_IMGS)\n",
    "\n",
    "ypreds = np.argmax(prediction, axis=1)\n",
    "yground = np.argmax(Y_val.to_numpy(), axis=1)\n",
    "# yground = np.argmax(targets, axis=1)\n",
    "\n",
    "print(ypreds)\n",
    "print(yground)\n",
    "\n",
    "\n",
    "\n",
    "score = accuracy_score(yground, ypreds)\n",
    "print('accuracy: ', score)\n",
    "\n",
    "loss = log_loss(yground, prediction)\n",
    "print('log_loss: ', loss)\n",
    "\n",
    "# Print the confusion matrix\n",
    "cmatrix = confusion_matrix(yground, ypreds)\n",
    "print(cmatrix)\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "creport = classification_report(yground, ypreds, digits=5)\n",
    "print(creport)\n",
    "\n",
    "\n",
    "evaluation = dict(zip(model.metrics_names,evaluation))\n",
    "\n",
    "# FF_f1_values.append(evaluation['f1_score'])\n",
    "\n",
    "\n",
    "\n",
    "print(evaluation)\n",
    "with open(str(evalspath) + 'EFFICIENTNET_F1_EVALUATION.txt', 'w') as f:\n",
    "    f.write('EFFICIENTNET FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    for k, v in evaluation.items():\n",
    "        f.write(str(k) + ' >>> '+ str(v) + '\\n\\n')\n",
    "    f.write('CNN FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    f.write('SKLEARN METRICS:')\n",
    "    f.write(str('Loss') + ' >>> '+ str(loss) + '\\n\\n')\n",
    "    f.write(str('Accuracy') + ' >>> '+ str(score) + '\\n\\n')\n",
    "    f.write(str('Confusion Matrix') + ' >>> \\n'+ str(cmatrix) + '\\n\\n')\n",
    "    f.write(str('CLASSIFICATION REPORT WITH CLASSES') + ' >>> \\n'+ str(creport) + '\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bestPt = checkpointPath / Path('efficientnet_Classification_acc')\n",
    "model = tf.keras.models.load_model(bestPt)\n",
    "\n",
    "print(\"Ground truth of the validation\")\n",
    "\n",
    "evaluation = model.evaluate(x = X_val_IMGS, y = Y_val)\n",
    "prediction = model.predict(X_val_IMGS)\n",
    "\n",
    "ypreds = np.argmax(prediction, axis=1)\n",
    "yground = np.argmax(Y_val.to_numpy(), axis=1)\n",
    "# yground = np.argmax(targets, axis=1)\n",
    "\n",
    "print(ypreds)\n",
    "print(yground)\n",
    "\n",
    "\n",
    "\n",
    "score = accuracy_score(yground, ypreds)\n",
    "print('accuracy: ', score)\n",
    "\n",
    "loss = log_loss(yground, prediction)\n",
    "print('log_loss: ', loss)\n",
    "\n",
    "\n",
    "# Print the confusion matrix\n",
    "cmatrix = confusion_matrix(yground, ypreds)\n",
    "print(cmatrix)\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "creport = classification_report(yground, ypreds, digits=5)\n",
    "print(creport)\n",
    "\n",
    "\n",
    "# FF_accuracy_values.append(score)\n",
    "\n",
    "evaluation = dict(zip(model.metrics_names,evaluation))\n",
    "print(evaluation)\n",
    "with open(str(evalspath) + 'EFFICIENTNET_acc_EVALUATION.txt', 'w') as f:\n",
    "    f.write('EFFICIENTNET FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    for k, v in evaluation.items():\n",
    "        f.write(str(k) + ' >>> '+ str(v) + '\\n\\n')\n",
    "    f.write('CNN FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    f.write('SKLEARN METRICS:')\n",
    "    f.write(str('Loss') + ' >>> '+ str(loss) + '\\n\\n')\n",
    "    f.write(str('Accuracy') + ' >>> '+ str(score) + '\\n\\n')\n",
    "    f.write(str('Confusion Matrix') + ' >>> \\n'+ str(cmatrix) + '\\n\\n')\n",
    "    f.write(str('CLASSIFICATION REPORT WITH CLASSES') + ' >>> \\n'+ str(creport) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### addestramento MLP (Dati Clinici)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim, regression=False):\n",
    "    #model definition\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16,input_dim=dim,activation='relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(8,activation='relu'))\n",
    "    model.add(Dense(8,activation='relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(4,activation='relu'))\n",
    "    #can continue the model\n",
    "\n",
    "    if regression:\n",
    "        model.add(Dense(2,activation='sigmoid'))\n",
    "    # else:\n",
    "        # model.add(Flatten())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP\n",
      "Epoch 1/1000\n",
      "19/31 [=================>............] - ETA: 0s - loss: 0.7021 - binary_accuracy: 0.5123 - f1_score: 0.5226 INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_F1\\assets\n",
      "31/31 [==============================] - 2s 36ms/step - loss: 0.6995 - binary_accuracy: 0.5126 - f1_score: 0.5110 - val_loss: 0.6925 - val_binary_accuracy: 0.5000 - val_f1_score: 0.4972\n",
      "Epoch 2/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.6948 - binary_accuracy: 0.4894 - f1_score: 0.4504 - val_loss: 0.6919 - val_binary_accuracy: 0.4640 - val_f1_score: 0.3393\n",
      "Epoch 3/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.6938 - binary_accuracy: 0.4995 - f1_score: 0.3934 - val_loss: 0.6906 - val_binary_accuracy: 0.4775 - val_f1_score: 0.3393\n",
      "Epoch 4/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6910 - binary_accuracy: 0.5010 - f1_score: 0.3899 - val_loss: 0.6864 - val_binary_accuracy: 0.4595 - val_f1_score: 0.3393\n",
      "Epoch 5/1000\n",
      "20/31 [==================>...........] - ETA: 0s - loss: 0.6944 - binary_accuracy: 0.5000 - f1_score: 0.3794INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.6918 - binary_accuracy: 0.4960 - f1_score: 0.3609 - val_loss: 0.6859 - val_binary_accuracy: 0.5135 - val_f1_score: 0.3393\n",
      "Epoch 6/1000\n",
      "19/31 [=================>............] - ETA: 0s - loss: 0.6888 - binary_accuracy: 0.4926 - f1_score: 0.3868INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.6930 - binary_accuracy: 0.4934 - f1_score: 0.3755 - val_loss: 0.6857 - val_binary_accuracy: 0.5856 - val_f1_score: 0.3393\n",
      "Epoch 7/1000\n",
      "19/31 [=================>............] - ETA: 0s - loss: 0.6897 - binary_accuracy: 0.5008 - f1_score: 0.3481INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.6887 - binary_accuracy: 0.5010 - f1_score: 0.3496 - val_loss: 0.6837 - val_binary_accuracy: 0.6261 - val_f1_score: 0.3393\n",
      "Epoch 8/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6837 - binary_accuracy: 0.5257 - f1_score: 0.3781 - val_loss: 0.6798 - val_binary_accuracy: 0.5811 - val_f1_score: 0.3393\n",
      "Epoch 9/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6839 - binary_accuracy: 0.5292 - f1_score: 0.4718 - val_loss: 0.6775 - val_binary_accuracy: 0.5901 - val_f1_score: 0.3393\n",
      "Epoch 10/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6847 - binary_accuracy: 0.5383 - f1_score: 0.5042 - val_loss: 0.6771 - val_binary_accuracy: 0.6216 - val_f1_score: 0.3393\n",
      "Epoch 11/1000\n",
      "24/31 [======================>.......] - ETA: 0s - loss: 0.6831 - binary_accuracy: 0.5371 - f1_score: 0.5656INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_F1\\assets\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 0.6816 - binary_accuracy: 0.5368 - f1_score: 0.5598 - val_loss: 0.6735 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7197\n",
      "Epoch 12/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6661 - binary_accuracy: 0.5862 - f1_score: 0.5987 - val_loss: 0.6637 - val_binary_accuracy: 0.5586 - val_f1_score: 0.4951\n",
      "Epoch 13/1000\n",
      "25/31 [=======================>......] - ETA: 0s - loss: 0.6746 - binary_accuracy: 0.5719 - f1_score: 0.5900INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 0.6747 - binary_accuracy: 0.5736 - f1_score: 0.5891 - val_loss: 0.6628 - val_binary_accuracy: 0.6486 - val_f1_score: 0.6718\n",
      "Epoch 14/1000\n",
      "21/31 [===================>..........] - ETA: 0s - loss: 0.6802 - binary_accuracy: 0.5781 - f1_score: 0.5703INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_F1\\assets\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.6743 - binary_accuracy: 0.5907 - f1_score: 0.5863 - val_loss: 0.6664 - val_binary_accuracy: 0.6757 - val_f1_score: 0.7206\n",
      "Epoch 15/1000\n",
      "17/31 [===============>..............] - ETA: 0s - loss: 0.6695 - binary_accuracy: 0.6020 - f1_score: 0.6114INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.6651 - binary_accuracy: 0.6190 - f1_score: 0.6315 - val_loss: 0.6542 - val_binary_accuracy: 0.6802 - val_f1_score: 0.6503\n",
      "Epoch 16/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.6620 - binary_accuracy: 0.6184 - f1_score: 0.6305 - val_loss: 0.6507 - val_binary_accuracy: 0.6622 - val_f1_score: 0.6314\n",
      "Epoch 17/1000\n",
      "18/31 [================>.............] - ETA: 0s - loss: 0.6652 - binary_accuracy: 0.6207 - f1_score: 0.6316INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.6690 - binary_accuracy: 0.6154 - f1_score: 0.6208 - val_loss: 0.6520 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7106\n",
      "Epoch 18/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.6535 - binary_accuracy: 0.6210 - f1_score: 0.6272 - val_loss: 0.6500 - val_binary_accuracy: 0.6847 - val_f1_score: 0.6754\n",
      "Epoch 19/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6627 - binary_accuracy: 0.6300 - f1_score: 0.6341 - val_loss: 0.6491 - val_binary_accuracy: 0.6937 - val_f1_score: 0.7115\n",
      "Epoch 20/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6507 - binary_accuracy: 0.6235 - f1_score: 0.6255 - val_loss: 0.6419 - val_binary_accuracy: 0.6802 - val_f1_score: 0.6935\n",
      "Epoch 21/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6477 - binary_accuracy: 0.6623 - f1_score: 0.6677 - val_loss: 0.6409 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7023\n",
      "Epoch 22/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6556 - binary_accuracy: 0.6310 - f1_score: 0.6245 - val_loss: 0.6418 - val_binary_accuracy: 0.6892 - val_f1_score: 0.6843\n",
      "Epoch 23/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6343 - binary_accuracy: 0.6653 - f1_score: 0.6714 - val_loss: 0.6237 - val_binary_accuracy: 0.6937 - val_f1_score: 0.6935\n",
      "Epoch 24/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6401 - binary_accuracy: 0.6436 - f1_score: 0.6413 - val_loss: 0.6300 - val_binary_accuracy: 0.6982 - val_f1_score: 0.7012\n",
      "Epoch 25/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6432 - binary_accuracy: 0.6487 - f1_score: 0.6477 - val_loss: 0.6295 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7098\n",
      "Epoch 26/1000\n",
      "23/31 [=====================>........] - ETA: 0s - loss: 0.6231 - binary_accuracy: 0.6726 - f1_score: 0.6717INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_F1\\assets\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.6306 - binary_accuracy: 0.6694 - f1_score: 0.6658 - val_loss: 0.6194 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7287\n",
      "Epoch 27/1000\n",
      "20/31 [==================>...........] - ETA: 0s - loss: 0.6172 - binary_accuracy: 0.6766 - f1_score: 0.6728INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_F1\\assets\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.6194 - binary_accuracy: 0.6759 - f1_score: 0.6699 - val_loss: 0.6127 - val_binary_accuracy: 0.7387 - val_f1_score: 0.7467\n",
      "Epoch 28/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.5979 - binary_accuracy: 0.6935 - f1_score: 0.6895 - val_loss: 0.6101 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7193\n",
      "Epoch 29/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.6274 - binary_accuracy: 0.6694 - f1_score: 0.6646 - val_loss: 0.6082 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7279\n",
      "Epoch 30/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6273 - binary_accuracy: 0.6638 - f1_score: 0.6526 - val_loss: 0.6072 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7106\n",
      "Epoch 31/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6376 - binary_accuracy: 0.6562 - f1_score: 0.6567 - val_loss: 0.6101 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7366\n",
      "Epoch 32/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6190 - binary_accuracy: 0.6724 - f1_score: 0.6662 - val_loss: 0.6148 - val_binary_accuracy: 0.7387 - val_f1_score: 0.7442\n",
      "Epoch 33/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6092 - binary_accuracy: 0.6794 - f1_score: 0.6829 - val_loss: 0.6012 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7199\n",
      "Epoch 34/1000\n",
      "25/31 [=======================>......] - ETA: 0s - loss: 0.6375 - binary_accuracy: 0.6719 - f1_score: 0.6660INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 0.6298 - binary_accuracy: 0.6694 - f1_score: 0.6628 - val_loss: 0.6064 - val_binary_accuracy: 0.7523 - val_f1_score: 0.7461\n",
      "Epoch 35/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5938 - binary_accuracy: 0.6915 - f1_score: 0.6947 - val_loss: 0.6018 - val_binary_accuracy: 0.7027 - val_f1_score: 0.6931\n",
      "Epoch 36/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5977 - binary_accuracy: 0.6971 - f1_score: 0.6983 - val_loss: 0.5942 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7374\n",
      "Epoch 37/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6106 - binary_accuracy: 0.6860 - f1_score: 0.6828 - val_loss: 0.6045 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7366\n",
      "Epoch 38/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5883 - binary_accuracy: 0.7041 - f1_score: 0.7036 - val_loss: 0.5994 - val_binary_accuracy: 0.6982 - val_f1_score: 0.7026\n",
      "Epoch 39/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6109 - binary_accuracy: 0.6835 - f1_score: 0.6824 - val_loss: 0.6031 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7184\n",
      "Epoch 40/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6094 - binary_accuracy: 0.6946 - f1_score: 0.6874 - val_loss: 0.6050 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7270\n",
      "Epoch 41/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6113 - binary_accuracy: 0.6830 - f1_score: 0.6811 - val_loss: 0.6014 - val_binary_accuracy: 0.7523 - val_f1_score: 0.7452\n",
      "Epoch 42/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5850 - binary_accuracy: 0.6986 - f1_score: 0.6952 - val_loss: 0.5990 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7106\n",
      "Epoch 43/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5848 - binary_accuracy: 0.7097 - f1_score: 0.7109 - val_loss: 0.5935 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7018\n",
      "Epoch 44/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5951 - binary_accuracy: 0.6976 - f1_score: 0.6984 - val_loss: 0.5995 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7270\n",
      "Epoch 45/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6104 - binary_accuracy: 0.6870 - f1_score: 0.6795 - val_loss: 0.6015 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7260\n",
      "Epoch 46/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5849 - binary_accuracy: 0.6925 - f1_score: 0.6923 - val_loss: 0.5885 - val_binary_accuracy: 0.7387 - val_f1_score: 0.7287\n",
      "Epoch 47/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6091 - binary_accuracy: 0.6946 - f1_score: 0.6973 - val_loss: 0.5960 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7098\n",
      "Epoch 48/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5914 - binary_accuracy: 0.6865 - f1_score: 0.6803 - val_loss: 0.5940 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7193\n",
      "Epoch 49/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5978 - binary_accuracy: 0.6991 - f1_score: 0.7007 - val_loss: 0.5935 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7106\n",
      "Epoch 50/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5759 - binary_accuracy: 0.7233 - f1_score: 0.7210 - val_loss: 0.5891 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7023\n",
      "Epoch 51/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5886 - binary_accuracy: 0.6991 - f1_score: 0.6972 - val_loss: 0.5935 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7270\n",
      "Epoch 52/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5904 - binary_accuracy: 0.7061 - f1_score: 0.7056 - val_loss: 0.5890 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7204\n",
      "Epoch 53/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5871 - binary_accuracy: 0.7051 - f1_score: 0.6994 - val_loss: 0.5913 - val_binary_accuracy: 0.7432 - val_f1_score: 0.7442\n",
      "Epoch 54/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5920 - binary_accuracy: 0.7011 - f1_score: 0.6976 - val_loss: 0.5802 - val_binary_accuracy: 0.7432 - val_f1_score: 0.7467\n",
      "Epoch 55/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5730 - binary_accuracy: 0.7046 - f1_score: 0.7035 - val_loss: 0.5808 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7279\n",
      "Epoch 56/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5963 - binary_accuracy: 0.6900 - f1_score: 0.6908 - val_loss: 0.5954 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7174\n",
      "Epoch 57/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5702 - binary_accuracy: 0.7137 - f1_score: 0.7080 - val_loss: 0.5930 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7279\n",
      "Epoch 58/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5829 - binary_accuracy: 0.7182 - f1_score: 0.7168 - val_loss: 0.5890 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7287\n",
      "Epoch 59/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5828 - binary_accuracy: 0.7092 - f1_score: 0.7074 - val_loss: 0.5874 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7199\n",
      "Epoch 60/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5758 - binary_accuracy: 0.7102 - f1_score: 0.7087 - val_loss: 0.5831 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7193\n",
      "Epoch 61/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5901 - binary_accuracy: 0.6880 - f1_score: 0.6886 - val_loss: 0.5865 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7279\n",
      "Epoch 62/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5708 - binary_accuracy: 0.7127 - f1_score: 0.7128 - val_loss: 0.5899 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 63/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5556 - binary_accuracy: 0.7263 - f1_score: 0.7238 - val_loss: 0.5904 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7279\n",
      "Epoch 64/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5623 - binary_accuracy: 0.7268 - f1_score: 0.7253 - val_loss: 0.5840 - val_binary_accuracy: 0.7387 - val_f1_score: 0.7387\n",
      "Epoch 65/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5638 - binary_accuracy: 0.7283 - f1_score: 0.7230 - val_loss: 0.5745 - val_binary_accuracy: 0.7387 - val_f1_score: 0.7380\n",
      "Epoch 66/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5803 - binary_accuracy: 0.7112 - f1_score: 0.7127 - val_loss: 0.5837 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7193\n",
      "Epoch 67/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5721 - binary_accuracy: 0.7303 - f1_score: 0.7263 - val_loss: 0.5859 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7003\n",
      "Epoch 68/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5707 - binary_accuracy: 0.7288 - f1_score: 0.7277 - val_loss: 0.5856 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7106\n",
      "Epoch 69/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5642 - binary_accuracy: 0.7349 - f1_score: 0.7345 - val_loss: 0.5815 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7193\n",
      "Epoch 70/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5559 - binary_accuracy: 0.7354 - f1_score: 0.7330 - val_loss: 0.5702 - val_binary_accuracy: 0.7387 - val_f1_score: 0.7387\n",
      "Epoch 71/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5778 - binary_accuracy: 0.7127 - f1_score: 0.7146 - val_loss: 0.5701 - val_binary_accuracy: 0.7432 - val_f1_score: 0.7387\n",
      "Epoch 72/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5842 - binary_accuracy: 0.7006 - f1_score: 0.7010 - val_loss: 0.5793 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7279\n",
      "Epoch 73/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5601 - binary_accuracy: 0.7223 - f1_score: 0.7230 - val_loss: 0.5785 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7207\n",
      "Epoch 74/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5517 - binary_accuracy: 0.7324 - f1_score: 0.7329 - val_loss: 0.5851 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7098\n",
      "Epoch 75/1000\n",
      "25/31 [=======================>......] - ETA: 0s - loss: 0.5365 - binary_accuracy: 0.7275 - f1_score: 0.7279INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_F1\\assets\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 0.5404 - binary_accuracy: 0.7263 - f1_score: 0.7271 - val_loss: 0.5733 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7472\n",
      "Epoch 76/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5475 - binary_accuracy: 0.7293 - f1_score: 0.7271 - val_loss: 0.5807 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7162\n",
      "Epoch 77/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5462 - binary_accuracy: 0.7364 - f1_score: 0.7334 - val_loss: 0.5667 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7206\n",
      "Epoch 78/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5459 - binary_accuracy: 0.7409 - f1_score: 0.7403 - val_loss: 0.5762 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7115\n",
      "Epoch 79/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5450 - binary_accuracy: 0.7445 - f1_score: 0.7436 - val_loss: 0.5664 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7295\n",
      "Epoch 80/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5441 - binary_accuracy: 0.7470 - f1_score: 0.7462 - val_loss: 0.5692 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7295\n",
      "Epoch 81/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5565 - binary_accuracy: 0.7303 - f1_score: 0.7316 - val_loss: 0.5727 - val_binary_accuracy: 0.7072 - val_f1_score: 0.6937\n",
      "Epoch 82/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5647 - binary_accuracy: 0.7228 - f1_score: 0.7205 - val_loss: 0.5729 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7206\n",
      "Epoch 83/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5616 - binary_accuracy: 0.7233 - f1_score: 0.7204 - val_loss: 0.5677 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7115\n",
      "Epoch 84/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5417 - binary_accuracy: 0.7364 - f1_score: 0.7317 - val_loss: 0.5707 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7295\n",
      "Epoch 85/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5532 - binary_accuracy: 0.7293 - f1_score: 0.7263 - val_loss: 0.5682 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7295\n",
      "Epoch 86/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5548 - binary_accuracy: 0.7152 - f1_score: 0.7150 - val_loss: 0.5722 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7297\n",
      "Epoch 87/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5401 - binary_accuracy: 0.7460 - f1_score: 0.7434 - val_loss: 0.5708 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7111\n",
      "Epoch 88/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5352 - binary_accuracy: 0.7450 - f1_score: 0.7435 - val_loss: 0.5676 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7106\n",
      "Epoch 89/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5408 - binary_accuracy: 0.7404 - f1_score: 0.7396 - val_loss: 0.5672 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7297\n",
      "Epoch 90/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5529 - binary_accuracy: 0.7369 - f1_score: 0.7354 - val_loss: 0.5703 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7111\n",
      "Epoch 91/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5747 - binary_accuracy: 0.7152 - f1_score: 0.7118 - val_loss: 0.5734 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7204\n",
      "Epoch 92/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5639 - binary_accuracy: 0.7238 - f1_score: 0.7253 - val_loss: 0.5736 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7193\n",
      "Epoch 93/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5614 - binary_accuracy: 0.7182 - f1_score: 0.7162 - val_loss: 0.5663 - val_binary_accuracy: 0.7432 - val_f1_score: 0.7384\n",
      "Epoch 94/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5522 - binary_accuracy: 0.7324 - f1_score: 0.7277 - val_loss: 0.5754 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7199\n",
      "Epoch 95/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5462 - binary_accuracy: 0.7369 - f1_score: 0.7353 - val_loss: 0.5754 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7117\n",
      "Epoch 96/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5439 - binary_accuracy: 0.7455 - f1_score: 0.7478 - val_loss: 0.5689 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7384\n",
      "Epoch 97/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5447 - binary_accuracy: 0.7550 - f1_score: 0.7537 - val_loss: 0.5729 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7027\n",
      "Epoch 98/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5496 - binary_accuracy: 0.7314 - f1_score: 0.7290 - val_loss: 0.5680 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7206\n",
      "Epoch 99/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5587 - binary_accuracy: 0.7288 - f1_score: 0.7293 - val_loss: 0.5705 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7204\n",
      "Epoch 100/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5434 - binary_accuracy: 0.7445 - f1_score: 0.7439 - val_loss: 0.5679 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7366\n",
      "Epoch 101/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5344 - binary_accuracy: 0.7455 - f1_score: 0.7443 - val_loss: 0.5739 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7027\n",
      "Epoch 102/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5183 - binary_accuracy: 0.7545 - f1_score: 0.7548 - val_loss: 0.5660 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7027\n",
      "Epoch 103/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5242 - binary_accuracy: 0.7445 - f1_score: 0.7433 - val_loss: 0.5669 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7292\n",
      "Epoch 104/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5192 - binary_accuracy: 0.7550 - f1_score: 0.7547 - val_loss: 0.5670 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7117\n",
      "Epoch 105/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5562 - binary_accuracy: 0.7308 - f1_score: 0.7314 - val_loss: 0.5649 - val_binary_accuracy: 0.6982 - val_f1_score: 0.7117\n",
      "Epoch 106/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5074 - binary_accuracy: 0.7692 - f1_score: 0.7695 - val_loss: 0.5662 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7026\n",
      "Epoch 107/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5405 - binary_accuracy: 0.7339 - f1_score: 0.7324 - val_loss: 0.5686 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7374\n",
      "Epoch 108/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5132 - binary_accuracy: 0.7787 - f1_score: 0.7796 - val_loss: 0.5542 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7472\n",
      "Epoch 109/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5413 - binary_accuracy: 0.7545 - f1_score: 0.7535 - val_loss: 0.5740 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7374\n",
      "Epoch 110/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5285 - binary_accuracy: 0.7586 - f1_score: 0.7566 - val_loss: 0.5726 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7279\n",
      "Epoch 111/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5240 - binary_accuracy: 0.7631 - f1_score: 0.7609 - val_loss: 0.5653 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7115\n",
      "Epoch 112/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5318 - binary_accuracy: 0.7450 - f1_score: 0.7415 - val_loss: 0.5604 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7384\n",
      "Epoch 113/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5455 - binary_accuracy: 0.7419 - f1_score: 0.7408 - val_loss: 0.5611 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7026\n",
      "Epoch 114/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5469 - binary_accuracy: 0.7515 - f1_score: 0.7494 - val_loss: 0.5610 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7452\n",
      "Epoch 115/1000\n",
      "26/31 [========================>.....] - ETA: 0s - loss: 0.5480 - binary_accuracy: 0.7506 - f1_score: 0.7480INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_F1\\assets\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 0.5515 - binary_accuracy: 0.7445 - f1_score: 0.7410 - val_loss: 0.5557 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7476\n",
      "Epoch 116/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5491 - binary_accuracy: 0.7445 - f1_score: 0.7411 - val_loss: 0.5664 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7018\n",
      "Epoch 117/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5269 - binary_accuracy: 0.7490 - f1_score: 0.7471 - val_loss: 0.5613 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7206\n",
      "Epoch 118/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5017 - binary_accuracy: 0.7792 - f1_score: 0.7782 - val_loss: 0.5579 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7199\n",
      "Epoch 119/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5289 - binary_accuracy: 0.7505 - f1_score: 0.7489 - val_loss: 0.5570 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7206\n",
      "Epoch 120/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5316 - binary_accuracy: 0.7475 - f1_score: 0.7473 - val_loss: 0.5592 - val_binary_accuracy: 0.6937 - val_f1_score: 0.6935\n",
      "Epoch 121/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5150 - binary_accuracy: 0.7697 - f1_score: 0.7681 - val_loss: 0.5617 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7115\n",
      "Epoch 122/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5348 - binary_accuracy: 0.7636 - f1_score: 0.7634 - val_loss: 0.5535 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7270\n",
      "Epoch 123/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5320 - binary_accuracy: 0.7626 - f1_score: 0.7612 - val_loss: 0.5616 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7374\n",
      "Epoch 124/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5327 - binary_accuracy: 0.7424 - f1_score: 0.7416 - val_loss: 0.5552 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7117\n",
      "Epoch 125/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5015 - binary_accuracy: 0.7661 - f1_score: 0.7676 - val_loss: 0.5636 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7026\n",
      "Epoch 126/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5271 - binary_accuracy: 0.7656 - f1_score: 0.7649 - val_loss: 0.5524 - val_binary_accuracy: 0.7432 - val_f1_score: 0.7380\n",
      "Epoch 127/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5156 - binary_accuracy: 0.7545 - f1_score: 0.7550 - val_loss: 0.5570 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7027\n",
      "Epoch 128/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5268 - binary_accuracy: 0.7586 - f1_score: 0.7569 - val_loss: 0.5580 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7204\n",
      "Epoch 129/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5128 - binary_accuracy: 0.7737 - f1_score: 0.7730 - val_loss: 0.5503 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7292\n",
      "Epoch 130/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5494 - binary_accuracy: 0.7308 - f1_score: 0.7312 - val_loss: 0.5569 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7023\n",
      "Epoch 131/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5245 - binary_accuracy: 0.7530 - f1_score: 0.7526 - val_loss: 0.5547 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7027\n",
      "Epoch 132/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4837 - binary_accuracy: 0.7807 - f1_score: 0.7800 - val_loss: 0.5521 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7115\n",
      "Epoch 133/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5038 - binary_accuracy: 0.7807 - f1_score: 0.7820 - val_loss: 0.5466 - val_binary_accuracy: 0.7387 - val_f1_score: 0.7476\n",
      "Epoch 134/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5207 - binary_accuracy: 0.7717 - f1_score: 0.7697 - val_loss: 0.5505 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7380\n",
      "Epoch 135/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4929 - binary_accuracy: 0.7823 - f1_score: 0.7811 - val_loss: 0.5476 - val_binary_accuracy: 0.7387 - val_f1_score: 0.7467\n",
      "Epoch 136/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5323 - binary_accuracy: 0.7545 - f1_score: 0.7523 - val_loss: 0.5538 - val_binary_accuracy: 0.7387 - val_f1_score: 0.7374\n",
      "Epoch 137/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5071 - binary_accuracy: 0.7686 - f1_score: 0.7688 - val_loss: 0.5449 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7295\n",
      "Epoch 138/1000\n",
      "24/31 [======================>.......] - ETA: 0s - loss: 0.4983 - binary_accuracy: 0.7643 - f1_score: 0.7643INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_F1\\assets\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.5093 - binary_accuracy: 0.7591 - f1_score: 0.7589 - val_loss: 0.5408 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7653\n",
      "Epoch 139/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5187 - binary_accuracy: 0.7651 - f1_score: 0.7653 - val_loss: 0.5507 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7366\n",
      "Epoch 140/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5119 - binary_accuracy: 0.7636 - f1_score: 0.7648 - val_loss: 0.5455 - val_binary_accuracy: 0.7613 - val_f1_score: 0.7564\n",
      "Epoch 141/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5033 - binary_accuracy: 0.7747 - f1_score: 0.7739 - val_loss: 0.5386 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7374\n",
      "Epoch 142/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5208 - binary_accuracy: 0.7621 - f1_score: 0.7605 - val_loss: 0.5327 - val_binary_accuracy: 0.7523 - val_f1_score: 0.7555\n",
      "Epoch 143/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4883 - binary_accuracy: 0.7923 - f1_score: 0.7912 - val_loss: 0.5430 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7206\n",
      "Epoch 144/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5297 - binary_accuracy: 0.7661 - f1_score: 0.7656 - val_loss: 0.5437 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7206\n",
      "Epoch 145/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5153 - binary_accuracy: 0.7676 - f1_score: 0.7675 - val_loss: 0.5456 - val_binary_accuracy: 0.7432 - val_f1_score: 0.7467\n",
      "Epoch 146/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5029 - binary_accuracy: 0.7762 - f1_score: 0.7761 - val_loss: 0.5431 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7461\n",
      "Epoch 147/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5127 - binary_accuracy: 0.7727 - f1_score: 0.7729 - val_loss: 0.5439 - val_binary_accuracy: 0.6982 - val_f1_score: 0.6937\n",
      "Epoch 148/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5047 - binary_accuracy: 0.7722 - f1_score: 0.7716 - val_loss: 0.5393 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7467\n",
      "Epoch 149/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5021 - binary_accuracy: 0.7833 - f1_score: 0.7820 - val_loss: 0.5405 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7295\n",
      "Epoch 150/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5114 - binary_accuracy: 0.7802 - f1_score: 0.7809 - val_loss: 0.5421 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7295\n",
      "Epoch 151/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4975 - binary_accuracy: 0.7807 - f1_score: 0.7810 - val_loss: 0.5348 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7472\n",
      "Epoch 152/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4756 - binary_accuracy: 0.7949 - f1_score: 0.7963 - val_loss: 0.5375 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7295\n",
      "Epoch 153/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5296 - binary_accuracy: 0.7571 - f1_score: 0.7566 - val_loss: 0.5326 - val_binary_accuracy: 0.7432 - val_f1_score: 0.7380\n",
      "Epoch 154/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4936 - binary_accuracy: 0.7838 - f1_score: 0.7830 - val_loss: 0.5458 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7117\n",
      "Epoch 155/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5172 - binary_accuracy: 0.7550 - f1_score: 0.7538 - val_loss: 0.5462 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7206\n",
      "Epoch 156/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4843 - binary_accuracy: 0.7969 - f1_score: 0.7972 - val_loss: 0.5407 - val_binary_accuracy: 0.7523 - val_f1_score: 0.7564\n",
      "Epoch 157/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4972 - binary_accuracy: 0.7757 - f1_score: 0.7749 - val_loss: 0.5395 - val_binary_accuracy: 0.7387 - val_f1_score: 0.7380\n",
      "Epoch 158/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5087 - binary_accuracy: 0.7712 - f1_score: 0.7708 - val_loss: 0.5376 - val_binary_accuracy: 0.7523 - val_f1_score: 0.7472\n",
      "Epoch 159/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5001 - binary_accuracy: 0.7752 - f1_score: 0.7737 - val_loss: 0.5367 - val_binary_accuracy: 0.7387 - val_f1_score: 0.7384\n",
      "Epoch 160/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4832 - binary_accuracy: 0.7888 - f1_score: 0.7880 - val_loss: 0.5338 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7295\n",
      "Epoch 161/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5051 - binary_accuracy: 0.7676 - f1_score: 0.7658 - val_loss: 0.5349 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7477\n",
      "Epoch 162/1000\n",
      "25/31 [=======================>......] - ETA: 0s - loss: 0.5028 - binary_accuracy: 0.7713 - f1_score: 0.7702INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_F1\\assets\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 0.5038 - binary_accuracy: 0.7732 - f1_score: 0.7726 - val_loss: 0.5271 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7657\n",
      "Epoch 163/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4925 - binary_accuracy: 0.7858 - f1_score: 0.7829 - val_loss: 0.5381 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7297\n",
      "Epoch 164/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4799 - binary_accuracy: 0.7767 - f1_score: 0.7760 - val_loss: 0.5360 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7555\n",
      "Epoch 165/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4918 - binary_accuracy: 0.7933 - f1_score: 0.7920 - val_loss: 0.5319 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7564\n",
      "Epoch 166/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5230 - binary_accuracy: 0.7560 - f1_score: 0.7553 - val_loss: 0.5389 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7279\n",
      "Epoch 167/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4786 - binary_accuracy: 0.7999 - f1_score: 0.8001 - val_loss: 0.5348 - val_binary_accuracy: 0.7613 - val_f1_score: 0.7555\n",
      "Epoch 168/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4787 - binary_accuracy: 0.7878 - f1_score: 0.7880 - val_loss: 0.5227 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7560\n",
      "Epoch 169/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5072 - binary_accuracy: 0.7762 - f1_score: 0.7761 - val_loss: 0.5251 - val_binary_accuracy: 0.7523 - val_f1_score: 0.7461\n",
      "Epoch 170/1000\n",
      "23/31 [=====================>........] - ETA: 0s - loss: 0.4666 - binary_accuracy: 0.8037 - f1_score: 0.8038INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_F1\\assets\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 0.4891 - binary_accuracy: 0.7858 - f1_score: 0.7850 - val_loss: 0.5232 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7736\n",
      "Epoch 171/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4725 - binary_accuracy: 0.7903 - f1_score: 0.7893 - val_loss: 0.5312 - val_binary_accuracy: 0.7523 - val_f1_score: 0.7539\n",
      "Epoch 172/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4772 - binary_accuracy: 0.7893 - f1_score: 0.7889 - val_loss: 0.5308 - val_binary_accuracy: 0.7432 - val_f1_score: 0.7476\n",
      "Epoch 173/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4917 - binary_accuracy: 0.7807 - f1_score: 0.7812 - val_loss: 0.5303 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7467\n",
      "Epoch 174/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4950 - binary_accuracy: 0.7818 - f1_score: 0.7809 - val_loss: 0.5267 - val_binary_accuracy: 0.7523 - val_f1_score: 0.7467\n",
      "Epoch 175/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4906 - binary_accuracy: 0.7848 - f1_score: 0.7858 - val_loss: 0.5188 - val_binary_accuracy: 0.7523 - val_f1_score: 0.7467\n",
      "Epoch 176/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4795 - binary_accuracy: 0.7918 - f1_score: 0.7903 - val_loss: 0.5259 - val_binary_accuracy: 0.7432 - val_f1_score: 0.7387\n",
      "Epoch 177/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4689 - binary_accuracy: 0.8039 - f1_score: 0.8033 - val_loss: 0.5315 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7564\n",
      "Epoch 178/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4830 - binary_accuracy: 0.7853 - f1_score: 0.7862 - val_loss: 0.5168 - val_binary_accuracy: 0.7523 - val_f1_score: 0.7560\n",
      "Epoch 179/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4797 - binary_accuracy: 0.8009 - f1_score: 0.8011 - val_loss: 0.5197 - val_binary_accuracy: 0.7613 - val_f1_score: 0.7653\n",
      "Epoch 180/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4968 - binary_accuracy: 0.7913 - f1_score: 0.7923 - val_loss: 0.5341 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7452\n",
      "Epoch 181/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4873 - binary_accuracy: 0.7969 - f1_score: 0.7971 - val_loss: 0.5384 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7548\n",
      "Epoch 182/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5170 - binary_accuracy: 0.7722 - f1_score: 0.7706 - val_loss: 0.5344 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7548\n",
      "Epoch 183/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4882 - binary_accuracy: 0.7757 - f1_score: 0.7755 - val_loss: 0.5323 - val_binary_accuracy: 0.7523 - val_f1_score: 0.7568\n",
      "Epoch 184/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4806 - binary_accuracy: 0.7883 - f1_score: 0.7888 - val_loss: 0.5250 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7295\n",
      "Epoch 185/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4708 - binary_accuracy: 0.7979 - f1_score: 0.7990 - val_loss: 0.5255 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7653\n",
      "Epoch 186/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4821 - binary_accuracy: 0.7903 - f1_score: 0.7891 - val_loss: 0.5202 - val_binary_accuracy: 0.7703 - val_f1_score: 0.7648\n",
      "Epoch 187/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5016 - binary_accuracy: 0.7742 - f1_score: 0.7739 - val_loss: 0.5250 - val_binary_accuracy: 0.7523 - val_f1_score: 0.7467\n",
      "Epoch 188/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4910 - binary_accuracy: 0.7787 - f1_score: 0.7790 - val_loss: 0.5298 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7560\n",
      "Epoch 189/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4845 - binary_accuracy: 0.7888 - f1_score: 0.7877 - val_loss: 0.5234 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7467\n",
      "Epoch 190/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4980 - binary_accuracy: 0.7767 - f1_score: 0.7769 - val_loss: 0.5277 - val_binary_accuracy: 0.7387 - val_f1_score: 0.7387\n",
      "Epoch 191/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4895 - binary_accuracy: 0.7873 - f1_score: 0.7871 - val_loss: 0.5202 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7472\n",
      "Epoch 192/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4837 - binary_accuracy: 0.7883 - f1_score: 0.7893 - val_loss: 0.5190 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7564\n",
      "Epoch 193/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4932 - binary_accuracy: 0.7873 - f1_score: 0.7882 - val_loss: 0.5217 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7567\n",
      "Epoch 194/1000\n",
      "27/31 [=========================>....] - ETA: 0s - loss: 0.4875 - binary_accuracy: 0.7737 - f1_score: 0.7726INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_F1\\assets\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.4816 - binary_accuracy: 0.7807 - f1_score: 0.7799 - val_loss: 0.5194 - val_binary_accuracy: 0.7793 - val_f1_score: 0.7829\n",
      "Epoch 195/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4893 - binary_accuracy: 0.7873 - f1_score: 0.7869 - val_loss: 0.5234 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7472\n",
      "Epoch 196/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4787 - binary_accuracy: 0.7893 - f1_score: 0.7891 - val_loss: 0.5168 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7736\n",
      "Epoch 197/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4890 - binary_accuracy: 0.7742 - f1_score: 0.7728 - val_loss: 0.5143 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7560\n",
      "Epoch 198/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4720 - binary_accuracy: 0.7928 - f1_score: 0.7940 - val_loss: 0.5225 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7452\n",
      "Epoch 199/1000\n",
      "25/31 [=======================>......] - ETA: 0s - loss: 0.4640 - binary_accuracy: 0.8094 - f1_score: 0.8082INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 0.4787 - binary_accuracy: 0.7959 - f1_score: 0.7948 - val_loss: 0.5194 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7829\n",
      "Epoch 200/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4648 - binary_accuracy: 0.7989 - f1_score: 0.7970 - val_loss: 0.5226 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7461\n",
      "Epoch 201/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4865 - binary_accuracy: 0.7868 - f1_score: 0.7856 - val_loss: 0.5155 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 202/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4862 - binary_accuracy: 0.7873 - f1_score: 0.7871 - val_loss: 0.5238 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 203/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4785 - binary_accuracy: 0.7858 - f1_score: 0.7852 - val_loss: 0.5245 - val_binary_accuracy: 0.7613 - val_f1_score: 0.7560\n",
      "Epoch 204/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4968 - binary_accuracy: 0.7878 - f1_score: 0.7868 - val_loss: 0.5201 - val_binary_accuracy: 0.7387 - val_f1_score: 0.7387\n",
      "Epoch 205/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4962 - binary_accuracy: 0.7823 - f1_score: 0.7830 - val_loss: 0.5216 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 206/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4839 - binary_accuracy: 0.7893 - f1_score: 0.7888 - val_loss: 0.5172 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7564\n",
      "Epoch 207/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4951 - binary_accuracy: 0.7818 - f1_score: 0.7821 - val_loss: 0.5195 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7656\n",
      "Epoch 208/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4787 - binary_accuracy: 0.7818 - f1_score: 0.7820 - val_loss: 0.5193 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 209/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4948 - binary_accuracy: 0.7868 - f1_score: 0.7881 - val_loss: 0.5182 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 210/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4730 - binary_accuracy: 0.7858 - f1_score: 0.7850 - val_loss: 0.5211 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7656\n",
      "Epoch 211/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4791 - binary_accuracy: 0.7928 - f1_score: 0.7922 - val_loss: 0.5240 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7656\n",
      "Epoch 212/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4815 - binary_accuracy: 0.7944 - f1_score: 0.7951 - val_loss: 0.5200 - val_binary_accuracy: 0.7387 - val_f1_score: 0.7387\n",
      "Epoch 213/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4747 - binary_accuracy: 0.7853 - f1_score: 0.7840 - val_loss: 0.5073 - val_binary_accuracy: 0.7703 - val_f1_score: 0.7648\n",
      "Epoch 214/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5033 - binary_accuracy: 0.7692 - f1_score: 0.7683 - val_loss: 0.5205 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7656\n",
      "Epoch 215/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4885 - binary_accuracy: 0.7928 - f1_score: 0.7921 - val_loss: 0.5160 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7653\n",
      "Epoch 216/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4758 - binary_accuracy: 0.7994 - f1_score: 0.7989 - val_loss: 0.5145 - val_binary_accuracy: 0.7432 - val_f1_score: 0.7452\n",
      "Epoch 217/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4814 - binary_accuracy: 0.7818 - f1_score: 0.7809 - val_loss: 0.5154 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7568\n",
      "Epoch 218/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4719 - binary_accuracy: 0.7954 - f1_score: 0.7952 - val_loss: 0.5176 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7548\n",
      "Epoch 219/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4676 - binary_accuracy: 0.7979 - f1_score: 0.7970 - val_loss: 0.5169 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 220/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4846 - binary_accuracy: 0.7944 - f1_score: 0.7930 - val_loss: 0.5205 - val_binary_accuracy: 0.7613 - val_f1_score: 0.7653\n",
      "Epoch 221/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4979 - binary_accuracy: 0.7807 - f1_score: 0.7797 - val_loss: 0.5159 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7548\n",
      "Epoch 222/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4971 - binary_accuracy: 0.7833 - f1_score: 0.7823 - val_loss: 0.5154 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7656\n",
      "Epoch 223/1000\n",
      "26/31 [========================>.....] - ETA: 0s - loss: 0.4923 - binary_accuracy: 0.7788 - f1_score: 0.7782INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_F1\\assets\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 0.4963 - binary_accuracy: 0.7777 - f1_score: 0.7778 - val_loss: 0.5169 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 224/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4668 - binary_accuracy: 0.7999 - f1_score: 0.7993 - val_loss: 0.5188 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7747\n",
      "Epoch 225/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4700 - binary_accuracy: 0.7989 - f1_score: 0.7983 - val_loss: 0.5126 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 226/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5012 - binary_accuracy: 0.7848 - f1_score: 0.7862 - val_loss: 0.5168 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 227/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4499 - binary_accuracy: 0.8206 - f1_score: 0.8213 - val_loss: 0.5229 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7477\n",
      "Epoch 228/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4709 - binary_accuracy: 0.7989 - f1_score: 0.7993 - val_loss: 0.5188 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7472\n",
      "Epoch 229/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4698 - binary_accuracy: 0.7893 - f1_score: 0.7887 - val_loss: 0.5206 - val_binary_accuracy: 0.7523 - val_f1_score: 0.7564\n",
      "Epoch 230/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4969 - binary_accuracy: 0.7848 - f1_score: 0.7860 - val_loss: 0.5179 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7564\n",
      "Epoch 231/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4641 - binary_accuracy: 0.8070 - f1_score: 0.8062 - val_loss: 0.5220 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7564\n",
      "Epoch 232/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4709 - binary_accuracy: 0.7853 - f1_score: 0.7852 - val_loss: 0.5210 - val_binary_accuracy: 0.7613 - val_f1_score: 0.7560\n",
      "Epoch 233/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4849 - binary_accuracy: 0.7903 - f1_score: 0.7899 - val_loss: 0.5205 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 234/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4480 - binary_accuracy: 0.8115 - f1_score: 0.8123 - val_loss: 0.5249 - val_binary_accuracy: 0.7523 - val_f1_score: 0.7476\n",
      "Epoch 235/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4754 - binary_accuracy: 0.8029 - f1_score: 0.8033 - val_loss: 0.5234 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7653\n",
      "Epoch 236/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4833 - binary_accuracy: 0.7878 - f1_score: 0.7865 - val_loss: 0.5215 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 237/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4702 - binary_accuracy: 0.7928 - f1_score: 0.7921 - val_loss: 0.5171 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7467\n",
      "Epoch 238/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4914 - binary_accuracy: 0.7752 - f1_score: 0.7746 - val_loss: 0.5203 - val_binary_accuracy: 0.7387 - val_f1_score: 0.7374\n",
      "Epoch 239/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4710 - binary_accuracy: 0.7883 - f1_score: 0.7857 - val_loss: 0.5224 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7568\n",
      "Epoch 240/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4690 - binary_accuracy: 0.8004 - f1_score: 0.8013 - val_loss: 0.5139 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7560\n",
      "Epoch 241/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4620 - binary_accuracy: 0.8034 - f1_score: 0.8032 - val_loss: 0.5226 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7477\n",
      "Epoch 242/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4870 - binary_accuracy: 0.7818 - f1_score: 0.7810 - val_loss: 0.5136 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7653\n",
      "Epoch 243/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4604 - binary_accuracy: 0.8029 - f1_score: 0.8031 - val_loss: 0.5110 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 244/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4856 - binary_accuracy: 0.7893 - f1_score: 0.7899 - val_loss: 0.5218 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7279\n",
      "Epoch 245/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5090 - binary_accuracy: 0.7843 - f1_score: 0.7838 - val_loss: 0.5180 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7567\n",
      "Epoch 246/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4774 - binary_accuracy: 0.8009 - f1_score: 0.8013 - val_loss: 0.5131 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7656\n",
      "Epoch 247/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4755 - binary_accuracy: 0.7954 - f1_score: 0.7959 - val_loss: 0.5117 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7560\n",
      "Epoch 248/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4686 - binary_accuracy: 0.7944 - f1_score: 0.7942 - val_loss: 0.5227 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7472\n",
      "Epoch 249/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4748 - binary_accuracy: 0.7858 - f1_score: 0.7860 - val_loss: 0.5194 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7656\n",
      "Epoch 250/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4870 - binary_accuracy: 0.7848 - f1_score: 0.7850 - val_loss: 0.5175 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7747\n",
      "Epoch 251/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4417 - binary_accuracy: 0.8226 - f1_score: 0.8215 - val_loss: 0.5162 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7656\n",
      "Epoch 252/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4867 - binary_accuracy: 0.7828 - f1_score: 0.7829 - val_loss: 0.5260 - val_binary_accuracy: 0.7387 - val_f1_score: 0.7366\n",
      "Epoch 253/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4500 - binary_accuracy: 0.8160 - f1_score: 0.8151 - val_loss: 0.5204 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7656\n",
      "Epoch 254/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4940 - binary_accuracy: 0.7702 - f1_score: 0.7693 - val_loss: 0.5100 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7656\n",
      "Epoch 255/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4455 - binary_accuracy: 0.8160 - f1_score: 0.8175 - val_loss: 0.5148 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 256/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4781 - binary_accuracy: 0.7939 - f1_score: 0.7928 - val_loss: 0.5114 - val_binary_accuracy: 0.7613 - val_f1_score: 0.7653\n",
      "Epoch 257/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4766 - binary_accuracy: 0.7969 - f1_score: 0.7970 - val_loss: 0.5137 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7642\n",
      "Epoch 258/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4681 - binary_accuracy: 0.7954 - f1_score: 0.7950 - val_loss: 0.5124 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 259/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4721 - binary_accuracy: 0.8044 - f1_score: 0.8053 - val_loss: 0.5074 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 260/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4841 - binary_accuracy: 0.7823 - f1_score: 0.7816 - val_loss: 0.5104 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7568\n",
      "Epoch 261/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4752 - binary_accuracy: 0.7933 - f1_score: 0.7942 - val_loss: 0.5143 - val_binary_accuracy: 0.7613 - val_f1_score: 0.7642\n",
      "Epoch 262/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4695 - binary_accuracy: 0.8009 - f1_score: 0.8009 - val_loss: 0.5133 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 263/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4707 - binary_accuracy: 0.7994 - f1_score: 0.7991 - val_loss: 0.5112 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 264/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4581 - binary_accuracy: 0.8100 - f1_score: 0.8104 - val_loss: 0.5068 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 265/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4899 - binary_accuracy: 0.7868 - f1_score: 0.7858 - val_loss: 0.5087 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7656\n",
      "Epoch 266/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4875 - binary_accuracy: 0.7888 - f1_score: 0.7891 - val_loss: 0.5109 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7656\n",
      "Epoch 267/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4652 - binary_accuracy: 0.7989 - f1_score: 0.7987 - val_loss: 0.5124 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 268/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4603 - binary_accuracy: 0.8034 - f1_score: 0.8020 - val_loss: 0.5156 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7564\n",
      "Epoch 269/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4650 - binary_accuracy: 0.8145 - f1_score: 0.8154 - val_loss: 0.5110 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7747\n",
      "Epoch 270/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4620 - binary_accuracy: 0.8009 - f1_score: 0.8012 - val_loss: 0.5150 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 271/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4802 - binary_accuracy: 0.7807 - f1_score: 0.7808 - val_loss: 0.5176 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 272/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4637 - binary_accuracy: 0.8085 - f1_score: 0.8081 - val_loss: 0.5104 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7467\n",
      "Epoch 273/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4586 - binary_accuracy: 0.8100 - f1_score: 0.8091 - val_loss: 0.5088 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 274/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4568 - binary_accuracy: 0.8130 - f1_score: 0.8133 - val_loss: 0.5110 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 275/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4597 - binary_accuracy: 0.8059 - f1_score: 0.8063 - val_loss: 0.5097 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7656\n",
      "Epoch 276/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4585 - binary_accuracy: 0.8065 - f1_score: 0.8062 - val_loss: 0.5119 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7461\n",
      "Epoch 277/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4661 - binary_accuracy: 0.8049 - f1_score: 0.8043 - val_loss: 0.5055 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7653\n",
      "Epoch 278/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4543 - binary_accuracy: 0.8216 - f1_score: 0.8214 - val_loss: 0.5077 - val_binary_accuracy: 0.7613 - val_f1_score: 0.7653\n",
      "Epoch 279/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4674 - binary_accuracy: 0.8044 - f1_score: 0.8041 - val_loss: 0.5115 - val_binary_accuracy: 0.7523 - val_f1_score: 0.7461\n",
      "Epoch 280/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4753 - binary_accuracy: 0.7949 - f1_score: 0.7941 - val_loss: 0.5104 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7476\n",
      "Epoch 281/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4761 - binary_accuracy: 0.7974 - f1_score: 0.7972 - val_loss: 0.5131 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 282/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4780 - binary_accuracy: 0.7858 - f1_score: 0.7847 - val_loss: 0.5126 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7452\n",
      "Epoch 283/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4471 - binary_accuracy: 0.8170 - f1_score: 0.8164 - val_loss: 0.5149 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7467\n",
      "Epoch 284/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4435 - binary_accuracy: 0.8135 - f1_score: 0.8132 - val_loss: 0.5083 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7657\n",
      "Epoch 285/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4659 - binary_accuracy: 0.8004 - f1_score: 0.8002 - val_loss: 0.5036 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 286/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4504 - binary_accuracy: 0.8145 - f1_score: 0.8145 - val_loss: 0.5046 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 287/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4704 - binary_accuracy: 0.7964 - f1_score: 0.7955 - val_loss: 0.5055 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 288/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4683 - binary_accuracy: 0.7994 - f1_score: 0.8002 - val_loss: 0.5068 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7555\n",
      "Epoch 289/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4501 - binary_accuracy: 0.8085 - f1_score: 0.8081 - val_loss: 0.5011 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 290/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4764 - binary_accuracy: 0.7863 - f1_score: 0.7858 - val_loss: 0.5003 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7564\n",
      "Epoch 291/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4806 - binary_accuracy: 0.7923 - f1_score: 0.7941 - val_loss: 0.5039 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 292/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4679 - binary_accuracy: 0.7974 - f1_score: 0.7971 - val_loss: 0.4998 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 293/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4671 - binary_accuracy: 0.7999 - f1_score: 0.7981 - val_loss: 0.5017 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 294/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4684 - binary_accuracy: 0.7964 - f1_score: 0.7952 - val_loss: 0.5057 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7829\n",
      "Epoch 295/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4685 - binary_accuracy: 0.7883 - f1_score: 0.7878 - val_loss: 0.4990 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 296/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4608 - binary_accuracy: 0.7959 - f1_score: 0.7971 - val_loss: 0.5069 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 297/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4683 - binary_accuracy: 0.7954 - f1_score: 0.7948 - val_loss: 0.5123 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7452\n",
      "Epoch 298/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4704 - binary_accuracy: 0.8039 - f1_score: 0.8032 - val_loss: 0.5033 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 299/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4545 - binary_accuracy: 0.8120 - f1_score: 0.8124 - val_loss: 0.4952 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7748\n",
      "Epoch 300/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4426 - binary_accuracy: 0.8130 - f1_score: 0.8123 - val_loss: 0.5069 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 301/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4685 - binary_accuracy: 0.7933 - f1_score: 0.7924 - val_loss: 0.5050 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 302/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4596 - binary_accuracy: 0.8120 - f1_score: 0.8133 - val_loss: 0.5070 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7560\n",
      "Epoch 303/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4760 - binary_accuracy: 0.7863 - f1_score: 0.7859 - val_loss: 0.5017 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7653\n",
      "Epoch 304/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4646 - binary_accuracy: 0.7903 - f1_score: 0.7877 - val_loss: 0.5023 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7560\n",
      "Epoch 305/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4658 - binary_accuracy: 0.8090 - f1_score: 0.8092 - val_loss: 0.5015 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 306/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4542 - binary_accuracy: 0.8065 - f1_score: 0.8059 - val_loss: 0.5049 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7560\n",
      "Epoch 307/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4486 - binary_accuracy: 0.8130 - f1_score: 0.8133 - val_loss: 0.5018 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 308/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4662 - binary_accuracy: 0.7984 - f1_score: 0.7982 - val_loss: 0.5017 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7564\n",
      "Epoch 309/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4768 - binary_accuracy: 0.7964 - f1_score: 0.7963 - val_loss: 0.5043 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 310/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4408 - binary_accuracy: 0.8150 - f1_score: 0.8142 - val_loss: 0.5036 - val_binary_accuracy: 0.7613 - val_f1_score: 0.7648\n",
      "Epoch 311/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4700 - binary_accuracy: 0.7979 - f1_score: 0.7980 - val_loss: 0.4997 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7829\n",
      "Epoch 312/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4569 - binary_accuracy: 0.7939 - f1_score: 0.7931 - val_loss: 0.4989 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7829\n",
      "Epoch 313/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4781 - binary_accuracy: 0.7913 - f1_score: 0.7908 - val_loss: 0.5006 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 314/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4634 - binary_accuracy: 0.8070 - f1_score: 0.8061 - val_loss: 0.5071 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7555\n",
      "Epoch 315/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4573 - binary_accuracy: 0.8165 - f1_score: 0.8154 - val_loss: 0.5091 - val_binary_accuracy: 0.7387 - val_f1_score: 0.7374\n",
      "Epoch 316/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4725 - binary_accuracy: 0.7964 - f1_score: 0.7962 - val_loss: 0.5055 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7560\n",
      "Epoch 317/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4569 - binary_accuracy: 0.8135 - f1_score: 0.8143 - val_loss: 0.5083 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7467\n",
      "Epoch 318/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4662 - binary_accuracy: 0.7984 - f1_score: 0.7981 - val_loss: 0.5090 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7653\n",
      "Epoch 319/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4774 - binary_accuracy: 0.7913 - f1_score: 0.7910 - val_loss: 0.5049 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7653\n",
      "Epoch 320/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4624 - binary_accuracy: 0.8024 - f1_score: 0.8024 - val_loss: 0.5042 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7747\n",
      "Epoch 321/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4621 - binary_accuracy: 0.8019 - f1_score: 0.8023 - val_loss: 0.5091 - val_binary_accuracy: 0.7387 - val_f1_score: 0.7380\n",
      "Epoch 322/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4733 - binary_accuracy: 0.8019 - f1_score: 0.8010 - val_loss: 0.5060 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7560\n",
      "Epoch 323/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4645 - binary_accuracy: 0.8044 - f1_score: 0.8053 - val_loss: 0.5050 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7560\n",
      "Epoch 324/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4566 - binary_accuracy: 0.7994 - f1_score: 0.7977 - val_loss: 0.5050 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7472\n",
      "Epoch 325/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4818 - binary_accuracy: 0.7878 - f1_score: 0.7880 - val_loss: 0.5001 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 326/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4357 - binary_accuracy: 0.8115 - f1_score: 0.8123 - val_loss: 0.5017 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7560\n",
      "Epoch 327/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4606 - binary_accuracy: 0.8024 - f1_score: 0.8031 - val_loss: 0.5039 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 328/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4414 - binary_accuracy: 0.8155 - f1_score: 0.8154 - val_loss: 0.5017 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7467\n",
      "Epoch 329/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4533 - binary_accuracy: 0.8120 - f1_score: 0.8112 - val_loss: 0.5021 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7656\n",
      "Epoch 330/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4280 - binary_accuracy: 0.8256 - f1_score: 0.8254 - val_loss: 0.5011 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 331/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4643 - binary_accuracy: 0.7949 - f1_score: 0.7948 - val_loss: 0.5029 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7653\n",
      "Epoch 332/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4316 - binary_accuracy: 0.8231 - f1_score: 0.8244 - val_loss: 0.5070 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7656\n",
      "Epoch 333/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4459 - binary_accuracy: 0.8105 - f1_score: 0.8114 - val_loss: 0.5028 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7555\n",
      "Epoch 334/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4605 - binary_accuracy: 0.8090 - f1_score: 0.8081 - val_loss: 0.4993 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7555\n",
      "Epoch 335/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4546 - binary_accuracy: 0.8059 - f1_score: 0.8070 - val_loss: 0.4984 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7560\n",
      "Epoch 336/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4542 - binary_accuracy: 0.8024 - f1_score: 0.8008 - val_loss: 0.5017 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7653\n",
      "Epoch 337/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4469 - binary_accuracy: 0.8054 - f1_score: 0.8060 - val_loss: 0.4986 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 338/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4570 - binary_accuracy: 0.8065 - f1_score: 0.8064 - val_loss: 0.5034 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 339/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4548 - binary_accuracy: 0.8125 - f1_score: 0.8113 - val_loss: 0.5046 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7564\n",
      "Epoch 340/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4524 - binary_accuracy: 0.8075 - f1_score: 0.8082 - val_loss: 0.5024 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 341/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4521 - binary_accuracy: 0.8054 - f1_score: 0.8052 - val_loss: 0.5013 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 342/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4582 - binary_accuracy: 0.8049 - f1_score: 0.8053 - val_loss: 0.5103 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7467\n",
      "Epoch 343/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4282 - binary_accuracy: 0.8246 - f1_score: 0.8244 - val_loss: 0.5035 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 344/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4549 - binary_accuracy: 0.8059 - f1_score: 0.8052 - val_loss: 0.5047 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7560\n",
      "Epoch 345/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4624 - binary_accuracy: 0.8029 - f1_score: 0.8030 - val_loss: 0.5002 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 346/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4612 - binary_accuracy: 0.7969 - f1_score: 0.7981 - val_loss: 0.4978 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 347/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4496 - binary_accuracy: 0.8090 - f1_score: 0.8083 - val_loss: 0.5036 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 348/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4467 - binary_accuracy: 0.8080 - f1_score: 0.8071 - val_loss: 0.4980 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 349/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4818 - binary_accuracy: 0.7959 - f1_score: 0.7961 - val_loss: 0.5051 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7656\n",
      "Epoch 350/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4629 - binary_accuracy: 0.8039 - f1_score: 0.8041 - val_loss: 0.5028 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 351/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4615 - binary_accuracy: 0.8054 - f1_score: 0.8051 - val_loss: 0.4983 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 352/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4404 - binary_accuracy: 0.8231 - f1_score: 0.8222 - val_loss: 0.5018 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7829\n",
      "Epoch 353/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4447 - binary_accuracy: 0.8211 - f1_score: 0.8204 - val_loss: 0.4954 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 354/1000\n",
      "26/31 [========================>.....] - ETA: 0s - loss: 0.4248 - binary_accuracy: 0.8233 - f1_score: 0.8209INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_F1\\assets\n",
      "31/31 [==============================] - 0s 15ms/step - loss: 0.4416 - binary_accuracy: 0.8110 - f1_score: 0.8084 - val_loss: 0.4962 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 355/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4626 - binary_accuracy: 0.7984 - f1_score: 0.7990 - val_loss: 0.5017 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7653\n",
      "Epoch 356/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4557 - binary_accuracy: 0.8105 - f1_score: 0.8099 - val_loss: 0.5005 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7653\n",
      "Epoch 357/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4487 - binary_accuracy: 0.8150 - f1_score: 0.8143 - val_loss: 0.4972 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 358/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4689 - binary_accuracy: 0.8019 - f1_score: 0.8019 - val_loss: 0.4953 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 359/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4576 - binary_accuracy: 0.8075 - f1_score: 0.8070 - val_loss: 0.4975 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 360/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4394 - binary_accuracy: 0.8256 - f1_score: 0.8256 - val_loss: 0.4933 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 361/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4662 - binary_accuracy: 0.7999 - f1_score: 0.8010 - val_loss: 0.4957 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 362/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4474 - binary_accuracy: 0.8110 - f1_score: 0.8111 - val_loss: 0.4917 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 363/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4643 - binary_accuracy: 0.8044 - f1_score: 0.8033 - val_loss: 0.4933 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 364/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4404 - binary_accuracy: 0.8090 - f1_score: 0.8090 - val_loss: 0.4967 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7648\n",
      "Epoch 365/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4626 - binary_accuracy: 0.7999 - f1_score: 0.8010 - val_loss: 0.4933 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 366/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4678 - binary_accuracy: 0.8054 - f1_score: 0.8040 - val_loss: 0.4931 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 367/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4584 - binary_accuracy: 0.8059 - f1_score: 0.8051 - val_loss: 0.4934 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 368/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4417 - binary_accuracy: 0.8145 - f1_score: 0.8142 - val_loss: 0.4949 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 369/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4503 - binary_accuracy: 0.8065 - f1_score: 0.8061 - val_loss: 0.4974 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 370/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4619 - binary_accuracy: 0.8029 - f1_score: 0.8021 - val_loss: 0.4911 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 371/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4481 - binary_accuracy: 0.8160 - f1_score: 0.8164 - val_loss: 0.4935 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 372/1000\n",
      "27/31 [=========================>....] - ETA: 0s - loss: 0.4399 - binary_accuracy: 0.8183 - f1_score: 0.8177INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_F1\\assets\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.4343 - binary_accuracy: 0.8236 - f1_score: 0.8233 - val_loss: 0.4889 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 373/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4451 - binary_accuracy: 0.8125 - f1_score: 0.8122 - val_loss: 0.4909 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 374/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4365 - binary_accuracy: 0.8191 - f1_score: 0.8185 - val_loss: 0.4859 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 375/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4732 - binary_accuracy: 0.8075 - f1_score: 0.8073 - val_loss: 0.4924 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 376/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4491 - binary_accuracy: 0.8125 - f1_score: 0.8123 - val_loss: 0.4945 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 377/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4666 - binary_accuracy: 0.7913 - f1_score: 0.7918 - val_loss: 0.4969 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 378/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4486 - binary_accuracy: 0.8105 - f1_score: 0.8101 - val_loss: 0.4964 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 379/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4596 - binary_accuracy: 0.8019 - f1_score: 0.8022 - val_loss: 0.4911 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 380/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4396 - binary_accuracy: 0.8296 - f1_score: 0.8295 - val_loss: 0.4878 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 381/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4684 - binary_accuracy: 0.7999 - f1_score: 0.7991 - val_loss: 0.4896 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 382/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4559 - binary_accuracy: 0.8196 - f1_score: 0.8195 - val_loss: 0.4880 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 383/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4216 - binary_accuracy: 0.8261 - f1_score: 0.8263 - val_loss: 0.4884 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 384/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4512 - binary_accuracy: 0.8059 - f1_score: 0.8062 - val_loss: 0.4856 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 385/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4425 - binary_accuracy: 0.8125 - f1_score: 0.8134 - val_loss: 0.4922 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 386/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4612 - binary_accuracy: 0.8059 - f1_score: 0.8063 - val_loss: 0.4908 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 387/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4642 - binary_accuracy: 0.8065 - f1_score: 0.8062 - val_loss: 0.4916 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7653\n",
      "Epoch 388/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4659 - binary_accuracy: 0.8070 - f1_score: 0.8074 - val_loss: 0.4919 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 389/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4554 - binary_accuracy: 0.8004 - f1_score: 0.7999 - val_loss: 0.4955 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 390/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4535 - binary_accuracy: 0.8024 - f1_score: 0.8032 - val_loss: 0.4937 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 391/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4444 - binary_accuracy: 0.8206 - f1_score: 0.8202 - val_loss: 0.4941 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 392/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4525 - binary_accuracy: 0.8165 - f1_score: 0.8172 - val_loss: 0.4941 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 393/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4580 - binary_accuracy: 0.8085 - f1_score: 0.8071 - val_loss: 0.4907 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 394/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4472 - binary_accuracy: 0.8125 - f1_score: 0.8123 - val_loss: 0.4998 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7653\n",
      "Epoch 395/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4402 - binary_accuracy: 0.8251 - f1_score: 0.8255 - val_loss: 0.4922 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7747\n",
      "Epoch 396/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4406 - binary_accuracy: 0.8160 - f1_score: 0.8162 - val_loss: 0.4845 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 397/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4478 - binary_accuracy: 0.8140 - f1_score: 0.8134 - val_loss: 0.4914 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 398/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4605 - binary_accuracy: 0.8075 - f1_score: 0.8069 - val_loss: 0.4877 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 399/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4482 - binary_accuracy: 0.8150 - f1_score: 0.8155 - val_loss: 0.4872 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 400/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4609 - binary_accuracy: 0.7969 - f1_score: 0.7961 - val_loss: 0.4850 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 401/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4479 - binary_accuracy: 0.8170 - f1_score: 0.8185 - val_loss: 0.4840 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 402/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4416 - binary_accuracy: 0.8170 - f1_score: 0.8163 - val_loss: 0.4864 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 403/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4445 - binary_accuracy: 0.8180 - f1_score: 0.8173 - val_loss: 0.4908 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 404/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4410 - binary_accuracy: 0.8054 - f1_score: 0.8051 - val_loss: 0.4867 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 405/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4469 - binary_accuracy: 0.8105 - f1_score: 0.8103 - val_loss: 0.4919 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 406/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4572 - binary_accuracy: 0.8095 - f1_score: 0.8091 - val_loss: 0.4875 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 407/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4440 - binary_accuracy: 0.8140 - f1_score: 0.8142 - val_loss: 0.4898 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 408/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4696 - binary_accuracy: 0.7949 - f1_score: 0.7961 - val_loss: 0.4889 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 409/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4283 - binary_accuracy: 0.8180 - f1_score: 0.8171 - val_loss: 0.4912 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 410/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4508 - binary_accuracy: 0.8105 - f1_score: 0.8104 - val_loss: 0.4915 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 411/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4508 - binary_accuracy: 0.8044 - f1_score: 0.8032 - val_loss: 0.4935 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 412/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4376 - binary_accuracy: 0.8170 - f1_score: 0.8161 - val_loss: 0.4912 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 413/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4368 - binary_accuracy: 0.8191 - f1_score: 0.8184 - val_loss: 0.4902 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 414/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4521 - binary_accuracy: 0.8196 - f1_score: 0.8194 - val_loss: 0.4897 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 415/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4176 - binary_accuracy: 0.8251 - f1_score: 0.8245 - val_loss: 0.4950 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 416/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4422 - binary_accuracy: 0.8180 - f1_score: 0.8172 - val_loss: 0.4926 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 417/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4474 - binary_accuracy: 0.8110 - f1_score: 0.8093 - val_loss: 0.4927 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 418/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4578 - binary_accuracy: 0.8014 - f1_score: 0.8009 - val_loss: 0.4926 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 419/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4706 - binary_accuracy: 0.8004 - f1_score: 0.8002 - val_loss: 0.4900 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 420/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4410 - binary_accuracy: 0.8256 - f1_score: 0.8255 - val_loss: 0.4896 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 421/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4667 - binary_accuracy: 0.8054 - f1_score: 0.8040 - val_loss: 0.4892 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 422/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4364 - binary_accuracy: 0.8231 - f1_score: 0.8234 - val_loss: 0.4904 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 423/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4237 - binary_accuracy: 0.8246 - f1_score: 0.8244 - val_loss: 0.4913 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 424/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4371 - binary_accuracy: 0.8226 - f1_score: 0.8224 - val_loss: 0.4881 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 425/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4577 - binary_accuracy: 0.8009 - f1_score: 0.8002 - val_loss: 0.4891 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 426/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4321 - binary_accuracy: 0.8145 - f1_score: 0.8142 - val_loss: 0.4930 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 427/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4353 - binary_accuracy: 0.8236 - f1_score: 0.8235 - val_loss: 0.4851 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 428/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4574 - binary_accuracy: 0.8039 - f1_score: 0.8041 - val_loss: 0.4910 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 429/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4496 - binary_accuracy: 0.8059 - f1_score: 0.8062 - val_loss: 0.4942 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 430/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4539 - binary_accuracy: 0.8125 - f1_score: 0.8122 - val_loss: 0.4892 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 431/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4593 - binary_accuracy: 0.7984 - f1_score: 0.7981 - val_loss: 0.4894 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8014\n",
      "Epoch 432/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4540 - binary_accuracy: 0.8009 - f1_score: 0.7990 - val_loss: 0.4914 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 433/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4583 - binary_accuracy: 0.8029 - f1_score: 0.8018 - val_loss: 0.4895 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 434/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4424 - binary_accuracy: 0.8125 - f1_score: 0.8113 - val_loss: 0.4941 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 435/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4378 - binary_accuracy: 0.8115 - f1_score: 0.8110 - val_loss: 0.4910 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 436/1000\n",
      "26/31 [========================>.....] - ETA: 0s - loss: 0.4409 - binary_accuracy: 0.8125 - f1_score: 0.8118INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_F1\\assets\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 0.4419 - binary_accuracy: 0.8125 - f1_score: 0.8121 - val_loss: 0.4845 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8108\n",
      "Epoch 437/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4384 - binary_accuracy: 0.8075 - f1_score: 0.8074 - val_loss: 0.4895 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 438/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4463 - binary_accuracy: 0.8085 - f1_score: 0.8081 - val_loss: 0.4943 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 439/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4466 - binary_accuracy: 0.8211 - f1_score: 0.8213 - val_loss: 0.4860 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 440/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4561 - binary_accuracy: 0.8024 - f1_score: 0.8022 - val_loss: 0.4854 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 441/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4611 - binary_accuracy: 0.8029 - f1_score: 0.8030 - val_loss: 0.4848 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 442/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4298 - binary_accuracy: 0.8236 - f1_score: 0.8233 - val_loss: 0.4878 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 443/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4369 - binary_accuracy: 0.8201 - f1_score: 0.8202 - val_loss: 0.4861 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7927\n",
      "Epoch 444/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4337 - binary_accuracy: 0.8276 - f1_score: 0.8286 - val_loss: 0.4867 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 445/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4371 - binary_accuracy: 0.8155 - f1_score: 0.8154 - val_loss: 0.4886 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 446/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4468 - binary_accuracy: 0.8150 - f1_score: 0.8152 - val_loss: 0.4910 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 447/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4123 - binary_accuracy: 0.8357 - f1_score: 0.8357 - val_loss: 0.4890 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 448/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4406 - binary_accuracy: 0.8135 - f1_score: 0.8132 - val_loss: 0.4852 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8108\n",
      "Epoch 449/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4484 - binary_accuracy: 0.8180 - f1_score: 0.8184 - val_loss: 0.4858 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 450/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4317 - binary_accuracy: 0.8175 - f1_score: 0.8174 - val_loss: 0.4931 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 451/1000\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.4385 - binary_accuracy: 0.8165 - f1_score: 0.8163 - val_loss: 0.4919 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 452/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4345 - binary_accuracy: 0.8221 - f1_score: 0.8222 - val_loss: 0.4960 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7653\n",
      "Epoch 453/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4701 - binary_accuracy: 0.8014 - f1_score: 0.8012 - val_loss: 0.4910 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 454/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4230 - binary_accuracy: 0.8246 - f1_score: 0.8254 - val_loss: 0.4873 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 455/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4584 - binary_accuracy: 0.8115 - f1_score: 0.8111 - val_loss: 0.4901 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 456/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4626 - binary_accuracy: 0.8014 - f1_score: 0.8009 - val_loss: 0.4895 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 457/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4237 - binary_accuracy: 0.8276 - f1_score: 0.8274 - val_loss: 0.4864 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 458/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4479 - binary_accuracy: 0.8165 - f1_score: 0.8174 - val_loss: 0.4882 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 459/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4596 - binary_accuracy: 0.8110 - f1_score: 0.8111 - val_loss: 0.4848 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 460/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4522 - binary_accuracy: 0.8054 - f1_score: 0.8053 - val_loss: 0.4889 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 461/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4478 - binary_accuracy: 0.8120 - f1_score: 0.8123 - val_loss: 0.4851 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 462/1000\n",
      "27/31 [=========================>....] - ETA: 0s - loss: 0.4244 - binary_accuracy: 0.8218 - f1_score: 0.8213INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_acc\\assets\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MLP_Classification_F1\\assets\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 0.4329 - binary_accuracy: 0.8125 - f1_score: 0.8121 - val_loss: 0.4779 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 463/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4411 - binary_accuracy: 0.8115 - f1_score: 0.8111 - val_loss: 0.4878 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 464/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4452 - binary_accuracy: 0.8059 - f1_score: 0.8064 - val_loss: 0.4864 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7922\n",
      "Epoch 465/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4477 - binary_accuracy: 0.8110 - f1_score: 0.8111 - val_loss: 0.4806 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 466/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4649 - binary_accuracy: 0.8009 - f1_score: 0.8000 - val_loss: 0.4796 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 467/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4327 - binary_accuracy: 0.8291 - f1_score: 0.8285 - val_loss: 0.4842 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 468/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4354 - binary_accuracy: 0.8191 - f1_score: 0.8184 - val_loss: 0.4864 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 469/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4446 - binary_accuracy: 0.8044 - f1_score: 0.8042 - val_loss: 0.4889 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 470/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4578 - binary_accuracy: 0.8039 - f1_score: 0.8042 - val_loss: 0.4844 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 471/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4202 - binary_accuracy: 0.8342 - f1_score: 0.8347 - val_loss: 0.4811 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8108\n",
      "Epoch 472/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4239 - binary_accuracy: 0.8251 - f1_score: 0.8245 - val_loss: 0.4802 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 473/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4354 - binary_accuracy: 0.8226 - f1_score: 0.8223 - val_loss: 0.4846 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 474/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4454 - binary_accuracy: 0.8120 - f1_score: 0.8122 - val_loss: 0.4816 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 475/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4396 - binary_accuracy: 0.8160 - f1_score: 0.8163 - val_loss: 0.4823 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 476/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4281 - binary_accuracy: 0.8221 - f1_score: 0.8225 - val_loss: 0.4850 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 477/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4305 - binary_accuracy: 0.8236 - f1_score: 0.8231 - val_loss: 0.4790 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 478/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4496 - binary_accuracy: 0.8140 - f1_score: 0.8141 - val_loss: 0.4778 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 479/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4179 - binary_accuracy: 0.8372 - f1_score: 0.8366 - val_loss: 0.4815 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 480/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4590 - binary_accuracy: 0.8049 - f1_score: 0.8041 - val_loss: 0.4897 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 481/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4516 - binary_accuracy: 0.8145 - f1_score: 0.8143 - val_loss: 0.4846 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 482/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4218 - binary_accuracy: 0.8241 - f1_score: 0.8255 - val_loss: 0.4801 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 483/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4560 - binary_accuracy: 0.8185 - f1_score: 0.8183 - val_loss: 0.4777 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 484/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4290 - binary_accuracy: 0.8256 - f1_score: 0.8255 - val_loss: 0.4850 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7833\n",
      "Epoch 485/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4561 - binary_accuracy: 0.8120 - f1_score: 0.8112 - val_loss: 0.4824 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 486/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4167 - binary_accuracy: 0.8226 - f1_score: 0.8223 - val_loss: 0.4810 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 487/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4424 - binary_accuracy: 0.8201 - f1_score: 0.8204 - val_loss: 0.4830 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 488/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4462 - binary_accuracy: 0.8160 - f1_score: 0.8154 - val_loss: 0.4891 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 489/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4330 - binary_accuracy: 0.8211 - f1_score: 0.8202 - val_loss: 0.4825 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 490/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4624 - binary_accuracy: 0.7979 - f1_score: 0.7979 - val_loss: 0.4870 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 491/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4385 - binary_accuracy: 0.8130 - f1_score: 0.8112 - val_loss: 0.4826 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 492/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4439 - binary_accuracy: 0.8115 - f1_score: 0.8112 - val_loss: 0.4891 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 493/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4478 - binary_accuracy: 0.8140 - f1_score: 0.8132 - val_loss: 0.4862 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 494/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4421 - binary_accuracy: 0.8090 - f1_score: 0.8090 - val_loss: 0.4877 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 495/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4357 - binary_accuracy: 0.8301 - f1_score: 0.8295 - val_loss: 0.4843 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 496/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4257 - binary_accuracy: 0.8291 - f1_score: 0.8295 - val_loss: 0.4835 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 497/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4430 - binary_accuracy: 0.8191 - f1_score: 0.8182 - val_loss: 0.4825 - val_binary_accuracy: 0.7973 - val_f1_score: 0.8017\n",
      "Epoch 498/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4543 - binary_accuracy: 0.8024 - f1_score: 0.8028 - val_loss: 0.4803 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 499/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4393 - binary_accuracy: 0.8196 - f1_score: 0.8202 - val_loss: 0.4823 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 500/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4542 - binary_accuracy: 0.8130 - f1_score: 0.8121 - val_loss: 0.4822 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 501/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4503 - binary_accuracy: 0.8145 - f1_score: 0.8141 - val_loss: 0.4840 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7741\n",
      "Epoch 502/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4062 - binary_accuracy: 0.8387 - f1_score: 0.8386 - val_loss: 0.4788 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 503/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4248 - binary_accuracy: 0.8271 - f1_score: 0.8276 - val_loss: 0.4825 - val_binary_accuracy: 0.8063 - val_f1_score: 0.8108\n",
      "Epoch 504/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4383 - binary_accuracy: 0.8120 - f1_score: 0.8119 - val_loss: 0.4844 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 505/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4271 - binary_accuracy: 0.8281 - f1_score: 0.8273 - val_loss: 0.4851 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 506/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4323 - binary_accuracy: 0.8236 - f1_score: 0.8235 - val_loss: 0.4827 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 507/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4073 - binary_accuracy: 0.8357 - f1_score: 0.8355 - val_loss: 0.4883 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 508/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4362 - binary_accuracy: 0.8090 - f1_score: 0.8071 - val_loss: 0.4839 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 509/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4518 - binary_accuracy: 0.8009 - f1_score: 0.8001 - val_loss: 0.4897 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 510/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4505 - binary_accuracy: 0.8115 - f1_score: 0.8123 - val_loss: 0.4882 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 511/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4347 - binary_accuracy: 0.8241 - f1_score: 0.8244 - val_loss: 0.4885 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 512/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4393 - binary_accuracy: 0.8145 - f1_score: 0.8141 - val_loss: 0.4914 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 513/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4372 - binary_accuracy: 0.8090 - f1_score: 0.8069 - val_loss: 0.4879 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 514/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4324 - binary_accuracy: 0.8226 - f1_score: 0.8224 - val_loss: 0.4882 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 515/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4430 - binary_accuracy: 0.8140 - f1_score: 0.8140 - val_loss: 0.4839 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 516/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4407 - binary_accuracy: 0.8085 - f1_score: 0.8081 - val_loss: 0.4876 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 517/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4188 - binary_accuracy: 0.8337 - f1_score: 0.8325 - val_loss: 0.4863 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 518/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4563 - binary_accuracy: 0.7974 - f1_score: 0.7967 - val_loss: 0.4870 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7653\n",
      "Epoch 519/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4403 - binary_accuracy: 0.8135 - f1_score: 0.8133 - val_loss: 0.4850 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7836\n",
      "Epoch 520/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4306 - binary_accuracy: 0.8216 - f1_score: 0.8213 - val_loss: 0.4853 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 521/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4535 - binary_accuracy: 0.8130 - f1_score: 0.8123 - val_loss: 0.4898 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7745\n",
      "Epoch 522/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4497 - binary_accuracy: 0.8115 - f1_score: 0.8108 - val_loss: 0.4868 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 523/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4550 - binary_accuracy: 0.7984 - f1_score: 0.7978 - val_loss: 0.4859 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 524/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4412 - binary_accuracy: 0.8185 - f1_score: 0.8173 - val_loss: 0.4893 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 525/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4533 - binary_accuracy: 0.8039 - f1_score: 0.8039 - val_loss: 0.4863 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 526/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4464 - binary_accuracy: 0.8130 - f1_score: 0.8125 - val_loss: 0.4880 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 527/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4398 - binary_accuracy: 0.8175 - f1_score: 0.8173 - val_loss: 0.4827 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 528/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4428 - binary_accuracy: 0.8165 - f1_score: 0.8163 - val_loss: 0.4799 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 529/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4317 - binary_accuracy: 0.8296 - f1_score: 0.8296 - val_loss: 0.4881 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 530/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4192 - binary_accuracy: 0.8231 - f1_score: 0.8224 - val_loss: 0.4829 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 531/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4213 - binary_accuracy: 0.8352 - f1_score: 0.8356 - val_loss: 0.4840 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 532/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4168 - binary_accuracy: 0.8362 - f1_score: 0.8364 - val_loss: 0.4816 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 533/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4322 - binary_accuracy: 0.8266 - f1_score: 0.8265 - val_loss: 0.4848 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7833\n",
      "Epoch 534/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4063 - binary_accuracy: 0.8412 - f1_score: 0.8415 - val_loss: 0.4809 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 535/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4480 - binary_accuracy: 0.8135 - f1_score: 0.8133 - val_loss: 0.4841 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 536/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4374 - binary_accuracy: 0.8256 - f1_score: 0.8254 - val_loss: 0.4815 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 537/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4203 - binary_accuracy: 0.8296 - f1_score: 0.8295 - val_loss: 0.4796 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 538/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4419 - binary_accuracy: 0.8206 - f1_score: 0.8204 - val_loss: 0.4847 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 539/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4369 - binary_accuracy: 0.8216 - f1_score: 0.8213 - val_loss: 0.4810 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 540/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4173 - binary_accuracy: 0.8322 - f1_score: 0.8316 - val_loss: 0.4833 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 541/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4393 - binary_accuracy: 0.8211 - f1_score: 0.8204 - val_loss: 0.4805 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 542/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4535 - binary_accuracy: 0.8105 - f1_score: 0.8101 - val_loss: 0.4799 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 543/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4198 - binary_accuracy: 0.8296 - f1_score: 0.8294 - val_loss: 0.4836 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 544/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4671 - binary_accuracy: 0.7918 - f1_score: 0.7898 - val_loss: 0.4828 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 545/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4349 - binary_accuracy: 0.8216 - f1_score: 0.8214 - val_loss: 0.4859 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7925\n",
      "Epoch 546/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4240 - binary_accuracy: 0.8246 - f1_score: 0.8244 - val_loss: 0.4830 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 547/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4441 - binary_accuracy: 0.8120 - f1_score: 0.8111 - val_loss: 0.4803 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 548/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4402 - binary_accuracy: 0.8246 - f1_score: 0.8245 - val_loss: 0.4815 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 549/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4647 - binary_accuracy: 0.8105 - f1_score: 0.8100 - val_loss: 0.4828 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 550/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4334 - binary_accuracy: 0.8281 - f1_score: 0.8285 - val_loss: 0.4816 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 551/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4392 - binary_accuracy: 0.8120 - f1_score: 0.8122 - val_loss: 0.4852 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 552/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4382 - binary_accuracy: 0.8221 - f1_score: 0.8215 - val_loss: 0.4844 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 553/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4466 - binary_accuracy: 0.8170 - f1_score: 0.8159 - val_loss: 0.4834 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 554/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4333 - binary_accuracy: 0.8135 - f1_score: 0.8132 - val_loss: 0.4799 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 555/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4369 - binary_accuracy: 0.8160 - f1_score: 0.8152 - val_loss: 0.4808 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 556/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4260 - binary_accuracy: 0.8226 - f1_score: 0.8223 - val_loss: 0.4801 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 557/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4100 - binary_accuracy: 0.8417 - f1_score: 0.8406 - val_loss: 0.4815 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 558/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4256 - binary_accuracy: 0.8246 - f1_score: 0.8245 - val_loss: 0.4822 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 559/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4257 - binary_accuracy: 0.8216 - f1_score: 0.8215 - val_loss: 0.4847 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 560/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4425 - binary_accuracy: 0.8115 - f1_score: 0.8113 - val_loss: 0.4832 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 561/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4425 - binary_accuracy: 0.8221 - f1_score: 0.8214 - val_loss: 0.4875 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 562/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4105 - binary_accuracy: 0.8347 - f1_score: 0.8344 - val_loss: 0.4838 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 563/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4299 - binary_accuracy: 0.8281 - f1_score: 0.8274 - val_loss: 0.4824 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 564/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4343 - binary_accuracy: 0.8185 - f1_score: 0.8184 - val_loss: 0.4779 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 565/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4139 - binary_accuracy: 0.8357 - f1_score: 0.8355 - val_loss: 0.4793 - val_binary_accuracy: 0.8153 - val_f1_score: 0.8107\n",
      "Epoch 566/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4266 - binary_accuracy: 0.8216 - f1_score: 0.8212 - val_loss: 0.4775 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 567/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4282 - binary_accuracy: 0.8226 - f1_score: 0.8225 - val_loss: 0.4777 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 568/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4454 - binary_accuracy: 0.8125 - f1_score: 0.8113 - val_loss: 0.4812 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 569/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4172 - binary_accuracy: 0.8417 - f1_score: 0.8416 - val_loss: 0.4839 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 570/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4459 - binary_accuracy: 0.8165 - f1_score: 0.8164 - val_loss: 0.4808 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 571/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4511 - binary_accuracy: 0.8080 - f1_score: 0.8078 - val_loss: 0.4789 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 572/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4541 - binary_accuracy: 0.8115 - f1_score: 0.8110 - val_loss: 0.4761 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 573/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4302 - binary_accuracy: 0.8332 - f1_score: 0.8336 - val_loss: 0.4776 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 574/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4323 - binary_accuracy: 0.8170 - f1_score: 0.8172 - val_loss: 0.4771 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 575/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4387 - binary_accuracy: 0.8160 - f1_score: 0.8153 - val_loss: 0.4790 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 576/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4270 - binary_accuracy: 0.8301 - f1_score: 0.8295 - val_loss: 0.4787 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 577/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4338 - binary_accuracy: 0.8216 - f1_score: 0.8225 - val_loss: 0.4773 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 578/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4260 - binary_accuracy: 0.8236 - f1_score: 0.8233 - val_loss: 0.4778 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 579/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4335 - binary_accuracy: 0.8317 - f1_score: 0.8314 - val_loss: 0.4765 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 580/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4420 - binary_accuracy: 0.8085 - f1_score: 0.8082 - val_loss: 0.4811 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 581/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4495 - binary_accuracy: 0.8135 - f1_score: 0.8131 - val_loss: 0.4780 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 582/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4444 - binary_accuracy: 0.8196 - f1_score: 0.8204 - val_loss: 0.4773 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 583/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4289 - binary_accuracy: 0.8160 - f1_score: 0.8152 - val_loss: 0.4790 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 584/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4551 - binary_accuracy: 0.8075 - f1_score: 0.8071 - val_loss: 0.4789 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 585/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4424 - binary_accuracy: 0.8185 - f1_score: 0.8184 - val_loss: 0.4845 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 586/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4272 - binary_accuracy: 0.8286 - f1_score: 0.8285 - val_loss: 0.4800 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 587/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4508 - binary_accuracy: 0.8140 - f1_score: 0.8145 - val_loss: 0.4805 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 588/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4086 - binary_accuracy: 0.8417 - f1_score: 0.8417 - val_loss: 0.4799 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 589/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4169 - binary_accuracy: 0.8337 - f1_score: 0.8334 - val_loss: 0.4824 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 590/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4266 - binary_accuracy: 0.8261 - f1_score: 0.8252 - val_loss: 0.4829 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 591/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4469 - binary_accuracy: 0.8054 - f1_score: 0.8051 - val_loss: 0.4792 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 592/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4336 - binary_accuracy: 0.8266 - f1_score: 0.8253 - val_loss: 0.4830 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7925\n",
      "Epoch 593/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4150 - binary_accuracy: 0.8342 - f1_score: 0.8336 - val_loss: 0.4841 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7925\n",
      "Epoch 594/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4345 - binary_accuracy: 0.8201 - f1_score: 0.8193 - val_loss: 0.4838 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 595/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4482 - binary_accuracy: 0.8095 - f1_score: 0.8090 - val_loss: 0.4807 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 596/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4080 - binary_accuracy: 0.8327 - f1_score: 0.8326 - val_loss: 0.4856 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 597/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4330 - binary_accuracy: 0.8226 - f1_score: 0.8223 - val_loss: 0.4836 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 598/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4491 - binary_accuracy: 0.8110 - f1_score: 0.8103 - val_loss: 0.4854 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 599/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4370 - binary_accuracy: 0.8196 - f1_score: 0.8194 - val_loss: 0.4815 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 600/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4315 - binary_accuracy: 0.8170 - f1_score: 0.8173 - val_loss: 0.4799 - val_binary_accuracy: 0.8153 - val_f1_score: 0.8107\n",
      "Epoch 601/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4278 - binary_accuracy: 0.8256 - f1_score: 0.8253 - val_loss: 0.4810 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 602/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4270 - binary_accuracy: 0.8170 - f1_score: 0.8175 - val_loss: 0.4855 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 603/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4352 - binary_accuracy: 0.8150 - f1_score: 0.8154 - val_loss: 0.4850 - val_binary_accuracy: 0.8063 - val_f1_score: 0.8017\n",
      "Epoch 604/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4400 - binary_accuracy: 0.8095 - f1_score: 0.8091 - val_loss: 0.4838 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 605/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4142 - binary_accuracy: 0.8352 - f1_score: 0.8355 - val_loss: 0.4848 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 606/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4348 - binary_accuracy: 0.8226 - f1_score: 0.8222 - val_loss: 0.4806 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 607/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4277 - binary_accuracy: 0.8256 - f1_score: 0.8254 - val_loss: 0.4816 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 608/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4334 - binary_accuracy: 0.8246 - f1_score: 0.8244 - val_loss: 0.4798 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 609/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4143 - binary_accuracy: 0.8347 - f1_score: 0.8335 - val_loss: 0.4810 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 610/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4401 - binary_accuracy: 0.8125 - f1_score: 0.8111 - val_loss: 0.4807 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 611/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4081 - binary_accuracy: 0.8337 - f1_score: 0.8336 - val_loss: 0.4817 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 612/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4566 - binary_accuracy: 0.8085 - f1_score: 0.8080 - val_loss: 0.4810 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 613/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4478 - binary_accuracy: 0.8075 - f1_score: 0.8069 - val_loss: 0.4794 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 614/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4385 - binary_accuracy: 0.8165 - f1_score: 0.8163 - val_loss: 0.4795 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 615/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4507 - binary_accuracy: 0.8135 - f1_score: 0.8142 - val_loss: 0.4819 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 616/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4269 - binary_accuracy: 0.8165 - f1_score: 0.8160 - val_loss: 0.4792 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 617/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4375 - binary_accuracy: 0.8231 - f1_score: 0.8235 - val_loss: 0.4782 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 618/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4058 - binary_accuracy: 0.8392 - f1_score: 0.8385 - val_loss: 0.4773 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 619/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4440 - binary_accuracy: 0.8155 - f1_score: 0.8143 - val_loss: 0.4807 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 620/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3950 - binary_accuracy: 0.8468 - f1_score: 0.8467 - val_loss: 0.4830 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 621/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4271 - binary_accuracy: 0.8246 - f1_score: 0.8243 - val_loss: 0.4823 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 622/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4274 - binary_accuracy: 0.8145 - f1_score: 0.8152 - val_loss: 0.4801 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 623/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4412 - binary_accuracy: 0.8125 - f1_score: 0.8111 - val_loss: 0.4809 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 624/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4287 - binary_accuracy: 0.8211 - f1_score: 0.8202 - val_loss: 0.4791 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 625/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4195 - binary_accuracy: 0.8402 - f1_score: 0.8406 - val_loss: 0.4753 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 626/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4444 - binary_accuracy: 0.8155 - f1_score: 0.8152 - val_loss: 0.4794 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 627/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4255 - binary_accuracy: 0.8347 - f1_score: 0.8345 - val_loss: 0.4786 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 628/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4212 - binary_accuracy: 0.8367 - f1_score: 0.8366 - val_loss: 0.4792 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 629/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4332 - binary_accuracy: 0.8276 - f1_score: 0.8275 - val_loss: 0.4801 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 630/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4497 - binary_accuracy: 0.8130 - f1_score: 0.8123 - val_loss: 0.4795 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 631/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4109 - binary_accuracy: 0.8291 - f1_score: 0.8295 - val_loss: 0.4784 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 632/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4459 - binary_accuracy: 0.8160 - f1_score: 0.8162 - val_loss: 0.4777 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 633/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4452 - binary_accuracy: 0.8180 - f1_score: 0.8185 - val_loss: 0.4814 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 634/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4299 - binary_accuracy: 0.8175 - f1_score: 0.8171 - val_loss: 0.4799 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 635/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4376 - binary_accuracy: 0.8145 - f1_score: 0.8142 - val_loss: 0.4797 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 636/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4519 - binary_accuracy: 0.8044 - f1_score: 0.8041 - val_loss: 0.4800 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 637/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4418 - binary_accuracy: 0.8266 - f1_score: 0.8262 - val_loss: 0.4785 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 638/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4167 - binary_accuracy: 0.8417 - f1_score: 0.8417 - val_loss: 0.4787 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 639/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4342 - binary_accuracy: 0.8140 - f1_score: 0.8131 - val_loss: 0.4798 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 640/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4314 - binary_accuracy: 0.8246 - f1_score: 0.8234 - val_loss: 0.4796 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 641/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4270 - binary_accuracy: 0.8226 - f1_score: 0.8224 - val_loss: 0.4809 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 642/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4408 - binary_accuracy: 0.8155 - f1_score: 0.8152 - val_loss: 0.4803 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 643/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4481 - binary_accuracy: 0.8080 - f1_score: 0.8082 - val_loss: 0.4788 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 644/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4418 - binary_accuracy: 0.8286 - f1_score: 0.8285 - val_loss: 0.4791 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 645/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4314 - binary_accuracy: 0.8226 - f1_score: 0.8223 - val_loss: 0.4793 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 646/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4597 - binary_accuracy: 0.8029 - f1_score: 0.8029 - val_loss: 0.4823 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 647/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4251 - binary_accuracy: 0.8246 - f1_score: 0.8244 - val_loss: 0.4809 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 648/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4186 - binary_accuracy: 0.8306 - f1_score: 0.8304 - val_loss: 0.4800 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 649/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4308 - binary_accuracy: 0.8286 - f1_score: 0.8285 - val_loss: 0.4779 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 650/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4377 - binary_accuracy: 0.8276 - f1_score: 0.8274 - val_loss: 0.4764 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 651/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4423 - binary_accuracy: 0.8065 - f1_score: 0.8060 - val_loss: 0.4783 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 652/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4070 - binary_accuracy: 0.8357 - f1_score: 0.8355 - val_loss: 0.4807 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 653/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4366 - binary_accuracy: 0.8175 - f1_score: 0.8174 - val_loss: 0.4793 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 654/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4401 - binary_accuracy: 0.8211 - f1_score: 0.8204 - val_loss: 0.4791 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 655/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4323 - binary_accuracy: 0.8165 - f1_score: 0.8163 - val_loss: 0.4794 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 656/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4579 - binary_accuracy: 0.8165 - f1_score: 0.8164 - val_loss: 0.4788 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 657/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4534 - binary_accuracy: 0.8034 - f1_score: 0.8019 - val_loss: 0.4826 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 658/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4225 - binary_accuracy: 0.8317 - f1_score: 0.8316 - val_loss: 0.4832 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 659/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4348 - binary_accuracy: 0.8226 - f1_score: 0.8224 - val_loss: 0.4830 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 660/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4198 - binary_accuracy: 0.8347 - f1_score: 0.8346 - val_loss: 0.4820 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 661/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4159 - binary_accuracy: 0.8296 - f1_score: 0.8292 - val_loss: 0.4769 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 662/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4522 - binary_accuracy: 0.8170 - f1_score: 0.8173 - val_loss: 0.4785 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 663/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4449 - binary_accuracy: 0.8196 - f1_score: 0.8193 - val_loss: 0.4797 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 664/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4327 - binary_accuracy: 0.8185 - f1_score: 0.8173 - val_loss: 0.4775 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 665/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4329 - binary_accuracy: 0.8246 - f1_score: 0.8243 - val_loss: 0.4780 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 666/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4163 - binary_accuracy: 0.8332 - f1_score: 0.8335 - val_loss: 0.4813 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 667/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4270 - binary_accuracy: 0.8266 - f1_score: 0.8264 - val_loss: 0.4823 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 668/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4252 - binary_accuracy: 0.8327 - f1_score: 0.8324 - val_loss: 0.4812 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 669/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4520 - binary_accuracy: 0.8044 - f1_score: 0.8039 - val_loss: 0.4795 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 670/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4194 - binary_accuracy: 0.8367 - f1_score: 0.8366 - val_loss: 0.4793 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 671/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4180 - binary_accuracy: 0.8301 - f1_score: 0.8305 - val_loss: 0.4781 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 672/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4085 - binary_accuracy: 0.8392 - f1_score: 0.8386 - val_loss: 0.4785 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 673/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4409 - binary_accuracy: 0.8226 - f1_score: 0.8224 - val_loss: 0.4792 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 674/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4425 - binary_accuracy: 0.8135 - f1_score: 0.8132 - val_loss: 0.4787 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 675/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4536 - binary_accuracy: 0.8054 - f1_score: 0.8050 - val_loss: 0.4768 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 676/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4371 - binary_accuracy: 0.8185 - f1_score: 0.8182 - val_loss: 0.4780 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 677/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4213 - binary_accuracy: 0.8206 - f1_score: 0.8192 - val_loss: 0.4774 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 678/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4364 - binary_accuracy: 0.8145 - f1_score: 0.8143 - val_loss: 0.4776 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 679/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4065 - binary_accuracy: 0.8387 - f1_score: 0.8385 - val_loss: 0.4796 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 680/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4371 - binary_accuracy: 0.8165 - f1_score: 0.8162 - val_loss: 0.4792 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 681/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4092 - binary_accuracy: 0.8322 - f1_score: 0.8315 - val_loss: 0.4776 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 682/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4382 - binary_accuracy: 0.8180 - f1_score: 0.8183 - val_loss: 0.4787 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 683/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3979 - binary_accuracy: 0.8407 - f1_score: 0.8407 - val_loss: 0.4803 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 684/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4209 - binary_accuracy: 0.8236 - f1_score: 0.8230 - val_loss: 0.4812 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 685/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4266 - binary_accuracy: 0.8246 - f1_score: 0.8244 - val_loss: 0.4821 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 686/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4497 - binary_accuracy: 0.8180 - f1_score: 0.8183 - val_loss: 0.4801 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 687/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4292 - binary_accuracy: 0.8276 - f1_score: 0.8274 - val_loss: 0.4789 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 688/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4407 - binary_accuracy: 0.8135 - f1_score: 0.8130 - val_loss: 0.4794 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 689/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4387 - binary_accuracy: 0.8120 - f1_score: 0.8123 - val_loss: 0.4818 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 690/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4322 - binary_accuracy: 0.8165 - f1_score: 0.8164 - val_loss: 0.4786 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 691/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4193 - binary_accuracy: 0.8362 - f1_score: 0.8356 - val_loss: 0.4825 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 692/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4124 - binary_accuracy: 0.8317 - f1_score: 0.8314 - val_loss: 0.4806 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 693/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4150 - binary_accuracy: 0.8211 - f1_score: 0.8214 - val_loss: 0.4806 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 694/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4154 - binary_accuracy: 0.8427 - f1_score: 0.8426 - val_loss: 0.4790 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 695/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4167 - binary_accuracy: 0.8407 - f1_score: 0.8406 - val_loss: 0.4813 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 696/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4339 - binary_accuracy: 0.8256 - f1_score: 0.8255 - val_loss: 0.4809 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 697/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4186 - binary_accuracy: 0.8337 - f1_score: 0.8335 - val_loss: 0.4832 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 698/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4278 - binary_accuracy: 0.8221 - f1_score: 0.8223 - val_loss: 0.4815 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 699/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4383 - binary_accuracy: 0.8286 - f1_score: 0.8296 - val_loss: 0.4812 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 700/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4174 - binary_accuracy: 0.8276 - f1_score: 0.8285 - val_loss: 0.4795 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 701/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4390 - binary_accuracy: 0.8256 - f1_score: 0.8255 - val_loss: 0.4796 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 702/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4093 - binary_accuracy: 0.8397 - f1_score: 0.8395 - val_loss: 0.4800 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 703/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4255 - binary_accuracy: 0.8256 - f1_score: 0.8254 - val_loss: 0.4797 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 704/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4402 - binary_accuracy: 0.8216 - f1_score: 0.8214 - val_loss: 0.4818 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 705/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4214 - binary_accuracy: 0.8332 - f1_score: 0.8324 - val_loss: 0.4801 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 706/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4419 - binary_accuracy: 0.8105 - f1_score: 0.8101 - val_loss: 0.4796 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 707/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4261 - binary_accuracy: 0.8397 - f1_score: 0.8397 - val_loss: 0.4798 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 708/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4232 - binary_accuracy: 0.8251 - f1_score: 0.8255 - val_loss: 0.4801 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 709/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4349 - binary_accuracy: 0.8226 - f1_score: 0.8223 - val_loss: 0.4803 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 710/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4223 - binary_accuracy: 0.8337 - f1_score: 0.8335 - val_loss: 0.4785 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 711/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4403 - binary_accuracy: 0.8145 - f1_score: 0.8142 - val_loss: 0.4803 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 712/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4239 - binary_accuracy: 0.8191 - f1_score: 0.8182 - val_loss: 0.4810 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 713/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4308 - binary_accuracy: 0.8251 - f1_score: 0.8255 - val_loss: 0.4805 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 714/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4019 - binary_accuracy: 0.8246 - f1_score: 0.8244 - val_loss: 0.4810 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 715/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4180 - binary_accuracy: 0.8347 - f1_score: 0.8345 - val_loss: 0.4805 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 716/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4406 - binary_accuracy: 0.8175 - f1_score: 0.8173 - val_loss: 0.4800 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 717/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4199 - binary_accuracy: 0.8332 - f1_score: 0.8325 - val_loss: 0.4840 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7925\n",
      "Epoch 718/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4234 - binary_accuracy: 0.8327 - f1_score: 0.8325 - val_loss: 0.4822 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 719/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4267 - binary_accuracy: 0.8211 - f1_score: 0.8204 - val_loss: 0.4811 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 720/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4358 - binary_accuracy: 0.8216 - f1_score: 0.8212 - val_loss: 0.4823 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 721/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3982 - binary_accuracy: 0.8412 - f1_score: 0.8405 - val_loss: 0.4800 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 722/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4218 - binary_accuracy: 0.8281 - f1_score: 0.8285 - val_loss: 0.4810 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 723/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4192 - binary_accuracy: 0.8196 - f1_score: 0.8192 - val_loss: 0.4814 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 724/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4192 - binary_accuracy: 0.8332 - f1_score: 0.8335 - val_loss: 0.4833 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 725/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4367 - binary_accuracy: 0.8150 - f1_score: 0.8153 - val_loss: 0.4810 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 726/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4068 - binary_accuracy: 0.8387 - f1_score: 0.8386 - val_loss: 0.4801 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 727/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4228 - binary_accuracy: 0.8317 - f1_score: 0.8315 - val_loss: 0.4812 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 728/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4284 - binary_accuracy: 0.8236 - f1_score: 0.8234 - val_loss: 0.4816 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7925\n",
      "Epoch 729/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4408 - binary_accuracy: 0.8175 - f1_score: 0.8174 - val_loss: 0.4818 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 730/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4123 - binary_accuracy: 0.8372 - f1_score: 0.8374 - val_loss: 0.4805 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 731/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4552 - binary_accuracy: 0.8075 - f1_score: 0.8072 - val_loss: 0.4802 - val_binary_accuracy: 0.8153 - val_f1_score: 0.8107\n",
      "Epoch 732/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4103 - binary_accuracy: 0.8412 - f1_score: 0.8415 - val_loss: 0.4812 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 733/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4433 - binary_accuracy: 0.8090 - f1_score: 0.8082 - val_loss: 0.4823 - val_binary_accuracy: 0.8063 - val_f1_score: 0.8107\n",
      "Epoch 734/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4223 - binary_accuracy: 0.8291 - f1_score: 0.8280 - val_loss: 0.4823 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 735/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4404 - binary_accuracy: 0.8135 - f1_score: 0.8131 - val_loss: 0.4816 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 736/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4176 - binary_accuracy: 0.8362 - f1_score: 0.8355 - val_loss: 0.4805 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 737/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4424 - binary_accuracy: 0.8145 - f1_score: 0.8142 - val_loss: 0.4823 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 738/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4207 - binary_accuracy: 0.8256 - f1_score: 0.8254 - val_loss: 0.4822 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 739/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4337 - binary_accuracy: 0.8196 - f1_score: 0.8193 - val_loss: 0.4810 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 740/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4451 - binary_accuracy: 0.8145 - f1_score: 0.8132 - val_loss: 0.4808 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 741/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4411 - binary_accuracy: 0.8135 - f1_score: 0.8133 - val_loss: 0.4780 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 742/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4243 - binary_accuracy: 0.8276 - f1_score: 0.8274 - val_loss: 0.4780 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 743/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4366 - binary_accuracy: 0.8196 - f1_score: 0.8193 - val_loss: 0.4797 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 744/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4363 - binary_accuracy: 0.8216 - f1_score: 0.8212 - val_loss: 0.4794 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 745/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4362 - binary_accuracy: 0.8170 - f1_score: 0.8163 - val_loss: 0.4795 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 746/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4379 - binary_accuracy: 0.8135 - f1_score: 0.8131 - val_loss: 0.4796 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 747/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4278 - binary_accuracy: 0.8196 - f1_score: 0.8192 - val_loss: 0.4779 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 748/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4462 - binary_accuracy: 0.8155 - f1_score: 0.8153 - val_loss: 0.4800 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 749/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4305 - binary_accuracy: 0.8185 - f1_score: 0.8184 - val_loss: 0.4804 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 750/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4460 - binary_accuracy: 0.8191 - f1_score: 0.8193 - val_loss: 0.4788 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 751/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4187 - binary_accuracy: 0.8367 - f1_score: 0.8366 - val_loss: 0.4798 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 752/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4161 - binary_accuracy: 0.8327 - f1_score: 0.8324 - val_loss: 0.4786 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 753/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4088 - binary_accuracy: 0.8286 - f1_score: 0.8286 - val_loss: 0.4815 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 754/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4263 - binary_accuracy: 0.8216 - f1_score: 0.8213 - val_loss: 0.4806 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 755/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4223 - binary_accuracy: 0.8317 - f1_score: 0.8316 - val_loss: 0.4817 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 756/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4374 - binary_accuracy: 0.8226 - f1_score: 0.8214 - val_loss: 0.4821 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 757/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4199 - binary_accuracy: 0.8448 - f1_score: 0.8447 - val_loss: 0.4828 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 758/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4321 - binary_accuracy: 0.8226 - f1_score: 0.8225 - val_loss: 0.4825 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 759/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4021 - binary_accuracy: 0.8478 - f1_score: 0.8477 - val_loss: 0.4819 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 760/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4070 - binary_accuracy: 0.8327 - f1_score: 0.8335 - val_loss: 0.4808 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 761/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4415 - binary_accuracy: 0.8175 - f1_score: 0.8172 - val_loss: 0.4819 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 762/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4242 - binary_accuracy: 0.8327 - f1_score: 0.8324 - val_loss: 0.4821 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 763/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4273 - binary_accuracy: 0.8251 - f1_score: 0.8243 - val_loss: 0.4825 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 764/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4006 - binary_accuracy: 0.8417 - f1_score: 0.8416 - val_loss: 0.4837 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 765/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4196 - binary_accuracy: 0.8266 - f1_score: 0.8261 - val_loss: 0.4823 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 766/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4213 - binary_accuracy: 0.8271 - f1_score: 0.8274 - val_loss: 0.4824 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 767/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4124 - binary_accuracy: 0.8327 - f1_score: 0.8324 - val_loss: 0.4841 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 768/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4271 - binary_accuracy: 0.8266 - f1_score: 0.8265 - val_loss: 0.4833 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 769/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4366 - binary_accuracy: 0.8185 - f1_score: 0.8184 - val_loss: 0.4844 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 770/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4398 - binary_accuracy: 0.8170 - f1_score: 0.8174 - val_loss: 0.4845 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 771/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4305 - binary_accuracy: 0.8150 - f1_score: 0.8149 - val_loss: 0.4848 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 772/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4014 - binary_accuracy: 0.8382 - f1_score: 0.8376 - val_loss: 0.4826 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 773/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4229 - binary_accuracy: 0.8246 - f1_score: 0.8245 - val_loss: 0.4831 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 774/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4299 - binary_accuracy: 0.8196 - f1_score: 0.8194 - val_loss: 0.4823 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 775/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4407 - binary_accuracy: 0.8191 - f1_score: 0.8194 - val_loss: 0.4805 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 776/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4227 - binary_accuracy: 0.8226 - f1_score: 0.8225 - val_loss: 0.4824 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 777/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4159 - binary_accuracy: 0.8332 - f1_score: 0.8334 - val_loss: 0.4810 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 778/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4456 - binary_accuracy: 0.8090 - f1_score: 0.8081 - val_loss: 0.4817 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 779/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4240 - binary_accuracy: 0.8236 - f1_score: 0.8235 - val_loss: 0.4814 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 780/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4377 - binary_accuracy: 0.8231 - f1_score: 0.8223 - val_loss: 0.4803 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 781/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4443 - binary_accuracy: 0.8135 - f1_score: 0.8132 - val_loss: 0.4825 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 782/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4288 - binary_accuracy: 0.8221 - f1_score: 0.8213 - val_loss: 0.4826 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 783/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3975 - binary_accuracy: 0.8417 - f1_score: 0.8415 - val_loss: 0.4803 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 784/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4364 - binary_accuracy: 0.8155 - f1_score: 0.8153 - val_loss: 0.4813 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 785/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4233 - binary_accuracy: 0.8201 - f1_score: 0.8202 - val_loss: 0.4814 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 786/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4059 - binary_accuracy: 0.8367 - f1_score: 0.8366 - val_loss: 0.4844 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 787/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4350 - binary_accuracy: 0.8196 - f1_score: 0.8193 - val_loss: 0.4840 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 788/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4301 - binary_accuracy: 0.8226 - f1_score: 0.8223 - val_loss: 0.4829 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 789/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4248 - binary_accuracy: 0.8271 - f1_score: 0.8263 - val_loss: 0.4868 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7833\n",
      "Epoch 790/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4492 - binary_accuracy: 0.8105 - f1_score: 0.8100 - val_loss: 0.4861 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 791/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4259 - binary_accuracy: 0.8362 - f1_score: 0.8364 - val_loss: 0.4815 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 792/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4345 - binary_accuracy: 0.8296 - f1_score: 0.8295 - val_loss: 0.4801 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 793/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4181 - binary_accuracy: 0.8256 - f1_score: 0.8254 - val_loss: 0.4815 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 794/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3957 - binary_accuracy: 0.8473 - f1_score: 0.8476 - val_loss: 0.4810 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 795/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4367 - binary_accuracy: 0.8286 - f1_score: 0.8285 - val_loss: 0.4815 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 796/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4345 - binary_accuracy: 0.8246 - f1_score: 0.8244 - val_loss: 0.4814 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 797/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4352 - binary_accuracy: 0.8236 - f1_score: 0.8233 - val_loss: 0.4822 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 798/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4236 - binary_accuracy: 0.8286 - f1_score: 0.8284 - val_loss: 0.4805 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 799/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4142 - binary_accuracy: 0.8337 - f1_score: 0.8335 - val_loss: 0.4808 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 800/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4222 - binary_accuracy: 0.8236 - f1_score: 0.8234 - val_loss: 0.4822 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 801/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4152 - binary_accuracy: 0.8367 - f1_score: 0.8365 - val_loss: 0.4819 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 802/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4237 - binary_accuracy: 0.8397 - f1_score: 0.8396 - val_loss: 0.4806 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 803/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4376 - binary_accuracy: 0.8216 - f1_score: 0.8213 - val_loss: 0.4837 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 804/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4026 - binary_accuracy: 0.8453 - f1_score: 0.8446 - val_loss: 0.4836 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 805/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4469 - binary_accuracy: 0.8175 - f1_score: 0.8172 - val_loss: 0.4824 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 806/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3928 - binary_accuracy: 0.8528 - f1_score: 0.8528 - val_loss: 0.4845 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 807/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4244 - binary_accuracy: 0.8256 - f1_score: 0.8252 - val_loss: 0.4843 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 808/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4050 - binary_accuracy: 0.8382 - f1_score: 0.8386 - val_loss: 0.4828 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 809/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4181 - binary_accuracy: 0.8296 - f1_score: 0.8295 - val_loss: 0.4833 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 810/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4289 - binary_accuracy: 0.8306 - f1_score: 0.8305 - val_loss: 0.4855 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 811/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4129 - binary_accuracy: 0.8286 - f1_score: 0.8283 - val_loss: 0.4839 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 812/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4253 - binary_accuracy: 0.8291 - f1_score: 0.8285 - val_loss: 0.4836 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 813/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4320 - binary_accuracy: 0.8251 - f1_score: 0.8243 - val_loss: 0.4842 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 814/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4231 - binary_accuracy: 0.8231 - f1_score: 0.8224 - val_loss: 0.4824 - val_binary_accuracy: 0.8153 - val_f1_score: 0.8198\n",
      "Epoch 815/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4089 - binary_accuracy: 0.8317 - f1_score: 0.8314 - val_loss: 0.4839 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 816/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4203 - binary_accuracy: 0.8317 - f1_score: 0.8304 - val_loss: 0.4847 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 817/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4157 - binary_accuracy: 0.8317 - f1_score: 0.8315 - val_loss: 0.4800 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 818/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4283 - binary_accuracy: 0.8271 - f1_score: 0.8274 - val_loss: 0.4795 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 819/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4184 - binary_accuracy: 0.8286 - f1_score: 0.8283 - val_loss: 0.4815 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 820/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4338 - binary_accuracy: 0.8226 - f1_score: 0.8225 - val_loss: 0.4813 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 821/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4218 - binary_accuracy: 0.8322 - f1_score: 0.8325 - val_loss: 0.4816 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 822/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4202 - binary_accuracy: 0.8412 - f1_score: 0.8416 - val_loss: 0.4828 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 823/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4231 - binary_accuracy: 0.8266 - f1_score: 0.8265 - val_loss: 0.4837 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 824/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4274 - binary_accuracy: 0.8211 - f1_score: 0.8202 - val_loss: 0.4831 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 825/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4101 - binary_accuracy: 0.8392 - f1_score: 0.8386 - val_loss: 0.4821 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 826/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4199 - binary_accuracy: 0.8327 - f1_score: 0.8326 - val_loss: 0.4819 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 827/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4439 - binary_accuracy: 0.8115 - f1_score: 0.8111 - val_loss: 0.4812 - val_binary_accuracy: 0.8153 - val_f1_score: 0.8198\n",
      "Epoch 828/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4190 - binary_accuracy: 0.8221 - f1_score: 0.8223 - val_loss: 0.4828 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 829/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4127 - binary_accuracy: 0.8317 - f1_score: 0.8315 - val_loss: 0.4831 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 830/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4344 - binary_accuracy: 0.8211 - f1_score: 0.8212 - val_loss: 0.4811 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 831/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4050 - binary_accuracy: 0.8392 - f1_score: 0.8396 - val_loss: 0.4812 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 832/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4105 - binary_accuracy: 0.8397 - f1_score: 0.8396 - val_loss: 0.4804 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 833/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4282 - binary_accuracy: 0.8256 - f1_score: 0.8255 - val_loss: 0.4800 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 834/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4335 - binary_accuracy: 0.8140 - f1_score: 0.8132 - val_loss: 0.4816 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 835/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4229 - binary_accuracy: 0.8276 - f1_score: 0.8275 - val_loss: 0.4823 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 836/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4331 - binary_accuracy: 0.8342 - f1_score: 0.8346 - val_loss: 0.4840 - val_binary_accuracy: 0.8063 - val_f1_score: 0.8107\n",
      "Epoch 837/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4204 - binary_accuracy: 0.8286 - f1_score: 0.8293 - val_loss: 0.4824 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 838/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4289 - binary_accuracy: 0.8216 - f1_score: 0.8212 - val_loss: 0.4813 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 839/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4249 - binary_accuracy: 0.8296 - f1_score: 0.8296 - val_loss: 0.4827 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 840/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4378 - binary_accuracy: 0.8145 - f1_score: 0.8148 - val_loss: 0.4829 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 841/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4366 - binary_accuracy: 0.8185 - f1_score: 0.8182 - val_loss: 0.4816 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 842/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4020 - binary_accuracy: 0.8412 - f1_score: 0.8415 - val_loss: 0.4822 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 843/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4144 - binary_accuracy: 0.8266 - f1_score: 0.8262 - val_loss: 0.4811 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 844/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4171 - binary_accuracy: 0.8251 - f1_score: 0.8245 - val_loss: 0.4808 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 845/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4084 - binary_accuracy: 0.8397 - f1_score: 0.8396 - val_loss: 0.4802 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 846/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4336 - binary_accuracy: 0.8276 - f1_score: 0.8275 - val_loss: 0.4814 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 847/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4189 - binary_accuracy: 0.8367 - f1_score: 0.8367 - val_loss: 0.4809 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 848/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4339 - binary_accuracy: 0.8261 - f1_score: 0.8265 - val_loss: 0.4841 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 849/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4033 - binary_accuracy: 0.8412 - f1_score: 0.8405 - val_loss: 0.4833 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 850/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4059 - binary_accuracy: 0.8387 - f1_score: 0.8376 - val_loss: 0.4824 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 851/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4318 - binary_accuracy: 0.8155 - f1_score: 0.8152 - val_loss: 0.4848 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 852/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4301 - binary_accuracy: 0.8130 - f1_score: 0.8132 - val_loss: 0.4819 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 853/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4252 - binary_accuracy: 0.8271 - f1_score: 0.8264 - val_loss: 0.4798 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 854/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4157 - binary_accuracy: 0.8347 - f1_score: 0.8356 - val_loss: 0.4820 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 855/1000\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.4347 - binary_accuracy: 0.8211 - f1_score: 0.8202 - val_loss: 0.4833 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 856/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4389 - binary_accuracy: 0.8165 - f1_score: 0.8163 - val_loss: 0.4835 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 857/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4482 - binary_accuracy: 0.8115 - f1_score: 0.8118 - val_loss: 0.4819 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 858/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4231 - binary_accuracy: 0.8251 - f1_score: 0.8253 - val_loss: 0.4819 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 859/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4347 - binary_accuracy: 0.8196 - f1_score: 0.8203 - val_loss: 0.4826 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 860/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4423 - binary_accuracy: 0.8155 - f1_score: 0.8153 - val_loss: 0.4836 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 861/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4135 - binary_accuracy: 0.8362 - f1_score: 0.8355 - val_loss: 0.4833 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8017\n",
      "Epoch 862/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4514 - binary_accuracy: 0.8110 - f1_score: 0.8113 - val_loss: 0.4831 - val_binary_accuracy: 0.8063 - val_f1_score: 0.8107\n",
      "Epoch 863/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3968 - binary_accuracy: 0.8458 - f1_score: 0.8457 - val_loss: 0.4808 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 864/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4493 - binary_accuracy: 0.8185 - f1_score: 0.8185 - val_loss: 0.4809 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 865/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4269 - binary_accuracy: 0.8196 - f1_score: 0.8192 - val_loss: 0.4809 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 866/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4296 - binary_accuracy: 0.8211 - f1_score: 0.8204 - val_loss: 0.4786 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 867/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4298 - binary_accuracy: 0.8231 - f1_score: 0.8223 - val_loss: 0.4780 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 868/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4404 - binary_accuracy: 0.8125 - f1_score: 0.8119 - val_loss: 0.4792 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 869/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4216 - binary_accuracy: 0.8347 - f1_score: 0.8345 - val_loss: 0.4800 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 870/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4331 - binary_accuracy: 0.8311 - f1_score: 0.8315 - val_loss: 0.4798 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 871/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4188 - binary_accuracy: 0.8296 - f1_score: 0.8294 - val_loss: 0.4803 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 872/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4164 - binary_accuracy: 0.8322 - f1_score: 0.8324 - val_loss: 0.4802 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 873/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4344 - binary_accuracy: 0.8221 - f1_score: 0.8215 - val_loss: 0.4805 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 874/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4203 - binary_accuracy: 0.8246 - f1_score: 0.8245 - val_loss: 0.4817 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 875/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4150 - binary_accuracy: 0.8322 - f1_score: 0.8325 - val_loss: 0.4812 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 876/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4224 - binary_accuracy: 0.8266 - f1_score: 0.8264 - val_loss: 0.4804 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 877/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4110 - binary_accuracy: 0.8332 - f1_score: 0.8323 - val_loss: 0.4809 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 878/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4227 - binary_accuracy: 0.8226 - f1_score: 0.8234 - val_loss: 0.4808 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 879/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4168 - binary_accuracy: 0.8387 - f1_score: 0.8387 - val_loss: 0.4807 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 880/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4401 - binary_accuracy: 0.8145 - f1_score: 0.8140 - val_loss: 0.4791 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 881/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3996 - binary_accuracy: 0.8432 - f1_score: 0.8436 - val_loss: 0.4799 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 882/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4453 - binary_accuracy: 0.8110 - f1_score: 0.8112 - val_loss: 0.4808 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 883/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4185 - binary_accuracy: 0.8352 - f1_score: 0.8345 - val_loss: 0.4812 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 884/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4190 - binary_accuracy: 0.8322 - f1_score: 0.8315 - val_loss: 0.4809 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 885/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4072 - binary_accuracy: 0.8448 - f1_score: 0.8446 - val_loss: 0.4791 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 886/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4115 - binary_accuracy: 0.8392 - f1_score: 0.8386 - val_loss: 0.4793 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 887/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4452 - binary_accuracy: 0.8160 - f1_score: 0.8153 - val_loss: 0.4786 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 888/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4316 - binary_accuracy: 0.8266 - f1_score: 0.8264 - val_loss: 0.4779 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 889/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4371 - binary_accuracy: 0.8196 - f1_score: 0.8195 - val_loss: 0.4788 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 890/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4176 - binary_accuracy: 0.8301 - f1_score: 0.8294 - val_loss: 0.4781 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 891/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4202 - binary_accuracy: 0.8246 - f1_score: 0.8242 - val_loss: 0.4790 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 892/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4399 - binary_accuracy: 0.8175 - f1_score: 0.8175 - val_loss: 0.4801 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 893/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4078 - binary_accuracy: 0.8367 - f1_score: 0.8365 - val_loss: 0.4803 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 894/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4324 - binary_accuracy: 0.8226 - f1_score: 0.8224 - val_loss: 0.4792 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 895/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4409 - binary_accuracy: 0.8155 - f1_score: 0.8153 - val_loss: 0.4796 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 896/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4343 - binary_accuracy: 0.8236 - f1_score: 0.8235 - val_loss: 0.4811 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 897/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4334 - binary_accuracy: 0.8256 - f1_score: 0.8254 - val_loss: 0.4805 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 898/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4181 - binary_accuracy: 0.8322 - f1_score: 0.8316 - val_loss: 0.4811 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 899/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4276 - binary_accuracy: 0.8216 - f1_score: 0.8214 - val_loss: 0.4798 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 900/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4383 - binary_accuracy: 0.8211 - f1_score: 0.8203 - val_loss: 0.4790 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 901/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4403 - binary_accuracy: 0.8135 - f1_score: 0.8132 - val_loss: 0.4796 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 902/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4037 - binary_accuracy: 0.8397 - f1_score: 0.8394 - val_loss: 0.4800 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 903/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4371 - binary_accuracy: 0.8256 - f1_score: 0.8263 - val_loss: 0.4795 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 904/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4172 - binary_accuracy: 0.8347 - f1_score: 0.8344 - val_loss: 0.4803 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 905/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4279 - binary_accuracy: 0.8337 - f1_score: 0.8334 - val_loss: 0.4799 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 906/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4298 - binary_accuracy: 0.8347 - f1_score: 0.8346 - val_loss: 0.4800 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 907/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4104 - binary_accuracy: 0.8291 - f1_score: 0.8285 - val_loss: 0.4799 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 908/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4318 - binary_accuracy: 0.8145 - f1_score: 0.8143 - val_loss: 0.4798 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 909/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4286 - binary_accuracy: 0.8236 - f1_score: 0.8234 - val_loss: 0.4805 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 910/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4197 - binary_accuracy: 0.8337 - f1_score: 0.8336 - val_loss: 0.4804 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 911/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4376 - binary_accuracy: 0.8145 - f1_score: 0.8153 - val_loss: 0.4804 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 912/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4384 - binary_accuracy: 0.8125 - f1_score: 0.8122 - val_loss: 0.4809 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 913/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4183 - binary_accuracy: 0.8311 - f1_score: 0.8315 - val_loss: 0.4802 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 914/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4154 - binary_accuracy: 0.8291 - f1_score: 0.8293 - val_loss: 0.4800 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 915/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4221 - binary_accuracy: 0.8322 - f1_score: 0.8325 - val_loss: 0.4799 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 916/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4168 - binary_accuracy: 0.8266 - f1_score: 0.8265 - val_loss: 0.4788 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 917/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4163 - binary_accuracy: 0.8286 - f1_score: 0.8283 - val_loss: 0.4796 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 918/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4216 - binary_accuracy: 0.8301 - f1_score: 0.8306 - val_loss: 0.4802 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 919/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4138 - binary_accuracy: 0.8347 - f1_score: 0.8345 - val_loss: 0.4799 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 920/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4246 - binary_accuracy: 0.8276 - f1_score: 0.8274 - val_loss: 0.4794 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 921/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.3940 - binary_accuracy: 0.8448 - f1_score: 0.8446 - val_loss: 0.4796 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 922/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4394 - binary_accuracy: 0.8241 - f1_score: 0.8245 - val_loss: 0.4799 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 923/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4030 - binary_accuracy: 0.8377 - f1_score: 0.8374 - val_loss: 0.4815 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 924/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4147 - binary_accuracy: 0.8256 - f1_score: 0.8253 - val_loss: 0.4794 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 925/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4282 - binary_accuracy: 0.8261 - f1_score: 0.8265 - val_loss: 0.4786 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 926/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4036 - binary_accuracy: 0.8387 - f1_score: 0.8386 - val_loss: 0.4802 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 927/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4180 - binary_accuracy: 0.8322 - f1_score: 0.8316 - val_loss: 0.4808 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 928/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4071 - binary_accuracy: 0.8362 - f1_score: 0.8365 - val_loss: 0.4809 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 929/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4398 - binary_accuracy: 0.8175 - f1_score: 0.8163 - val_loss: 0.4812 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 930/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4445 - binary_accuracy: 0.8165 - f1_score: 0.8163 - val_loss: 0.4808 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 931/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4471 - binary_accuracy: 0.8065 - f1_score: 0.8061 - val_loss: 0.4810 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 932/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4199 - binary_accuracy: 0.8256 - f1_score: 0.8253 - val_loss: 0.4806 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 933/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4221 - binary_accuracy: 0.8246 - f1_score: 0.8243 - val_loss: 0.4802 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 934/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4163 - binary_accuracy: 0.8387 - f1_score: 0.8386 - val_loss: 0.4806 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 935/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4302 - binary_accuracy: 0.8191 - f1_score: 0.8181 - val_loss: 0.4795 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 936/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4342 - binary_accuracy: 0.8231 - f1_score: 0.8234 - val_loss: 0.4794 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 937/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4259 - binary_accuracy: 0.8276 - f1_score: 0.8274 - val_loss: 0.4796 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 938/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4153 - binary_accuracy: 0.8337 - f1_score: 0.8336 - val_loss: 0.4800 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 939/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4041 - binary_accuracy: 0.8412 - f1_score: 0.8414 - val_loss: 0.4811 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 940/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4225 - binary_accuracy: 0.8337 - f1_score: 0.8336 - val_loss: 0.4820 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 941/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4271 - binary_accuracy: 0.8276 - f1_score: 0.8264 - val_loss: 0.4809 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 942/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4405 - binary_accuracy: 0.8105 - f1_score: 0.8102 - val_loss: 0.4808 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 943/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4426 - binary_accuracy: 0.8145 - f1_score: 0.8140 - val_loss: 0.4811 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 944/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4398 - binary_accuracy: 0.8196 - f1_score: 0.8194 - val_loss: 0.4809 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 945/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4286 - binary_accuracy: 0.8266 - f1_score: 0.8265 - val_loss: 0.4806 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 946/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4243 - binary_accuracy: 0.8201 - f1_score: 0.8192 - val_loss: 0.4806 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 947/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4326 - binary_accuracy: 0.8231 - f1_score: 0.8233 - val_loss: 0.4806 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 948/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4285 - binary_accuracy: 0.8216 - f1_score: 0.8213 - val_loss: 0.4823 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 949/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4272 - binary_accuracy: 0.8357 - f1_score: 0.8344 - val_loss: 0.4820 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 950/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4147 - binary_accuracy: 0.8347 - f1_score: 0.8346 - val_loss: 0.4819 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 951/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4062 - binary_accuracy: 0.8377 - f1_score: 0.8375 - val_loss: 0.4813 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 952/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4214 - binary_accuracy: 0.8357 - f1_score: 0.8356 - val_loss: 0.4813 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 953/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4258 - binary_accuracy: 0.8266 - f1_score: 0.8265 - val_loss: 0.4812 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 954/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4373 - binary_accuracy: 0.8206 - f1_score: 0.8205 - val_loss: 0.4823 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 955/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4197 - binary_accuracy: 0.8306 - f1_score: 0.8304 - val_loss: 0.4819 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 956/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4090 - binary_accuracy: 0.8387 - f1_score: 0.8387 - val_loss: 0.4813 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 957/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4196 - binary_accuracy: 0.8281 - f1_score: 0.8285 - val_loss: 0.4816 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 958/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4225 - binary_accuracy: 0.8236 - f1_score: 0.8234 - val_loss: 0.4819 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8108\n",
      "Epoch 959/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4386 - binary_accuracy: 0.8196 - f1_score: 0.8194 - val_loss: 0.4808 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 960/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4277 - binary_accuracy: 0.8291 - f1_score: 0.8294 - val_loss: 0.4810 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 961/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4097 - binary_accuracy: 0.8347 - f1_score: 0.8344 - val_loss: 0.4808 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 962/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4135 - binary_accuracy: 0.8362 - f1_score: 0.8353 - val_loss: 0.4808 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 963/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4431 - binary_accuracy: 0.8170 - f1_score: 0.8163 - val_loss: 0.4807 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 964/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4333 - binary_accuracy: 0.8231 - f1_score: 0.8232 - val_loss: 0.4804 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 965/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4294 - binary_accuracy: 0.8276 - f1_score: 0.8275 - val_loss: 0.4806 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 966/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4307 - binary_accuracy: 0.8170 - f1_score: 0.8172 - val_loss: 0.4804 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 967/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4489 - binary_accuracy: 0.8135 - f1_score: 0.8132 - val_loss: 0.4800 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 968/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4172 - binary_accuracy: 0.8276 - f1_score: 0.8265 - val_loss: 0.4805 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 969/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4134 - binary_accuracy: 0.8322 - f1_score: 0.8325 - val_loss: 0.4807 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 970/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4119 - binary_accuracy: 0.8317 - f1_score: 0.8315 - val_loss: 0.4808 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 971/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4257 - binary_accuracy: 0.8216 - f1_score: 0.8213 - val_loss: 0.4803 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 972/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4216 - binary_accuracy: 0.8276 - f1_score: 0.8274 - val_loss: 0.4793 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 973/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4248 - binary_accuracy: 0.8342 - f1_score: 0.8334 - val_loss: 0.4801 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 974/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4358 - binary_accuracy: 0.8236 - f1_score: 0.8235 - val_loss: 0.4801 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 975/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4334 - binary_accuracy: 0.8266 - f1_score: 0.8265 - val_loss: 0.4812 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 976/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4200 - binary_accuracy: 0.8306 - f1_score: 0.8304 - val_loss: 0.4817 - val_binary_accuracy: 0.8108 - val_f1_score: 0.8107\n",
      "Epoch 977/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4365 - binary_accuracy: 0.8175 - f1_score: 0.8173 - val_loss: 0.4815 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 978/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4206 - binary_accuracy: 0.8296 - f1_score: 0.8294 - val_loss: 0.4821 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 979/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4203 - binary_accuracy: 0.8306 - f1_score: 0.8304 - val_loss: 0.4815 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 980/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4260 - binary_accuracy: 0.8286 - f1_score: 0.8295 - val_loss: 0.4822 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 981/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4524 - binary_accuracy: 0.8145 - f1_score: 0.8144 - val_loss: 0.4829 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 982/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4112 - binary_accuracy: 0.8347 - f1_score: 0.8345 - val_loss: 0.4821 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 983/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4156 - binary_accuracy: 0.8276 - f1_score: 0.8274 - val_loss: 0.4817 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 984/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4028 - binary_accuracy: 0.8463 - f1_score: 0.8467 - val_loss: 0.4812 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 985/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4179 - binary_accuracy: 0.8276 - f1_score: 0.8273 - val_loss: 0.4818 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 986/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3990 - binary_accuracy: 0.8427 - f1_score: 0.8426 - val_loss: 0.4813 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 987/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4161 - binary_accuracy: 0.8332 - f1_score: 0.8335 - val_loss: 0.4813 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 988/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4212 - binary_accuracy: 0.8306 - f1_score: 0.8305 - val_loss: 0.4814 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 989/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4088 - binary_accuracy: 0.8438 - f1_score: 0.8426 - val_loss: 0.4817 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 990/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3993 - binary_accuracy: 0.8448 - f1_score: 0.8457 - val_loss: 0.4808 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 991/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4474 - binary_accuracy: 0.8135 - f1_score: 0.8132 - val_loss: 0.4815 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 992/1000\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.3966 - binary_accuracy: 0.8488 - f1_score: 0.8487 - val_loss: 0.4819 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 993/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4192 - binary_accuracy: 0.8256 - f1_score: 0.8252 - val_loss: 0.4806 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 994/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4165 - binary_accuracy: 0.8407 - f1_score: 0.8406 - val_loss: 0.4803 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 995/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4295 - binary_accuracy: 0.8342 - f1_score: 0.8334 - val_loss: 0.4809 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 996/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4159 - binary_accuracy: 0.8301 - f1_score: 0.8304 - val_loss: 0.4809 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 997/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4198 - binary_accuracy: 0.8256 - f1_score: 0.8253 - val_loss: 0.4806 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 998/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4203 - binary_accuracy: 0.8306 - f1_score: 0.8305 - val_loss: 0.4805 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 999/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4352 - binary_accuracy: 0.8211 - f1_score: 0.8203 - val_loss: 0.4803 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n",
      "Epoch 1000/1000\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4172 - binary_accuracy: 0.8317 - f1_score: 0.8315 - val_loss: 0.4802 - val_binary_accuracy: 0.8198 - val_f1_score: 0.8198\n"
     ]
    }
   ],
   "source": [
    "mlpOnly = create_mlp(X_train.shape[1],regression=True)\n",
    "\n",
    "plot_model(mlpOnly, to_file=plotpath / Path('MLP_only.png'), show_shapes=True, show_layer_names=True)\n",
    "\n",
    "\n",
    "initial_learning_rate = 1e-3  # migliore come accuracy massima  0.69\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=100, decay_rate=0.99, staircase=False)\n",
    "\n",
    "mlpOnly.compile(\n",
    "        # optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-2, decay=1e-4),   #FUSION 2022\n",
    "        optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "        # optimizer=tf.keras.optimizers.Adam(0.0011),\n",
    "        # optimizer = tf.keras.optimizers.Adam(1e-2),\n",
    "        # optimizer=tf.keras.optimizers.Adam(1.1e-3),\n",
    "        # loss=\"categorical_crossentropy\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        # loss=\"categorical_crossentropy\",\n",
    "        # metrics=[\"accuracy\",dice_coef,\"categorical_accuracy\"],\n",
    "        # metrics=[\"categorical_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=4)],\n",
    "        # metrics=[\"categorical_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=2)],\n",
    "        metrics=[\"binary_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=2)],\n",
    "        # metrics=[\"categorical_accuracy\"],\n",
    ")\n",
    "# CREATE CALLBACKS\n",
    "checkpointClassification = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        checkpointPath / Path('MLP_Classification_acc'),\n",
    "                        monitor='val_binary_accuracy', verbose=0, \n",
    "                        # monitor='val_dm_pignistic_16_categorical_accuracy', verbose=0, \n",
    "                        save_best_only=True, mode='max')\n",
    "                                                # CREATE CALLBACKS\n",
    "\n",
    "\n",
    "checkpointClassificationF1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        checkpointPath / Path('MLP_Classification_F1'), \n",
    "                        monitor='val_f1_score', verbose=0, \n",
    "                        save_best_only=True, mode='max')\n",
    "                                                # CREATE CALLBACKS\n",
    "\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "\n",
    "    \n",
    "# lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "\n",
    "cb = TimingCallback()\n",
    "\n",
    "print(checkpointPath / Path('MLP'))\n",
    "\n",
    "# callbacks_list = [checkpointClassification]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb, schedule_lr,lrate]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb, schedule_lr]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, cb, schedule_lr]\n",
    "callbacks_list = [checkpointClassification, checkpointClassificationF1, cb]\n",
    "# callbacks_list = [checkpointClassification, cb]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb]\n",
    "# callbacks_list = [checkpointClassificationF1, earlyStopping]\n",
    "\n",
    "history = mlpOnly.fit(\n",
    "        # x = X_train_IMGS,\n",
    "        # trainset,\n",
    "        x = X_train,\n",
    "        # y = [Y_train,Y_tr_unc],\n",
    "        y = Y_train,\n",
    "        # epochs = 1,\n",
    "        epochs = 1000,\n",
    "        validation_data = [X_val,Y_val],\n",
    "        # validation_data = [X_val_IMGS,[Y_val,Y_val_unc]],\n",
    "        # validation_data = validset,\n",
    "        callbacks = callbacks_list,    #some problems with DS layers and callbacks\n",
    "        batch_size=32,\n",
    "        # batch_size=1,\n",
    "        # class_weight=c_w,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train loss",
         "type": "scatter",
         "y": [
          0.6994985938072205,
          0.6947794556617737,
          0.6937928199768066,
          0.6909528970718384,
          0.6917517185211182,
          0.6930493712425232,
          0.6886584162712097,
          0.6836548447608948,
          0.6839052438735962,
          0.6847349405288696,
          0.6815566420555115,
          0.6661456227302551,
          0.6747413277626038,
          0.6743469834327698,
          0.6651312708854675,
          0.6620209217071533,
          0.6689750552177429,
          0.6535362601280212,
          0.6627389788627625,
          0.6507350206375122,
          0.6476776599884033,
          0.6556287407875061,
          0.6342871785163879,
          0.6400517225265503,
          0.643232524394989,
          0.6306139826774597,
          0.6193690896034241,
          0.5979208946228027,
          0.6273567080497742,
          0.6272701025009155,
          0.6375501751899719,
          0.618985652923584,
          0.6091541647911072,
          0.629805862903595,
          0.5937710404396057,
          0.59772127866745,
          0.6106256246566772,
          0.588299572467804,
          0.6108787059783936,
          0.6094304919242859,
          0.6112596988677979,
          0.5850045084953308,
          0.584805965423584,
          0.5951380729675293,
          0.610393226146698,
          0.5848790407180786,
          0.60910564661026,
          0.591447651386261,
          0.5978310704231262,
          0.5758926868438721,
          0.5885584950447083,
          0.5904391407966614,
          0.5871211290359497,
          0.5920101404190063,
          0.5730424523353577,
          0.5963441133499146,
          0.570202648639679,
          0.5828962922096252,
          0.5827733874320984,
          0.5757531523704529,
          0.5900984406471252,
          0.5708142518997192,
          0.5555891394615173,
          0.5623249411582947,
          0.5638208985328674,
          0.5802897810935974,
          0.5720832943916321,
          0.5706883668899536,
          0.564220666885376,
          0.5558968782424927,
          0.5777779817581177,
          0.5842232704162598,
          0.5600800514221191,
          0.5517221689224243,
          0.5404217839241028,
          0.5475084781646729,
          0.546180248260498,
          0.545904815196991,
          0.5449687242507935,
          0.5441046357154846,
          0.5565270185470581,
          0.5647338628768921,
          0.5615639686584473,
          0.5417240262031555,
          0.5531754493713379,
          0.5547863841056824,
          0.5400770902633667,
          0.5351943373680115,
          0.5408416986465454,
          0.5528957843780518,
          0.5746986269950867,
          0.5639032125473022,
          0.561374843120575,
          0.5522246360778809,
          0.5462087988853455,
          0.5438583493232727,
          0.544698178768158,
          0.5495588183403015,
          0.558734118938446,
          0.5434216260910034,
          0.5344393253326416,
          0.5182535648345947,
          0.5242074131965637,
          0.5191528797149658,
          0.556205153465271,
          0.5074051022529602,
          0.5405303239822388,
          0.5131739974021912,
          0.5412600040435791,
          0.5284814238548279,
          0.5240092873573303,
          0.5318161845207214,
          0.5455325245857239,
          0.5468965172767639,
          0.5514837503433228,
          0.549140453338623,
          0.5269048810005188,
          0.5016612410545349,
          0.5289415717124939,
          0.5316355228424072,
          0.5150344371795654,
          0.5348302125930786,
          0.5320090651512146,
          0.5327187180519104,
          0.5014552474021912,
          0.5270805358886719,
          0.5156323909759521,
          0.5268306732177734,
          0.5127588510513306,
          0.5493932962417603,
          0.5245200395584106,
          0.4836645722389221,
          0.5037781000137329,
          0.5206822156906128,
          0.4928780794143677,
          0.5323256850242615,
          0.5071361660957336,
          0.5092657208442688,
          0.5186649560928345,
          0.5118809938430786,
          0.5033496618270874,
          0.5207809209823608,
          0.4882519543170929,
          0.5296539068222046,
          0.5152824521064758,
          0.5029375553131104,
          0.5127014517784119,
          0.5047286748886108,
          0.5021437406539917,
          0.5114320516586304,
          0.4974888563156128,
          0.4756482243537903,
          0.5296474695205688,
          0.493567556142807,
          0.5171513557434082,
          0.4842590093612671,
          0.4972497224807739,
          0.5086804628372192,
          0.5000757575035095,
          0.4832204580307007,
          0.5051334500312805,
          0.5037826299667358,
          0.49248966574668884,
          0.47987663745880127,
          0.4917556941509247,
          0.5230180621147156,
          0.4786246418952942,
          0.47873803973197937,
          0.5072050094604492,
          0.48912060260772705,
          0.47248202562332153,
          0.4771985709667206,
          0.49172481894493103,
          0.49504902958869934,
          0.49059826135635376,
          0.47949519753456116,
          0.46890875697135925,
          0.4830356538295746,
          0.4797402322292328,
          0.496797651052475,
          0.487320214509964,
          0.5169631838798523,
          0.4881840944290161,
          0.48059725761413574,
          0.4708375930786133,
          0.4820900559425354,
          0.5015906691551208,
          0.4910339415073395,
          0.4845148026943207,
          0.49798253178596497,
          0.48945295810699463,
          0.4837149977684021,
          0.4931841492652893,
          0.48164448142051697,
          0.4892685115337372,
          0.4786738157272339,
          0.4890292286872864,
          0.4720011055469513,
          0.47870343923568726,
          0.46484050154685974,
          0.48652535676956177,
          0.48617714643478394,
          0.4785086214542389,
          0.4968411326408386,
          0.49620649218559265,
          0.48388633131980896,
          0.49507442116737366,
          0.4787273108959198,
          0.4948080778121948,
          0.47304725646972656,
          0.4791181981563568,
          0.4815099835395813,
          0.4746708273887634,
          0.5033318996429443,
          0.48847439885139465,
          0.4757627546787262,
          0.48136356472969055,
          0.47191566228866577,
          0.46760523319244385,
          0.4845915734767914,
          0.49785736203193665,
          0.4970795810222626,
          0.49631327390670776,
          0.4668232798576355,
          0.46999672055244446,
          0.5011513233184814,
          0.44992372393608093,
          0.4708505868911743,
          0.4698496460914612,
          0.4968622326850891,
          0.4640612304210663,
          0.470948189496994,
          0.4849074184894562,
          0.44802549481391907,
          0.4753859043121338,
          0.4833415448665619,
          0.47016698122024536,
          0.4913637936115265,
          0.4709976613521576,
          0.4690437912940979,
          0.46196502447128296,
          0.4870435297489166,
          0.46039044857025146,
          0.48563554883003235,
          0.5089937448501587,
          0.477434903383255,
          0.47545090317726135,
          0.46860554814338684,
          0.47477972507476807,
          0.4870130717754364,
          0.441697895526886,
          0.48674124479293823,
          0.4500191807746887,
          0.49397093057632446,
          0.44546422362327576,
          0.47810062766075134,
          0.4765755534172058,
          0.4681266248226166,
          0.4720711410045624,
          0.4841192364692688,
          0.47516608238220215,
          0.46947363018989563,
          0.4707023799419403,
          0.4581182301044464,
          0.4898607134819031,
          0.4875175952911377,
          0.46518200635910034,
          0.4602822959423065,
          0.46503838896751404,
          0.4619513154029846,
          0.4802028238773346,
          0.46368059515953064,
          0.45855867862701416,
          0.4567549526691437,
          0.4596904218196869,
          0.45853927731513977,
          0.46606287360191345,
          0.4542931914329529,
          0.4674118459224701,
          0.4753473103046417,
          0.47614410519599915,
          0.47795623540878296,
          0.4470784366130829,
          0.44352927803993225,
          0.4658733010292053,
          0.45039331912994385,
          0.4703766107559204,
          0.468331903219223,
          0.45013880729675293,
          0.47643551230430603,
          0.48056307435035706,
          0.4679203927516937,
          0.4670749604701996,
          0.4684046506881714,
          0.4684523344039917,
          0.4607744812965393,
          0.4682931900024414,
          0.4704364538192749,
          0.454485684633255,
          0.4426034092903137,
          0.4685191810131073,
          0.4596419632434845,
          0.47597575187683105,
          0.4645873010158539,
          0.4657570719718933,
          0.4541682302951813,
          0.4486199617385864,
          0.46624672412872314,
          0.4767889380455017,
          0.44081345200538635,
          0.4699927270412445,
          0.4569353461265564,
          0.47805315256118774,
          0.46338772773742676,
          0.4572693109512329,
          0.4725106358528137,
          0.456922709941864,
          0.46617448329925537,
          0.477407842874527,
          0.4624048173427582,
          0.462058961391449,
          0.4733280837535858,
          0.46449071168899536,
          0.4565770924091339,
          0.48176470398902893,
          0.43574878573417664,
          0.4606058597564697,
          0.441386878490448,
          0.4533366858959198,
          0.4279913306236267,
          0.4643276035785675,
          0.43156737089157104,
          0.44588032364845276,
          0.4605279862880707,
          0.45456215739250183,
          0.4541884660720825,
          0.4469389021396637,
          0.4569903612136841,
          0.45481714606285095,
          0.45237997174263,
          0.45210519433021545,
          0.458232581615448,
          0.4281577467918396,
          0.4549306631088257,
          0.46242576837539673,
          0.46115875244140625,
          0.4495637118816376,
          0.44668135046958923,
          0.4818021059036255,
          0.4629327952861786,
          0.4615419805049896,
          0.44035208225250244,
          0.4447105824947357,
          0.4415982961654663,
          0.46256890892982483,
          0.4556501507759094,
          0.44870778918266296,
          0.4688856303691864,
          0.4575933516025543,
          0.43935075402259827,
          0.4661843478679657,
          0.44741490483283997,
          0.464346706867218,
          0.4404124915599823,
          0.46259233355522156,
          0.4677920639514923,
          0.4584442377090454,
          0.44166746735572815,
          0.450294554233551,
          0.46185940504074097,
          0.44808247685432434,
          0.43433547019958496,
          0.44507020711898804,
          0.43650275468826294,
          0.4732448160648346,
          0.449068158864975,
          0.4666382372379303,
          0.44863569736480713,
          0.45961248874664307,
          0.4396378695964813,
          0.46843114495277405,
          0.45590561628341675,
          0.42157626152038574,
          0.4511694610118866,
          0.44252488017082214,
          0.46118101477622986,
          0.46421223878860474,
          0.4659162163734436,
          0.4553964138031006,
          0.45348772406578064,
          0.444376140832901,
          0.4525371193885803,
          0.4579872786998749,
          0.44720691442489624,
          0.4402284026145935,
          0.44063490629196167,
          0.4478016495704651,
          0.4605129063129425,
          0.4481503367424011,
          0.4609268009662628,
          0.4478817582130432,
          0.44162896275520325,
          0.44454824924468994,
          0.4410204291343689,
          0.4469384551048279,
          0.457179456949234,
          0.4439504146575928,
          0.46964240074157715,
          0.428293913602829,
          0.45077481865882874,
          0.4507974088191986,
          0.437572717666626,
          0.4368363618850708,
          0.4520537257194519,
          0.41760289669036865,
          0.4422019124031067,
          0.447353720664978,
          0.4577738046646118,
          0.4706379473209381,
          0.4410473108291626,
          0.46669942140579224,
          0.43635594844818115,
          0.42367205023765564,
          0.4371236562728882,
          0.45769473910331726,
          0.4320758879184723,
          0.4353220462799072,
          0.45739537477493286,
          0.44962531328201294,
          0.45385637879371643,
          0.45928066968917847,
          0.4539546072483063,
          0.4583422839641571,
          0.4423946738243103,
          0.4377508759498596,
          0.4419001340866089,
          0.43843910098075867,
          0.4463166892528534,
          0.44663020968437195,
          0.45605355501174927,
          0.4611347019672394,
          0.429784893989563,
          0.4369148910045624,
          0.43366366624832153,
          0.43711188435554504,
          0.446830153465271,
          0.41228553652763367,
          0.4405752420425415,
          0.4483563303947449,
          0.43172189593315125,
          0.43851906061172485,
          0.43454739451408386,
          0.4700797200202942,
          0.4230324923992157,
          0.45835641026496887,
          0.46263182163238525,
          0.42366474866867065,
          0.4479469060897827,
          0.45958641171455383,
          0.45215481519699097,
          0.44780269265174866,
          0.43290984630584717,
          0.4411357343196869,
          0.4452401101589203,
          0.44771045446395874,
          0.464878112077713,
          0.43273091316223145,
          0.43539705872535706,
          0.4445858597755432,
          0.45779216289520264,
          0.4201749861240387,
          0.4239468276500702,
          0.4354042410850525,
          0.4454248547554016,
          0.43963369727134705,
          0.4280896484851837,
          0.4304857850074768,
          0.4495949447154999,
          0.41788679361343384,
          0.45904824137687683,
          0.4515962302684784,
          0.42182695865631104,
          0.4560411870479584,
          0.4289892315864563,
          0.4561190903186798,
          0.41670677065849304,
          0.442385733127594,
          0.44618308544158936,
          0.43302762508392334,
          0.4624232053756714,
          0.43851926922798157,
          0.4438736140727997,
          0.44777801632881165,
          0.442128449678421,
          0.43570199608802795,
          0.4257342517375946,
          0.44296199083328247,
          0.4542783498764038,
          0.43933284282684326,
          0.4542310833930969,
          0.45028647780418396,
          0.4062024652957916,
          0.4248325824737549,
          0.43832290172576904,
          0.42709359526634216,
          0.43228664994239807,
          0.4073188006877899,
          0.4362165033817291,
          0.4518384635448456,
          0.45047828555107117,
          0.4346758723258972,
          0.4393133223056793,
          0.43715858459472656,
          0.43243399262428284,
          0.4429801404476166,
          0.4406975209712982,
          0.4187995195388794,
          0.45631951093673706,
          0.4403396248817444,
          0.43063148856163025,
          0.4534859359264374,
          0.4497060477733612,
          0.4550180435180664,
          0.44118499755859375,
          0.453279048204422,
          0.4464198350906372,
          0.439849853515625,
          0.442830890417099,
          0.4317377805709839,
          0.4191551208496094,
          0.4213311970233917,
          0.41678306460380554,
          0.4322127103805542,
          0.4063422679901123,
          0.44804880023002625,
          0.4373626112937927,
          0.42025887966156006,
          0.44185009598731995,
          0.43688663840293884,
          0.4172765016555786,
          0.4393075704574585,
          0.4535243809223175,
          0.41979748010635376,
          0.46705904603004456,
          0.43492940068244934,
          0.42400869727134705,
          0.4441281855106354,
          0.44023600220680237,
          0.46466928720474243,
          0.433421790599823,
          0.43924516439437866,
          0.4382401406764984,
          0.44656693935394287,
          0.4332743287086487,
          0.43693920969963074,
          0.42601412534713745,
          0.40998923778533936,
          0.42556849122047424,
          0.4256645441055298,
          0.4424716532230377,
          0.4425242245197296,
          0.4104604125022888,
          0.4299468398094177,
          0.4342598617076874,
          0.41392773389816284,
          0.4265613555908203,
          0.4281933009624481,
          0.4453899562358856,
          0.4172285795211792,
          0.44593921303749084,
          0.4510553181171417,
          0.45405036211013794,
          0.43020620942115784,
          0.4323088824748993,
          0.4387161135673523,
          0.42704418301582336,
          0.4338045120239258,
          0.42600950598716736,
          0.43351227045059204,
          0.44199761748313904,
          0.44951537251472473,
          0.4443644881248474,
          0.42886075377464294,
          0.45507746934890747,
          0.4423881471157074,
          0.42724403738975525,
          0.4508022665977478,
          0.4086424708366394,
          0.4168575704097748,
          0.4266004264354706,
          0.4469495117664337,
          0.43359875679016113,
          0.4149776101112366,
          0.4344998598098755,
          0.4482184052467346,
          0.40798723697662354,
          0.43295735120773315,
          0.44905611872673035,
          0.43703028559684753,
          0.43153801560401917,
          0.427771657705307,
          0.4269861578941345,
          0.4352099299430847,
          0.4399733245372772,
          0.41418910026550293,
          0.4347749650478363,
          0.42772191762924194,
          0.433414101600647,
          0.4143163561820984,
          0.44010183215141296,
          0.40813201665878296,
          0.4566066563129425,
          0.4478309452533722,
          0.4385327398777008,
          0.45069384574890137,
          0.42690446972846985,
          0.4374559223651886,
          0.4058300256729126,
          0.4440472424030304,
          0.39496490359306335,
          0.42706555128097534,
          0.42735353112220764,
          0.4411645829677582,
          0.42872947454452515,
          0.41947296261787415,
          0.4443618357181549,
          0.42545685172080994,
          0.4212488830089569,
          0.43317654728889465,
          0.44972413778305054,
          0.41092759370803833,
          0.4458581209182739,
          0.4451625943183899,
          0.42986932396888733,
          0.43757301568984985,
          0.4519336223602295,
          0.44183430075645447,
          0.4166683852672577,
          0.4341723620891571,
          0.43136295676231384,
          0.4269878566265106,
          0.440832257270813,
          0.44805699586868286,
          0.4417896270751953,
          0.43136870861053467,
          0.45965391397476196,
          0.4251115918159485,
          0.4185579717159271,
          0.4308234453201294,
          0.43769240379333496,
          0.4423215985298157,
          0.407035768032074,
          0.4366329312324524,
          0.4400623142719269,
          0.43234992027282715,
          0.4578651785850525,
          0.4534395635128021,
          0.4224869906902313,
          0.43482962250709534,
          0.41980767250061035,
          0.41594237089157104,
          0.4522288739681244,
          0.4448910057544708,
          0.4327150881290436,
          0.43288835883140564,
          0.4162684381008148,
          0.42695102095603943,
          0.4251643419265747,
          0.45198988914489746,
          0.41938516497612,
          0.4180205762386322,
          0.40850991010665894,
          0.4409077763557434,
          0.4425466060638428,
          0.45364707708358765,
          0.4370655417442322,
          0.4212642312049866,
          0.4363594055175781,
          0.40650469064712524,
          0.43713170289993286,
          0.4092399477958679,
          0.43823301792144775,
          0.3978813886642456,
          0.420882910490036,
          0.42664945125579834,
          0.4497147798538208,
          0.4291607141494751,
          0.4406525194644928,
          0.43865981698036194,
          0.43224841356277466,
          0.41931915283203125,
          0.41236451268196106,
          0.41503939032554626,
          0.4153791069984436,
          0.4166775345802307,
          0.4339390695095062,
          0.41859182715415955,
          0.4277670383453369,
          0.4382953941822052,
          0.41736486554145813,
          0.4389960467815399,
          0.4093119204044342,
          0.4254780113697052,
          0.44018998742103577,
          0.42138606309890747,
          0.4419192373752594,
          0.42612335085868835,
          0.42317691445350647,
          0.43489113450050354,
          0.42228302359580994,
          0.44034016132354736,
          0.4238520562648773,
          0.4307677447795868,
          0.4018743932247162,
          0.41800105571746826,
          0.4405815005302429,
          0.4198804795742035,
          0.4233977794647217,
          0.426693856716156,
          0.435772567987442,
          0.3981844186782837,
          0.42175835371017456,
          0.41916385293006897,
          0.4191775619983673,
          0.436662495136261,
          0.4067907929420471,
          0.42284253239631653,
          0.4284150302410126,
          0.44080525636672974,
          0.41232988238334656,
          0.45517101883888245,
          0.41025060415267944,
          0.44334307312965393,
          0.4222700297832489,
          0.44040757417678833,
          0.41758808493614197,
          0.44238513708114624,
          0.4207100570201874,
          0.4337097406387329,
          0.44512537121772766,
          0.4410852789878845,
          0.4242805242538452,
          0.4366293251514435,
          0.4363335072994232,
          0.4362013339996338,
          0.43788599967956543,
          0.4277764856815338,
          0.44621098041534424,
          0.4304874539375305,
          0.4460242688655853,
          0.4186958372592926,
          0.41610661149024963,
          0.40876448154449463,
          0.4263109564781189,
          0.42225056886672974,
          0.4374004006385803,
          0.419869989156723,
          0.4321144223213196,
          0.4020705819129944,
          0.4069666266441345,
          0.4414517879486084,
          0.42418602108955383,
          0.427279531955719,
          0.40059778094291687,
          0.4195796549320221,
          0.4213119149208069,
          0.41242414712905884,
          0.42708712816238403,
          0.43659916520118713,
          0.4398151636123657,
          0.43045520782470703,
          0.4014478623867035,
          0.42290523648262024,
          0.4299027919769287,
          0.440660297870636,
          0.4226621389389038,
          0.41590529680252075,
          0.44555991888046265,
          0.4240441918373108,
          0.4376841187477112,
          0.44427159428596497,
          0.42882952094078064,
          0.39746037125587463,
          0.4363761842250824,
          0.42328324913978577,
          0.4058598279953003,
          0.43503519892692566,
          0.4300800859928131,
          0.4248350262641907,
          0.44921037554740906,
          0.4259208142757416,
          0.4344837963581085,
          0.4181002676486969,
          0.3957292437553406,
          0.43669962882995605,
          0.4344590902328491,
          0.4352411925792694,
          0.423641562461853,
          0.4142126739025116,
          0.42218926548957825,
          0.4151788055896759,
          0.42372971773147583,
          0.43763086199760437,
          0.40259605646133423,
          0.44689443707466125,
          0.3927651047706604,
          0.4244009554386139,
          0.4050200879573822,
          0.41807499527931213,
          0.4288564920425415,
          0.4129146635532379,
          0.42533427476882935,
          0.43200168013572693,
          0.4231314957141876,
          0.4089159667491913,
          0.420341819524765,
          0.41571733355522156,
          0.42834141850471497,
          0.4184466004371643,
          0.4338001012802124,
          0.42175742983818054,
          0.4201700985431671,
          0.4231446385383606,
          0.4274052679538727,
          0.41007569432258606,
          0.4199329912662506,
          0.44386571645736694,
          0.4190409779548645,
          0.4127237796783447,
          0.43438056111335754,
          0.40496495366096497,
          0.4105478823184967,
          0.4282229542732239,
          0.43351292610168457,
          0.422922819852829,
          0.4330586791038513,
          0.42038896679878235,
          0.4288698434829712,
          0.42490217089653015,
          0.437804639339447,
          0.43662920594215393,
          0.4020186960697174,
          0.414386510848999,
          0.41711336374282837,
          0.40838149189949036,
          0.43360021710395813,
          0.4188772439956665,
          0.4339466989040375,
          0.4032541513442993,
          0.4059123992919922,
          0.431838721036911,
          0.4301396608352661,
          0.4252365827560425,
          0.4157339632511139,
          0.43469488620758057,
          0.4389054477214813,
          0.4481533169746399,
          0.4231097102165222,
          0.43474534153938293,
          0.44230714440345764,
          0.4134746491909027,
          0.45142388343811035,
          0.3967924118041992,
          0.44930344820022583,
          0.4268999993801117,
          0.42955729365348816,
          0.42979666590690613,
          0.4403502643108368,
          0.42159178853034973,
          0.4330974817276001,
          0.41876062750816345,
          0.4164476692676544,
          0.43436089158058167,
          0.4202527701854706,
          0.41495412588119507,
          0.42236989736557007,
          0.4110300540924072,
          0.42272788286209106,
          0.41681966185569763,
          0.4400971531867981,
          0.3996291756629944,
          0.44528016448020935,
          0.41850101947784424,
          0.41903337836265564,
          0.4071928560733795,
          0.4114655554294586,
          0.4451535642147064,
          0.431640088558197,
          0.4370785653591156,
          0.41758593916893005,
          0.4201732277870178,
          0.4399147629737854,
          0.40782803297042847,
          0.4323868453502655,
          0.4408881366252899,
          0.4342556595802307,
          0.43337199091911316,
          0.41809454560279846,
          0.4275745153427124,
          0.43832895159721375,
          0.44033533334732056,
          0.40369004011154175,
          0.43706345558166504,
          0.4171977639198303,
          0.42786145210266113,
          0.42983946204185486,
          0.41039755940437317,
          0.4318235218524933,
          0.4286099076271057,
          0.4197312593460083,
          0.43762385845184326,
          0.4383918344974518,
          0.41827648878097534,
          0.4153956174850464,
          0.4221188426017761,
          0.41678929328918457,
          0.4163142144680023,
          0.42156410217285156,
          0.41380804777145386,
          0.4246384799480438,
          0.3939952552318573,
          0.4394401013851166,
          0.40304630994796753,
          0.4146727919578552,
          0.4281517267227173,
          0.40364211797714233,
          0.4179956614971161,
          0.40714800357818604,
          0.4398098886013031,
          0.44451960921287537,
          0.44711560010910034,
          0.4198545515537262,
          0.42210036516189575,
          0.41629359126091003,
          0.4301755428314209,
          0.4342282712459564,
          0.4258527457714081,
          0.4153168201446533,
          0.4041125476360321,
          0.4225347936153412,
          0.42711520195007324,
          0.4405170977115631,
          0.44258353114128113,
          0.43975701928138733,
          0.42858368158340454,
          0.42425060272216797,
          0.4325783848762512,
          0.42850950360298157,
          0.4271831810474396,
          0.41470763087272644,
          0.4062201976776123,
          0.4213637709617615,
          0.4258461892604828,
          0.43732738494873047,
          0.41974881291389465,
          0.40896570682525635,
          0.4195714294910431,
          0.4225064814090729,
          0.4385988414287567,
          0.4277017414569855,
          0.4097067415714264,
          0.41346511244773865,
          0.44308924674987793,
          0.4333213269710541,
          0.4294221103191376,
          0.43065154552459717,
          0.4489198327064514,
          0.41715964674949646,
          0.4133757948875427,
          0.4119272530078888,
          0.42568838596343994,
          0.4215927720069885,
          0.42482298612594604,
          0.43578261137008667,
          0.43342170119285583,
          0.4199812710285187,
          0.43654441833496094,
          0.4206341505050659,
          0.42033329606056213,
          0.425996869802475,
          0.4523892104625702,
          0.4111538231372833,
          0.4155958592891693,
          0.40275129675865173,
          0.4178538918495178,
          0.3989757299423218,
          0.41613757610321045,
          0.42119118571281433,
          0.40880081057548523,
          0.3993273675441742,
          0.44742780923843384,
          0.396637886762619,
          0.4192163646221161,
          0.41646379232406616,
          0.4295411705970764,
          0.415912002325058,
          0.4197585880756378,
          0.4202735722064972,
          0.4352378249168396,
          0.41720134019851685
         ]
        },
        {
         "name": "Validation loss",
         "type": "scatter",
         "y": [
          0.6925442218780518,
          0.6919081211090088,
          0.6906221508979797,
          0.6864170432090759,
          0.6859152317047119,
          0.6857425570487976,
          0.6836628913879395,
          0.6797863841056824,
          0.6775246262550354,
          0.677074670791626,
          0.6735283136367798,
          0.6637408137321472,
          0.6628450155258179,
          0.6663758158683777,
          0.6542071104049683,
          0.6506791114807129,
          0.6519656777381897,
          0.6500076651573181,
          0.6490685343742371,
          0.641932487487793,
          0.6409260034561157,
          0.6418480277061462,
          0.623707115650177,
          0.6299580931663513,
          0.62950599193573,
          0.6193510890007019,
          0.6127473711967468,
          0.6101421117782593,
          0.6081973314285278,
          0.60723876953125,
          0.6101337671279907,
          0.6148461699485779,
          0.6012480854988098,
          0.6063530445098877,
          0.6017544269561768,
          0.5941671133041382,
          0.6045002341270447,
          0.5994154214859009,
          0.6031486392021179,
          0.6050335168838501,
          0.6013517379760742,
          0.5989698171615601,
          0.5934904217720032,
          0.5995200276374817,
          0.6014564633369446,
          0.5884954929351807,
          0.5959875583648682,
          0.5939797163009644,
          0.5934962630271912,
          0.5891291499137878,
          0.5934641361236572,
          0.5890361070632935,
          0.5913178324699402,
          0.5801715850830078,
          0.5808280110359192,
          0.5954172611236572,
          0.5929731130599976,
          0.5889731049537659,
          0.5874158143997192,
          0.5831114053726196,
          0.5864917039871216,
          0.5898510813713074,
          0.5903778076171875,
          0.5839531421661377,
          0.5745017528533936,
          0.5836591124534607,
          0.5858557224273682,
          0.585586667060852,
          0.581549882888794,
          0.5701759457588196,
          0.5701332688331604,
          0.579281210899353,
          0.5785127878189087,
          0.5851477980613708,
          0.5733158588409424,
          0.5807469487190247,
          0.5667448043823242,
          0.5762375593185425,
          0.5664461255073547,
          0.5691819787025452,
          0.5727094411849976,
          0.5728514790534973,
          0.5677458047866821,
          0.5706753730773926,
          0.5682176947593689,
          0.5722383260726929,
          0.5707741975784302,
          0.567581832408905,
          0.5671847462654114,
          0.5702942609786987,
          0.5734036564826965,
          0.5735616087913513,
          0.5662649273872375,
          0.5753920674324036,
          0.5754296183586121,
          0.5688643455505371,
          0.5729190111160278,
          0.5679733157157898,
          0.5705237984657288,
          0.5679168105125427,
          0.5739123821258545,
          0.5660195350646973,
          0.5669201612472534,
          0.566953182220459,
          0.5648697018623352,
          0.5661907196044922,
          0.5685973167419434,
          0.5541853904724121,
          0.5739710927009583,
          0.5725966691970825,
          0.5652968287467957,
          0.5604240894317627,
          0.5611374378204346,
          0.5609601736068726,
          0.5557296872138977,
          0.5664497017860413,
          0.5612698197364807,
          0.5579327940940857,
          0.5570285320281982,
          0.5591992735862732,
          0.5617113709449768,
          0.5534538626670837,
          0.5616199970245361,
          0.5552195906639099,
          0.5635989904403687,
          0.5523536801338196,
          0.5569964647293091,
          0.5579978823661804,
          0.5502854585647583,
          0.5569204092025757,
          0.5546677708625793,
          0.5520920753479004,
          0.5465654134750366,
          0.5504515171051025,
          0.5475685596466064,
          0.5538164377212524,
          0.5449144244194031,
          0.5407873392105103,
          0.5507373213768005,
          0.5454810261726379,
          0.538561999797821,
          0.5326972007751465,
          0.542982816696167,
          0.5437084436416626,
          0.5455646514892578,
          0.5431002378463745,
          0.5438522100448608,
          0.5393247008323669,
          0.5404968857765198,
          0.5420646667480469,
          0.5348019003868103,
          0.5375083684921265,
          0.5326417684555054,
          0.5457759499549866,
          0.5461505055427551,
          0.5407388806343079,
          0.539548933506012,
          0.5375511050224304,
          0.5367278456687927,
          0.5337933301925659,
          0.5348598957061768,
          0.5270841717720032,
          0.5381104350090027,
          0.5360272526741028,
          0.5318871736526489,
          0.5389153957366943,
          0.534827470779419,
          0.5226714611053467,
          0.5250577330589294,
          0.5231599807739258,
          0.5311937928199768,
          0.5307590365409851,
          0.5303167104721069,
          0.5266733765602112,
          0.5187587738037109,
          0.5259447693824768,
          0.5314961075782776,
          0.5167842507362366,
          0.519691526889801,
          0.5340937972068787,
          0.5383721590042114,
          0.5344008207321167,
          0.5322961807250977,
          0.5249903798103333,
          0.525489091873169,
          0.5201828479766846,
          0.5250453352928162,
          0.5298033356666565,
          0.5234386920928955,
          0.5276946425437927,
          0.5202352404594421,
          0.5190457105636597,
          0.5216928720474243,
          0.5193607211112976,
          0.5234326124191284,
          0.5168148279190063,
          0.5143086910247803,
          0.5225344300270081,
          0.5193565487861633,
          0.5226425528526306,
          0.5155125260353088,
          0.5238227248191833,
          0.5244607329368591,
          0.5200994610786438,
          0.5216127634048462,
          0.5172316431999207,
          0.5195357799530029,
          0.5193312168121338,
          0.5182362794876099,
          0.5210888981819153,
          0.5239917635917664,
          0.5200015902519226,
          0.5072509050369263,
          0.5205411314964294,
          0.515963613986969,
          0.5144822001457214,
          0.5154364705085754,
          0.517585813999176,
          0.5168650150299072,
          0.5204671621322632,
          0.5159243941307068,
          0.5154253840446472,
          0.5168820023536682,
          0.5187533497810364,
          0.5126017928123474,
          0.5168318748474121,
          0.5229447484016418,
          0.518772542476654,
          0.5206333994865417,
          0.5179121494293213,
          0.5219672918319702,
          0.5210296511650085,
          0.5205276012420654,
          0.5248680710792542,
          0.5233566164970398,
          0.5215479731559753,
          0.5170875191688538,
          0.5203132629394531,
          0.52244633436203,
          0.5139436721801758,
          0.5226333737373352,
          0.5135620832443237,
          0.5110101103782654,
          0.5218389630317688,
          0.5180490016937256,
          0.5131052136421204,
          0.5116957426071167,
          0.5226814150810242,
          0.5193547606468201,
          0.5174542665481567,
          0.5161641836166382,
          0.52601158618927,
          0.5204191207885742,
          0.5100248456001282,
          0.5148008465766907,
          0.511406660079956,
          0.5136975646018982,
          0.5123811960220337,
          0.5074067115783691,
          0.5103759169578552,
          0.5143445134162903,
          0.5132737755775452,
          0.5111721158027649,
          0.5067669749259949,
          0.5087068676948547,
          0.5108642578125,
          0.5124008655548096,
          0.5155676007270813,
          0.5110378265380859,
          0.5150097012519836,
          0.517587423324585,
          0.5104451775550842,
          0.508810818195343,
          0.5109947323799133,
          0.5097352266311646,
          0.5119049549102783,
          0.5054962635040283,
          0.5076798796653748,
          0.5115427374839783,
          0.5103728175163269,
          0.5131301283836365,
          0.512553334236145,
          0.5148876309394836,
          0.5082860589027405,
          0.5036361813545227,
          0.5046494007110596,
          0.5055208206176758,
          0.5067843198776245,
          0.5011239051818848,
          0.5002886652946472,
          0.5039255023002625,
          0.49982741475105286,
          0.5016780495643616,
          0.505707859992981,
          0.49904704093933105,
          0.5068665742874146,
          0.5123124122619629,
          0.5033129453659058,
          0.4951779544353485,
          0.5068775415420532,
          0.5050314664840698,
          0.5069979429244995,
          0.5017438530921936,
          0.5023225545883179,
          0.5015181303024292,
          0.504862368106842,
          0.5018134117126465,
          0.5016847848892212,
          0.5043386816978455,
          0.5036174058914185,
          0.4997168779373169,
          0.49886205792427063,
          0.5005518198013306,
          0.5071269273757935,
          0.5091257095336914,
          0.5055471658706665,
          0.5082745552062988,
          0.5090099573135376,
          0.504896342754364,
          0.5041730999946594,
          0.5090858340263367,
          0.5060418844223022,
          0.5050324201583862,
          0.5049944519996643,
          0.500120997428894,
          0.501710057258606,
          0.503933310508728,
          0.5016815662384033,
          0.5021414160728455,
          0.5011019110679626,
          0.5028666853904724,
          0.5069742798805237,
          0.5028024315834045,
          0.49934521317481995,
          0.49842119216918945,
          0.5017220377922058,
          0.4986080229282379,
          0.5034087896347046,
          0.5046031475067139,
          0.5023503303527832,
          0.501315712928772,
          0.5102658867835999,
          0.5034685730934143,
          0.5047178268432617,
          0.5002399682998657,
          0.497799277305603,
          0.5036465525627136,
          0.4980107247829437,
          0.5051385760307312,
          0.5027625560760498,
          0.4983106851577759,
          0.5018380880355835,
          0.495410680770874,
          0.4962467849254608,
          0.5016504526138306,
          0.5004807114601135,
          0.4972148537635803,
          0.49528971314430237,
          0.49752554297447205,
          0.49326765537261963,
          0.4957244396209717,
          0.4917113482952118,
          0.49325162172317505,
          0.49668341875076294,
          0.4933384358882904,
          0.4931408166885376,
          0.4933972954750061,
          0.4949168264865875,
          0.49739599227905273,
          0.4910733103752136,
          0.49351638555526733,
          0.48887670040130615,
          0.4908706247806549,
          0.4858878254890442,
          0.4923740327358246,
          0.494475781917572,
          0.49690625071525574,
          0.49643591046333313,
          0.49113816022872925,
          0.4877738356590271,
          0.48955273628234863,
          0.487966924905777,
          0.4883919656276703,
          0.48557794094085693,
          0.49219220876693726,
          0.4908456802368164,
          0.49157795310020447,
          0.491873174905777,
          0.49548235535621643,
          0.4937235116958618,
          0.49412450194358826,
          0.494069904088974,
          0.4907100796699524,
          0.499843955039978,
          0.4922254681587219,
          0.48446163535118103,
          0.491423636674881,
          0.4876604974269867,
          0.48715925216674805,
          0.48496362566947937,
          0.4840176999568939,
          0.48643964529037476,
          0.49076858162879944,
          0.4866568148136139,
          0.49193909764289856,
          0.487543523311615,
          0.48979365825653076,
          0.488910436630249,
          0.49117711186408997,
          0.4915204346179962,
          0.4934830963611603,
          0.4911514222621918,
          0.4902387857437134,
          0.4897424280643463,
          0.49497395753860474,
          0.4925539791584015,
          0.49269187450408936,
          0.492615282535553,
          0.4899987280368805,
          0.4896054267883301,
          0.4891667366027832,
          0.49035537242889404,
          0.49129951000213623,
          0.4881210923194885,
          0.48908576369285583,
          0.49303770065307617,
          0.4850586950778961,
          0.4910382926464081,
          0.49417150020599365,
          0.4891760051250458,
          0.48943468928337097,
          0.4914000928401947,
          0.4895392060279846,
          0.49410879611968994,
          0.49101197719573975,
          0.48452654480934143,
          0.4894922077655792,
          0.4942744970321655,
          0.4860159456729889,
          0.48538610339164734,
          0.48479053378105164,
          0.48783355951309204,
          0.4860597550868988,
          0.4867342710494995,
          0.488596111536026,
          0.49101001024246216,
          0.48903366923332214,
          0.4852253198623657,
          0.4858478903770447,
          0.49312591552734375,
          0.4919466972351074,
          0.49604615569114685,
          0.49101704359054565,
          0.48734959959983826,
          0.49005764722824097,
          0.48947763442993164,
          0.4863591194152832,
          0.488168865442276,
          0.4847707748413086,
          0.4888855516910553,
          0.4851315915584564,
          0.47793662548065186,
          0.4878164827823639,
          0.48642128705978394,
          0.4805997610092163,
          0.4796379506587982,
          0.4841819405555725,
          0.4863537549972534,
          0.48887673020362854,
          0.484369158744812,
          0.4811343848705292,
          0.48017293214797974,
          0.4846124053001404,
          0.4815550744533539,
          0.4823354482650757,
          0.4850032329559326,
          0.4789862334728241,
          0.4777878224849701,
          0.4815131723880768,
          0.48974883556365967,
          0.48455873131752014,
          0.48007017374038696,
          0.47771328687667847,
          0.4849763810634613,
          0.48243170976638794,
          0.48104074597358704,
          0.4830222725868225,
          0.4890821874141693,
          0.48245349526405334,
          0.48704802989959717,
          0.4825737178325653,
          0.48913031816482544,
          0.4861564040184021,
          0.4877200424671173,
          0.48427531123161316,
          0.4834904670715332,
          0.4825066924095154,
          0.4803270101547241,
          0.48234274983406067,
          0.482248455286026,
          0.48402395844459534,
          0.47876036167144775,
          0.4824824333190918,
          0.4844062030315399,
          0.48505133390426636,
          0.48265770077705383,
          0.4882681965827942,
          0.48386356234550476,
          0.4896893799304962,
          0.48823443055152893,
          0.4884660840034485,
          0.49136990308761597,
          0.487942636013031,
          0.4881584942340851,
          0.48385757207870483,
          0.4875943958759308,
          0.4862704575061798,
          0.48696181178092957,
          0.48502862453460693,
          0.485309362411499,
          0.48978734016418457,
          0.4867621660232544,
          0.48586922883987427,
          0.48932337760925293,
          0.48634064197540283,
          0.4880356788635254,
          0.48267680406570435,
          0.479861855506897,
          0.4881407618522644,
          0.4829410910606384,
          0.48395875096321106,
          0.4815686345100403,
          0.48482319712638855,
          0.48086389899253845,
          0.48406782746315,
          0.48147255182266235,
          0.4796048700809479,
          0.48469817638397217,
          0.4810149073600769,
          0.48334750533103943,
          0.4804927408695221,
          0.47985997796058655,
          0.4836268424987793,
          0.48282569646835327,
          0.48593491315841675,
          0.4829823970794678,
          0.48032063245773315,
          0.4814537465572357,
          0.48283112049102783,
          0.4816196858882904,
          0.48521339893341064,
          0.4843961000442505,
          0.48344436287879944,
          0.47992751002311707,
          0.48077642917633057,
          0.48009786009788513,
          0.48153746128082275,
          0.4822174608707428,
          0.48472994565963745,
          0.4832470417022705,
          0.4874718487262726,
          0.48378095030784607,
          0.48240670561790466,
          0.47785913944244385,
          0.47932878136634827,
          0.47753673791885376,
          0.47770681977272034,
          0.4812331795692444,
          0.48389261960983276,
          0.4807656705379486,
          0.4789488613605499,
          0.4761268198490143,
          0.4775882959365845,
          0.4771328568458557,
          0.4790354073047638,
          0.47865357995033264,
          0.4772726595401764,
          0.4778425991535187,
          0.4765291213989258,
          0.48107296228408813,
          0.4779900312423706,
          0.47729775309562683,
          0.47900158166885376,
          0.4788992702960968,
          0.4844863712787628,
          0.479984313249588,
          0.48045384883880615,
          0.4798939824104309,
          0.48240792751312256,
          0.4829089045524597,
          0.4792384207248688,
          0.48295262455940247,
          0.4841133952140808,
          0.48378175497055054,
          0.4807092249393463,
          0.48564067482948303,
          0.4835805594921112,
          0.48537397384643555,
          0.48153814673423767,
          0.4799427390098572,
          0.48103243112564087,
          0.48547425866127014,
          0.48499125242233276,
          0.4838167428970337,
          0.48482513427734375,
          0.48058947920799255,
          0.4816170632839203,
          0.479812353849411,
          0.4809553027153015,
          0.4806676506996155,
          0.4817184507846832,
          0.48101675510406494,
          0.47943705320358276,
          0.4794645607471466,
          0.4819307029247284,
          0.47920161485671997,
          0.4781726598739624,
          0.4773048162460327,
          0.4807048738002777,
          0.48299869894981384,
          0.4823494851589203,
          0.4800620675086975,
          0.4809066355228424,
          0.4791066646575928,
          0.4753383696079254,
          0.4794062376022339,
          0.4785521626472473,
          0.47916752099990845,
          0.48012736439704895,
          0.4794853925704956,
          0.4783798158168793,
          0.47771692276000977,
          0.4813712239265442,
          0.4799409508705139,
          0.47974395751953125,
          0.4799621105194092,
          0.47854024171829224,
          0.4786834716796875,
          0.4798313081264496,
          0.47964194416999817,
          0.480883926153183,
          0.48033761978149414,
          0.4787951409816742,
          0.47909030318260193,
          0.4792573153972626,
          0.4822818338871002,
          0.48093220591545105,
          0.4799741804599762,
          0.4779110252857208,
          0.476421594619751,
          0.47831135988235474,
          0.480663001537323,
          0.4792918264865875,
          0.4791179299354553,
          0.47939997911453247,
          0.478797048330307,
          0.4826132357120514,
          0.48316481709480286,
          0.4829738438129425,
          0.4820280075073242,
          0.47694259881973267,
          0.47850215435028076,
          0.4796660244464874,
          0.4775187075138092,
          0.4780121147632599,
          0.4812713861465454,
          0.48226556181907654,
          0.4811687171459198,
          0.47947177290916443,
          0.4792724549770355,
          0.4780668020248413,
          0.4785297214984894,
          0.4791664481163025,
          0.4786917269229889,
          0.4768347144126892,
          0.47796496748924255,
          0.4774097502231598,
          0.4775558412075043,
          0.47957906126976013,
          0.47922465205192566,
          0.4775587320327759,
          0.47870808839797974,
          0.4803254306316376,
          0.48116251826286316,
          0.48211470246315,
          0.48010605573654175,
          0.4789312779903412,
          0.4793602526187897,
          0.4818369448184967,
          0.4785943329334259,
          0.48253658413887024,
          0.48058390617370605,
          0.48061296343803406,
          0.47904255986213684,
          0.48127371072769165,
          0.48091089725494385,
          0.48322829604148865,
          0.4814731776714325,
          0.481231689453125,
          0.4794984459877014,
          0.4795864224433899,
          0.47997167706489563,
          0.4797157049179077,
          0.4818499684333801,
          0.48007890582084656,
          0.4795650839805603,
          0.47977587580680847,
          0.4800563156604767,
          0.4802551865577698,
          0.478529155254364,
          0.48026755452156067,
          0.48101985454559326,
          0.48046621680259705,
          0.48098233342170715,
          0.4805268943309784,
          0.4800252616405487,
          0.4839876890182495,
          0.4822303056716919,
          0.4811397194862366,
          0.4822660982608795,
          0.47999945282936096,
          0.48104485869407654,
          0.4814484119415283,
          0.4833030104637146,
          0.48096156120300293,
          0.48008301854133606,
          0.48124581575393677,
          0.48164188861846924,
          0.48178479075431824,
          0.48052072525024414,
          0.4801875948905945,
          0.4811696708202362,
          0.4822883903980255,
          0.482250839471817,
          0.48158693313598633,
          0.48051077127456665,
          0.4823094308376312,
          0.482217937707901,
          0.4809998869895935,
          0.4807920753955841,
          0.4780396521091461,
          0.47802847623825073,
          0.47972437739372253,
          0.47936779260635376,
          0.47952136397361755,
          0.4796021580696106,
          0.4778723418712616,
          0.4799835681915283,
          0.4803556799888611,
          0.47880271077156067,
          0.4798051416873932,
          0.47859105467796326,
          0.48154690861701965,
          0.4805515706539154,
          0.4817071855068207,
          0.48211777210235596,
          0.48284095525741577,
          0.48246142268180847,
          0.48187991976737976,
          0.4807664453983307,
          0.4818877577781677,
          0.48205339908599854,
          0.4825475215911865,
          0.48369020223617554,
          0.4823032021522522,
          0.4823939800262451,
          0.48408329486846924,
          0.48331958055496216,
          0.4843960404396057,
          0.48447948694229126,
          0.48482808470726013,
          0.48257821798324585,
          0.48311564326286316,
          0.4822879135608673,
          0.4805084764957428,
          0.48242589831352234,
          0.4809921681880951,
          0.4816884994506836,
          0.48144960403442383,
          0.48026159405708313,
          0.4825228452682495,
          0.48255082964897156,
          0.48028743267059326,
          0.4812990128993988,
          0.4814147353172302,
          0.48438066244125366,
          0.48400235176086426,
          0.4828638434410095,
          0.4868074059486389,
          0.4861201345920563,
          0.4815157651901245,
          0.48011571168899536,
          0.48154228925704956,
          0.4809766411781311,
          0.4814542829990387,
          0.4814453721046448,
          0.48215579986572266,
          0.4805102050304413,
          0.4807853400707245,
          0.48223355412483215,
          0.48187586665153503,
          0.48059651255607605,
          0.48368966579437256,
          0.48357102274894714,
          0.4824046194553375,
          0.4844878017902374,
          0.48425713181495667,
          0.48281195759773254,
          0.48327165842056274,
          0.4855368733406067,
          0.48388582468032837,
          0.48360690474510193,
          0.48416879773139954,
          0.4823697805404663,
          0.48391351103782654,
          0.4846513867378235,
          0.4800078868865967,
          0.47952187061309814,
          0.4814686179161072,
          0.48126667737960815,
          0.48156988620758057,
          0.4828420877456665,
          0.4837053418159485,
          0.4830671548843384,
          0.48213255405426025,
          0.4819161295890808,
          0.48122817277908325,
          0.48281294107437134,
          0.483111709356308,
          0.4811350107192993,
          0.481173574924469,
          0.4804246723651886,
          0.4800291061401367,
          0.4816100299358368,
          0.4823395311832428,
          0.4840143322944641,
          0.4823669493198395,
          0.4812798500061035,
          0.4827415943145752,
          0.48292163014411926,
          0.4816112816333771,
          0.48216530680656433,
          0.48114094138145447,
          0.4807623028755188,
          0.4802393913269043,
          0.481426864862442,
          0.4809493124485016,
          0.4840957224369049,
          0.4833475351333618,
          0.4824409782886505,
          0.4848334491252899,
          0.48191994428634644,
          0.4798453748226166,
          0.48197486996650696,
          0.4833430349826813,
          0.4835284352302551,
          0.4819331169128418,
          0.48190006613731384,
          0.482587605714798,
          0.4835648536682129,
          0.4832988977432251,
          0.4831058979034424,
          0.4808444380760193,
          0.48085179924964905,
          0.4808754324913025,
          0.4786355793476105,
          0.4779612123966217,
          0.47919464111328125,
          0.4800450801849365,
          0.47984349727630615,
          0.480290949344635,
          0.48018649220466614,
          0.4805479645729065,
          0.48168644309043884,
          0.481222003698349,
          0.4804355502128601,
          0.48090142011642456,
          0.4807741045951843,
          0.48066195845603943,
          0.479093074798584,
          0.4799255430698395,
          0.4808165431022644,
          0.4811554551124573,
          0.48090073466300964,
          0.4790753424167633,
          0.47926393151283264,
          0.47858670353889465,
          0.4778999090194702,
          0.4787720739841461,
          0.47807174921035767,
          0.4789940118789673,
          0.4800683856010437,
          0.4803130626678467,
          0.4792028069496155,
          0.4796379506587982,
          0.4811221957206726,
          0.4805392920970917,
          0.48113808035850525,
          0.4797951579093933,
          0.4789864420890808,
          0.47964057326316833,
          0.4800262749195099,
          0.47953999042510986,
          0.4802916944026947,
          0.4798769950866699,
          0.4800344705581665,
          0.4799105226993561,
          0.4797521233558655,
          0.48047390580177307,
          0.4804222881793976,
          0.4803779423236847,
          0.48093482851982117,
          0.4801996052265167,
          0.4799778461456299,
          0.4798581302165985,
          0.47877076268196106,
          0.47963348031044006,
          0.4801555871963501,
          0.4798734188079834,
          0.47936907410621643,
          0.47958776354789734,
          0.4798606336116791,
          0.48149996995925903,
          0.47942861914634705,
          0.4786343276500702,
          0.4802039861679077,
          0.48081034421920776,
          0.480878621339798,
          0.481160968542099,
          0.4807882308959961,
          0.48097050189971924,
          0.4806155860424042,
          0.4801950752735138,
          0.4806380271911621,
          0.4795156717300415,
          0.4794156551361084,
          0.4796361029148102,
          0.480022132396698,
          0.4811181128025055,
          0.48202386498451233,
          0.4809100925922394,
          0.4808017611503601,
          0.4810812771320343,
          0.4808962643146515,
          0.4806102514266968,
          0.48057472705841064,
          0.48056507110595703,
          0.48230504989624023,
          0.4819961190223694,
          0.4819207489490509,
          0.48133760690689087,
          0.4813486933708191,
          0.4812164604663849,
          0.4823096692562103,
          0.4818655848503113,
          0.48133108019828796,
          0.4816005825996399,
          0.48188576102256775,
          0.4808366596698761,
          0.48102107644081116,
          0.4808363914489746,
          0.4807891249656677,
          0.4807376265525818,
          0.4803641438484192,
          0.48061099648475647,
          0.48040327429771423,
          0.4799864590167999,
          0.4804602265357971,
          0.48071518540382385,
          0.4807779788970947,
          0.4803415834903717,
          0.4792514741420746,
          0.48011359572410583,
          0.4800504446029663,
          0.48117202520370483,
          0.4817357659339905,
          0.48145702481269836,
          0.48209449648857117,
          0.4814871847629547,
          0.4822455644607544,
          0.4829200506210327,
          0.4821164011955261,
          0.48173514008522034,
          0.48118138313293457,
          0.4817850887775421,
          0.48132580518722534,
          0.48125016689300537,
          0.4813712239265442,
          0.4817121624946594,
          0.4807802438735962,
          0.48149439692497253,
          0.48193642497062683,
          0.4806363880634308,
          0.4802948832511902,
          0.48087579011917114,
          0.4809412956237793,
          0.4806029200553894,
          0.4804992377758026,
          0.48029085993766785,
          0.48019635677337646
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Loss MLP"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train accuracy",
         "type": "scatter",
         "y": [
          0.5126007795333862,
          0.48941531777381897,
          0.4994959533214569,
          0.5010080933570862,
          0.4959677457809448,
          0.49344757199287415,
          0.5010080933570862,
          0.5257056355476379,
          0.5292338728904724,
          0.538306474685669,
          0.5367943644523621,
          0.5861895084381104,
          0.5735887289047241,
          0.5907257795333862,
          0.6189516186714172,
          0.6184476017951965,
          0.6154233813285828,
          0.6209677457809448,
          0.6300403475761414,
          0.6234878897666931,
          0.6622983813285828,
          0.6310483813285828,
          0.6653226017951965,
          0.6436492204666138,
          0.6486895084381104,
          0.6693548560142517,
          0.6759072542190552,
          0.6935483813285828,
          0.6693548560142517,
          0.6638104915618896,
          0.65625,
          0.6723790168762207,
          0.6794354915618896,
          0.6693548560142517,
          0.6915322542190552,
          0.6970766186714172,
          0.6859878897666931,
          0.7041330933570862,
          0.6834677457809448,
          0.694556474685669,
          0.6829637289047241,
          0.6985887289047241,
          0.7096773982048035,
          0.6975806355476379,
          0.6869959831237793,
          0.6925403475761414,
          0.694556474685669,
          0.6864919066429138,
          0.6990927457809448,
          0.7232862710952759,
          0.6990927457809448,
          0.7061492204666138,
          0.7051411271095276,
          0.7011088728904724,
          0.7046371102333069,
          0.6900201439857483,
          0.7137096524238586,
          0.7182459831237793,
          0.7091733813285828,
          0.710181474685669,
          0.6880040168762207,
          0.7127016186714172,
          0.7263104915618896,
          0.7268145084381104,
          0.7283266186714172,
          0.7111895084381104,
          0.7303427457809448,
          0.7288306355476379,
          0.7348790168762207,
          0.7353830933570862,
          0.7127016186714172,
          0.7006048560142517,
          0.7222782373428345,
          0.7323588728904724,
          0.7263104915618896,
          0.7293346524238586,
          0.7363911271095276,
          0.7409273982048035,
          0.7444556355476379,
          0.7469757795333862,
          0.7303427457809448,
          0.7227822542190552,
          0.7232862710952759,
          0.7363911271095276,
          0.7293346524238586,
          0.7152217626571655,
          0.7459677457809448,
          0.7449596524238586,
          0.7404233813285828,
          0.7368951439857483,
          0.7152217626571655,
          0.7237903475761414,
          0.7182459831237793,
          0.7323588728904724,
          0.7368951439857483,
          0.7454637289047241,
          0.7550403475761414,
          0.7313507795333862,
          0.7288306355476379,
          0.7444556355476379,
          0.7454637289047241,
          0.7545362710952759,
          0.7444556355476379,
          0.7550403475761414,
          0.7308467626571655,
          0.7691532373428345,
          0.7338709831237793,
          0.7787298560142517,
          0.7545362710952759,
          0.758568525314331,
          0.7631048560142517,
          0.7449596524238586,
          0.7419354915618896,
          0.7515121102333069,
          0.7444556355476379,
          0.7444556355476379,
          0.7489919066429138,
          0.7792338728904724,
          0.7505040168762207,
          0.7474798560142517,
          0.7696572542190552,
          0.7636088728904724,
          0.7626007795333862,
          0.7424395084381104,
          0.7661290168762207,
          0.765625,
          0.7545362710952759,
          0.758568525314331,
          0.7736895084381104,
          0.7308467626571655,
          0.7530242204666138,
          0.7807459831237793,
          0.7807459831237793,
          0.7716733813285828,
          0.7822580933570862,
          0.7545362710952759,
          0.7686492204666138,
          0.7590726017951965,
          0.7651209831237793,
          0.7636088728904724,
          0.7746976017951965,
          0.7620967626571655,
          0.7923387289047241,
          0.7661290168762207,
          0.7676411271095276,
          0.7762096524238586,
          0.772681474685669,
          0.7721773982048035,
          0.7832661271095276,
          0.7802419066429138,
          0.7807459831237793,
          0.7948588728904724,
          0.757056474685669,
          0.7837701439857483,
          0.7550403475761414,
          0.796875,
          0.7757056355476379,
          0.7711693644523621,
          0.7752016186714172,
          0.7888104915618896,
          0.7676411271095276,
          0.7731854915618896,
          0.7857862710952759,
          0.7767137289047241,
          0.7933467626571655,
          0.7560483813285828,
          0.7998992204666138,
          0.7878023982048035,
          0.7762096524238586,
          0.7857862710952759,
          0.7903226017951965,
          0.7893145084381104,
          0.7807459831237793,
          0.7817540168762207,
          0.7847782373428345,
          0.7918346524238586,
          0.803931474685669,
          0.7852822542190552,
          0.8009072542190552,
          0.7913306355476379,
          0.796875,
          0.7721773982048035,
          0.7757056355476379,
          0.788306474685669,
          0.7978830933570862,
          0.7903226017951965,
          0.774193525314331,
          0.7787298560142517,
          0.7888104915618896,
          0.7767137289047241,
          0.7872983813285828,
          0.788306474685669,
          0.7872983813285828,
          0.7807459831237793,
          0.7872983813285828,
          0.7893145084381104,
          0.774193525314331,
          0.7928427457809448,
          0.7958669066429138,
          0.7988911271095276,
          0.7867943644523621,
          0.7872983813285828,
          0.7857862710952759,
          0.7878023982048035,
          0.7822580933570862,
          0.7893145084381104,
          0.7817540168762207,
          0.7817540168762207,
          0.7867943644523621,
          0.7857862710952759,
          0.7928427457809448,
          0.7943548560142517,
          0.7852822542190552,
          0.7691532373428345,
          0.7928427457809448,
          0.7993951439857483,
          0.7817540168762207,
          0.7953628897666931,
          0.7978830933570862,
          0.7943548560142517,
          0.7807459831237793,
          0.7832661271095276,
          0.7777217626571655,
          0.7998992204666138,
          0.7988911271095276,
          0.7847782373428345,
          0.8205645084381104,
          0.7988911271095276,
          0.7893145084381104,
          0.7847782373428345,
          0.8069556355476379,
          0.7852822542190552,
          0.7903226017951965,
          0.8114919066429138,
          0.8029233813285828,
          0.7878023982048035,
          0.7928427457809448,
          0.7752016186714172,
          0.788306474685669,
          0.8004032373428345,
          0.8034273982048035,
          0.7817540168762207,
          0.8029233813285828,
          0.7893145084381104,
          0.7842742204666138,
          0.8009072542190552,
          0.7953628897666931,
          0.7943548560142517,
          0.7857862710952759,
          0.7847782373428345,
          0.8225806355476379,
          0.7827621102333069,
          0.8160282373428345,
          0.7701612710952759,
          0.8160282373428345,
          0.7938507795333862,
          0.796875,
          0.7953628897666931,
          0.8044354915618896,
          0.7822580933570862,
          0.7933467626571655,
          0.8009072542190552,
          0.7993951439857483,
          0.8099798560142517,
          0.7867943644523621,
          0.7888104915618896,
          0.7988911271095276,
          0.8034273982048035,
          0.8145161271095276,
          0.8009072542190552,
          0.7807459831237793,
          0.8084677457809448,
          0.8099798560142517,
          0.8130040168762207,
          0.8059476017951965,
          0.8064516186714172,
          0.8049395084381104,
          0.8215726017951965,
          0.8044354915618896,
          0.7948588728904724,
          0.7973790168762207,
          0.7857862710952759,
          0.8170362710952759,
          0.8135080933570862,
          0.8004032373428345,
          0.8145161271095276,
          0.7963709831237793,
          0.7993951439857483,
          0.8084677457809448,
          0.7862903475761414,
          0.7923387289047241,
          0.7973790168762207,
          0.7998992204666138,
          0.7963709831237793,
          0.788306474685669,
          0.7958669066429138,
          0.7953628897666931,
          0.803931474685669,
          0.8119959831237793,
          0.8130040168762207,
          0.7933467626571655,
          0.8119959831237793,
          0.7862903475761414,
          0.7903226017951965,
          0.8089717626571655,
          0.8064516186714172,
          0.8130040168762207,
          0.7983871102333069,
          0.7963709831237793,
          0.8150201439857483,
          0.7978830933570862,
          0.7938507795333862,
          0.7913306355476379,
          0.8069556355476379,
          0.8165322542190552,
          0.7963709831237793,
          0.8135080933570862,
          0.7983871102333069,
          0.7913306355476379,
          0.8024193644523621,
          0.8019153475761414,
          0.8019153475761414,
          0.8044354915618896,
          0.7993951439857483,
          0.7878023982048035,
          0.8114919066429138,
          0.8024193644523621,
          0.8155242204666138,
          0.8119959831237793,
          0.8256048560142517,
          0.7948588728904724,
          0.8230846524238586,
          0.8104838728904724,
          0.8089717626571655,
          0.8059476017951965,
          0.8024193644523621,
          0.805443525314331,
          0.8064516186714172,
          0.8125,
          0.8074596524238586,
          0.805443525314331,
          0.8049395084381104,
          0.8245967626571655,
          0.8059476017951965,
          0.8029233813285828,
          0.796875,
          0.8089717626571655,
          0.8079637289047241,
          0.7958669066429138,
          0.803931474685669,
          0.805443525314331,
          0.8230846524238586,
          0.821068525314331,
          0.8109878897666931,
          0.7983871102333069,
          0.8104838728904724,
          0.8150201439857483,
          0.8019153475761414,
          0.8074596524238586,
          0.8256048560142517,
          0.7998992204666138,
          0.8109878897666931,
          0.8044354915618896,
          0.8089717626571655,
          0.7998992204666138,
          0.805443525314331,
          0.8059476017951965,
          0.8145161271095276,
          0.8064516186714172,
          0.8029233813285828,
          0.8160282373428345,
          0.8235887289047241,
          0.8125,
          0.8190523982048035,
          0.8074596524238586,
          0.8125,
          0.7913306355476379,
          0.8104838728904724,
          0.8019153475761414,
          0.8296371102333069,
          0.7998992204666138,
          0.819556474685669,
          0.8261088728904724,
          0.8059476017951965,
          0.8125,
          0.8059476017951965,
          0.8064516186714172,
          0.8069556355476379,
          0.8004032373428345,
          0.8024193644523621,
          0.8205645084381104,
          0.8165322542190552,
          0.8084677457809448,
          0.8125,
          0.8251007795333862,
          0.8160282373428345,
          0.8140121102333069,
          0.8074596524238586,
          0.8150201439857483,
          0.796875,
          0.8170362710952759,
          0.8170362710952759,
          0.8180443644523621,
          0.805443525314331,
          0.8104838728904724,
          0.8094757795333862,
          0.8140121102333069,
          0.7948588728904724,
          0.8180443644523621,
          0.8104838728904724,
          0.8044354915618896,
          0.8170362710952759,
          0.8190523982048035,
          0.819556474685669,
          0.8251007795333862,
          0.8180443644523621,
          0.8109878897666931,
          0.8014112710952759,
          0.8004032373428345,
          0.8256048560142517,
          0.805443525314331,
          0.8230846524238586,
          0.8245967626571655,
          0.8225806355476379,
          0.8009072542190552,
          0.8145161271095276,
          0.8235887289047241,
          0.803931474685669,
          0.8059476017951965,
          0.8125,
          0.7983871102333069,
          0.8009072542190552,
          0.8029233813285828,
          0.8125,
          0.8114919066429138,
          0.8125,
          0.8074596524238586,
          0.8084677457809448,
          0.821068525314331,
          0.8024193644523621,
          0.8029233813285828,
          0.8235887289047241,
          0.8200604915618896,
          0.8276209831237793,
          0.8155242204666138,
          0.8150201439857483,
          0.8356854915618896,
          0.8135080933570862,
          0.8180443644523621,
          0.8175403475761414,
          0.8165322542190552,
          0.8220766186714172,
          0.8014112710952759,
          0.8245967626571655,
          0.8114919066429138,
          0.8014112710952759,
          0.8276209831237793,
          0.8165322542190552,
          0.8109878897666931,
          0.805443525314331,
          0.8119959831237793,
          0.8125,
          0.8114919066429138,
          0.8059476017951965,
          0.8109878897666931,
          0.8009072542190552,
          0.8291330933570862,
          0.8190523982048035,
          0.8044354915618896,
          0.803931474685669,
          0.8341733813285828,
          0.8251007795333862,
          0.8225806355476379,
          0.8119959831237793,
          0.8160282373428345,
          0.8220766186714172,
          0.8235887289047241,
          0.8140121102333069,
          0.8371976017951965,
          0.8049395084381104,
          0.8145161271095276,
          0.8240927457809448,
          0.8185483813285828,
          0.8256048560142517,
          0.8119959831237793,
          0.8225806355476379,
          0.8200604915618896,
          0.8160282373428345,
          0.821068525314331,
          0.7978830933570862,
          0.8130040168762207,
          0.8114919066429138,
          0.8140121102333069,
          0.8089717626571655,
          0.8301411271095276,
          0.8291330933570862,
          0.8190523982048035,
          0.8024193644523621,
          0.819556474685669,
          0.8130040168762207,
          0.8145161271095276,
          0.8387096524238586,
          0.8271169066429138,
          0.8119959831237793,
          0.828125,
          0.8235887289047241,
          0.8356854915618896,
          0.8089717626571655,
          0.8009072542190552,
          0.8114919066429138,
          0.8240927457809448,
          0.8145161271095276,
          0.8089717626571655,
          0.8225806355476379,
          0.8140121102333069,
          0.8084677457809448,
          0.8336693644523621,
          0.7973790168762207,
          0.8135080933570862,
          0.8215726017951965,
          0.8130040168762207,
          0.8114919066429138,
          0.7983871102333069,
          0.8185483813285828,
          0.803931474685669,
          0.8130040168762207,
          0.8175403475761414,
          0.8165322542190552,
          0.8296371102333069,
          0.8230846524238586,
          0.835181474685669,
          0.8361895084381104,
          0.8266128897666931,
          0.8412298560142517,
          0.8135080933570862,
          0.8256048560142517,
          0.8296371102333069,
          0.8205645084381104,
          0.8215726017951965,
          0.8321572542190552,
          0.821068525314331,
          0.8104838728904724,
          0.8296371102333069,
          0.7918346524238586,
          0.8215726017951965,
          0.8245967626571655,
          0.8119959831237793,
          0.8245967626571655,
          0.8104838728904724,
          0.828125,
          0.8119959831237793,
          0.8220766186714172,
          0.8170362710952759,
          0.8135080933570862,
          0.8160282373428345,
          0.8225806355476379,
          0.8417338728904724,
          0.8245967626571655,
          0.8215726017951965,
          0.8114919066429138,
          0.8220766186714172,
          0.8346773982048035,
          0.828125,
          0.8185483813285828,
          0.8356854915618896,
          0.8215726017951965,
          0.8225806355476379,
          0.8125,
          0.8417338728904724,
          0.8165322542190552,
          0.8079637289047241,
          0.8114919066429138,
          0.8331653475761414,
          0.8170362710952759,
          0.8160282373428345,
          0.8301411271095276,
          0.8215726017951965,
          0.8235887289047241,
          0.8316532373428345,
          0.8084677457809448,
          0.8135080933570862,
          0.819556474685669,
          0.8160282373428345,
          0.8074596524238586,
          0.8185483813285828,
          0.8286290168762207,
          0.8140121102333069,
          0.8417338728904724,
          0.8336693644523621,
          0.8261088728904724,
          0.805443525314331,
          0.8266128897666931,
          0.8341733813285828,
          0.8200604915618896,
          0.8094757795333862,
          0.8326612710952759,
          0.8225806355476379,
          0.8109878897666931,
          0.819556474685669,
          0.8170362710952759,
          0.8256048560142517,
          0.8170362710952759,
          0.8150201439857483,
          0.8094757795333862,
          0.835181474685669,
          0.8225806355476379,
          0.8256048560142517,
          0.8245967626571655,
          0.8346773982048035,
          0.8125,
          0.8336693644523621,
          0.8084677457809448,
          0.8074596524238586,
          0.8165322542190552,
          0.8135080933570862,
          0.8165322542190552,
          0.8230846524238586,
          0.8392137289047241,
          0.8155242204666138,
          0.8467742204666138,
          0.8245967626571655,
          0.8145161271095276,
          0.8125,
          0.821068525314331,
          0.8402217626571655,
          0.8155242204666138,
          0.8346773982048035,
          0.836693525314331,
          0.8276209831237793,
          0.8130040168762207,
          0.8291330933570862,
          0.8160282373428345,
          0.8180443644523621,
          0.8175403475761414,
          0.8145161271095276,
          0.8044354915618896,
          0.8266128897666931,
          0.8417338728904724,
          0.8140121102333069,
          0.8245967626571655,
          0.8225806355476379,
          0.8155242204666138,
          0.8079637289047241,
          0.8286290168762207,
          0.8225806355476379,
          0.8029233813285828,
          0.8245967626571655,
          0.8306451439857483,
          0.8286290168762207,
          0.8276209831237793,
          0.8064516186714172,
          0.8356854915618896,
          0.8175403475761414,
          0.821068525314331,
          0.8165322542190552,
          0.8165322542190552,
          0.8034273982048035,
          0.8316532373428345,
          0.8225806355476379,
          0.8346773982048035,
          0.8296371102333069,
          0.8170362710952759,
          0.819556474685669,
          0.8185483813285828,
          0.8245967626571655,
          0.8331653475761414,
          0.8266128897666931,
          0.8326612710952759,
          0.8044354915618896,
          0.836693525314331,
          0.8301411271095276,
          0.8392137289047241,
          0.8225806355476379,
          0.8135080933570862,
          0.805443525314331,
          0.8185483813285828,
          0.8205645084381104,
          0.8145161271095276,
          0.8387096524238586,
          0.8165322542190552,
          0.8321572542190552,
          0.8180443644523621,
          0.8407257795333862,
          0.8235887289047241,
          0.8245967626571655,
          0.8180443644523621,
          0.8276209831237793,
          0.8135080933570862,
          0.8119959831237793,
          0.8165322542190552,
          0.8361895084381104,
          0.8316532373428345,
          0.821068525314331,
          0.8427419066429138,
          0.8407257795333862,
          0.8256048560142517,
          0.8336693644523621,
          0.8220766186714172,
          0.8286290168762207,
          0.8276209831237793,
          0.8256048560142517,
          0.8397177457809448,
          0.8256048560142517,
          0.8215726017951965,
          0.8331653475761414,
          0.8104838728904724,
          0.8397177457809448,
          0.8251007795333862,
          0.8225806355476379,
          0.8336693644523621,
          0.8145161271095276,
          0.8190523982048035,
          0.8251007795333862,
          0.8245967626571655,
          0.8346773982048035,
          0.8175403475761414,
          0.8331653475761414,
          0.8326612710952759,
          0.821068525314331,
          0.8215726017951965,
          0.8412298560142517,
          0.828125,
          0.819556474685669,
          0.8331653475761414,
          0.8150201439857483,
          0.8387096524238586,
          0.8316532373428345,
          0.8235887289047241,
          0.8175403475761414,
          0.8371976017951965,
          0.8074596524238586,
          0.8412298560142517,
          0.8089717626571655,
          0.8291330933570862,
          0.8135080933570862,
          0.8361895084381104,
          0.8145161271095276,
          0.8256048560142517,
          0.819556474685669,
          0.8145161271095276,
          0.8135080933570862,
          0.8276209831237793,
          0.819556474685669,
          0.8215726017951965,
          0.8170362710952759,
          0.8135080933570862,
          0.819556474685669,
          0.8155242204666138,
          0.8185483813285828,
          0.8190523982048035,
          0.836693525314331,
          0.8326612710952759,
          0.8286290168762207,
          0.8215726017951965,
          0.8316532373428345,
          0.8225806355476379,
          0.8447580933570862,
          0.8225806355476379,
          0.8477822542190552,
          0.8326612710952759,
          0.8175403475761414,
          0.8326612710952759,
          0.8251007795333862,
          0.8417338728904724,
          0.8266128897666931,
          0.8271169066429138,
          0.8326612710952759,
          0.8266128897666931,
          0.8185483813285828,
          0.8170362710952759,
          0.8150201439857483,
          0.8382056355476379,
          0.8245967626571655,
          0.819556474685669,
          0.8190523982048035,
          0.8225806355476379,
          0.8331653475761414,
          0.8089717626571655,
          0.8235887289047241,
          0.8230846524238586,
          0.8135080933570862,
          0.8220766186714172,
          0.8417338728904724,
          0.8155242204666138,
          0.8200604915618896,
          0.836693525314331,
          0.819556474685669,
          0.8225806355476379,
          0.8271169066429138,
          0.8104838728904724,
          0.8361895084381104,
          0.8296371102333069,
          0.8256048560142517,
          0.8472782373428345,
          0.8286290168762207,
          0.8245967626571655,
          0.8235887289047241,
          0.8286290168762207,
          0.8336693644523621,
          0.8235887289047241,
          0.836693525314331,
          0.8397177457809448,
          0.8215726017951965,
          0.8452621102333069,
          0.8175403475761414,
          0.8528226017951965,
          0.8256048560142517,
          0.8382056355476379,
          0.8296371102333069,
          0.8306451439857483,
          0.8286290168762207,
          0.8291330933570862,
          0.8251007795333862,
          0.8230846524238586,
          0.8316532373428345,
          0.8316532373428345,
          0.8316532373428345,
          0.8271169066429138,
          0.8286290168762207,
          0.8225806355476379,
          0.8321572542190552,
          0.8412298560142517,
          0.8266128897666931,
          0.821068525314331,
          0.8392137289047241,
          0.8326612710952759,
          0.8114919066429138,
          0.8220766186714172,
          0.8316532373428345,
          0.821068525314331,
          0.8392137289047241,
          0.8397177457809448,
          0.8256048560142517,
          0.8140121102333069,
          0.8276209831237793,
          0.8341733813285828,
          0.8286290168762207,
          0.8215726017951965,
          0.8296371102333069,
          0.8145161271095276,
          0.8185483813285828,
          0.8412298560142517,
          0.8266128897666931,
          0.8251007795333862,
          0.8397177457809448,
          0.8276209831237793,
          0.836693525314331,
          0.8261088728904724,
          0.8412298560142517,
          0.8387096524238586,
          0.8155242204666138,
          0.8130040168762207,
          0.8271169066429138,
          0.8346773982048035,
          0.821068525314331,
          0.8165322542190552,
          0.8114919066429138,
          0.8251007795333862,
          0.819556474685669,
          0.8155242204666138,
          0.8361895084381104,
          0.8109878897666931,
          0.8457661271095276,
          0.8185483813285828,
          0.819556474685669,
          0.821068525314331,
          0.8230846524238586,
          0.8125,
          0.8346773982048035,
          0.8311492204666138,
          0.8296371102333069,
          0.8321572542190552,
          0.8220766186714172,
          0.8245967626571655,
          0.8321572542190552,
          0.8266128897666931,
          0.8331653475761414,
          0.8225806355476379,
          0.8387096524238586,
          0.8145161271095276,
          0.8432459831237793,
          0.8109878897666931,
          0.835181474685669,
          0.8321572542190552,
          0.8447580933570862,
          0.8392137289047241,
          0.8160282373428345,
          0.8266128897666931,
          0.819556474685669,
          0.8301411271095276,
          0.8245967626571655,
          0.8175403475761414,
          0.836693525314331,
          0.8225806355476379,
          0.8155242204666138,
          0.8235887289047241,
          0.8256048560142517,
          0.8321572542190552,
          0.8215726017951965,
          0.821068525314331,
          0.8135080933570862,
          0.8397177457809448,
          0.8256048560142517,
          0.8346773982048035,
          0.8336693644523621,
          0.8346773982048035,
          0.8291330933570862,
          0.8145161271095276,
          0.8235887289047241,
          0.8336693644523621,
          0.8145161271095276,
          0.8125,
          0.8311492204666138,
          0.8291330933570862,
          0.8321572542190552,
          0.8266128897666931,
          0.8286290168762207,
          0.8301411271095276,
          0.8346773982048035,
          0.8276209831237793,
          0.8447580933570862,
          0.8240927457809448,
          0.8377016186714172,
          0.8256048560142517,
          0.8261088728904724,
          0.8387096524238586,
          0.8321572542190552,
          0.8361895084381104,
          0.8175403475761414,
          0.8165322542190552,
          0.8064516186714172,
          0.8256048560142517,
          0.8245967626571655,
          0.8387096524238586,
          0.8190523982048035,
          0.8230846524238586,
          0.8276209831237793,
          0.8336693644523621,
          0.8412298560142517,
          0.8336693644523621,
          0.8276209831237793,
          0.8104838728904724,
          0.8145161271095276,
          0.819556474685669,
          0.8266128897666931,
          0.8200604915618896,
          0.8230846524238586,
          0.8215726017951965,
          0.8356854915618896,
          0.8346773982048035,
          0.8377016186714172,
          0.8356854915618896,
          0.8266128897666931,
          0.8205645084381104,
          0.8306451439857483,
          0.8387096524238586,
          0.828125,
          0.8235887289047241,
          0.819556474685669,
          0.8291330933570862,
          0.8346773982048035,
          0.8361895084381104,
          0.8170362710952759,
          0.8230846524238586,
          0.8276209831237793,
          0.8170362710952759,
          0.8135080933570862,
          0.8276209831237793,
          0.8321572542190552,
          0.8316532373428345,
          0.8215726017951965,
          0.8276209831237793,
          0.8341733813285828,
          0.8235887289047241,
          0.8266128897666931,
          0.8306451439857483,
          0.8175403475761414,
          0.8296371102333069,
          0.8306451439857483,
          0.8286290168762207,
          0.8145161271095276,
          0.8346773982048035,
          0.8276209831237793,
          0.8462701439857483,
          0.8276209831237793,
          0.8427419066429138,
          0.8331653475761414,
          0.8306451439857483,
          0.84375,
          0.8447580933570862,
          0.8135080933570862,
          0.8487903475761414,
          0.8256048560142517,
          0.8407257795333862,
          0.8341733813285828,
          0.8301411271095276,
          0.8256048560142517,
          0.8306451439857483,
          0.821068525314331,
          0.8316532373428345
         ]
        },
        {
         "name": "Validation accuracy",
         "type": "scatter",
         "y": [
          0.5,
          0.46396395564079285,
          0.477477490901947,
          0.45945945382118225,
          0.5135135054588318,
          0.5855855941772461,
          0.6261261105537415,
          0.5810810923576355,
          0.5900900959968567,
          0.6216216087341309,
          0.6261261105537415,
          0.5585585832595825,
          0.6486486196517944,
          0.6756756901741028,
          0.6801801919937134,
          0.662162184715271,
          0.7162162065505981,
          0.684684693813324,
          0.6936936974525452,
          0.6801801919937134,
          0.7027027010917664,
          0.6891891956329346,
          0.6936936974525452,
          0.6981981992721558,
          0.7162162065505981,
          0.7297297120094299,
          0.7387387156486511,
          0.7252252101898193,
          0.7342342138290405,
          0.7162162065505981,
          0.7297297120094299,
          0.7387387156486511,
          0.7117117047309875,
          0.7522522807121277,
          0.7027027010917664,
          0.7477477192878723,
          0.7297297120094299,
          0.6981981992721558,
          0.7297297120094299,
          0.7342342138290405,
          0.7522522807121277,
          0.7162162065505981,
          0.707207202911377,
          0.7297297120094299,
          0.7252252101898193,
          0.7387387156486511,
          0.7207207083702087,
          0.7162162065505981,
          0.7117117047309875,
          0.7117117047309875,
          0.7297297120094299,
          0.7207207083702087,
          0.7432432174682617,
          0.7432432174682617,
          0.7252252101898193,
          0.7252252101898193,
          0.7342342138290405,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7297297120094299,
          0.7252252101898193,
          0.7252252101898193,
          0.7387387156486511,
          0.7387387156486511,
          0.707207202911377,
          0.707207202911377,
          0.707207202911377,
          0.7207207083702087,
          0.7387387156486511,
          0.7432432174682617,
          0.7297297120094299,
          0.7162162065505981,
          0.7162162065505981,
          0.7297297120094299,
          0.7207207083702087,
          0.7252252101898193,
          0.7117117047309875,
          0.7297297120094299,
          0.7297297120094299,
          0.707207202911377,
          0.7162162065505981,
          0.7297297120094299,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7207207083702087,
          0.7162162065505981,
          0.7297297120094299,
          0.7207207083702087,
          0.7162162065505981,
          0.7162162065505981,
          0.7432432174682617,
          0.707207202911377,
          0.7162162065505981,
          0.7297297120094299,
          0.7027027010917664,
          0.7207207083702087,
          0.7162162065505981,
          0.7342342138290405,
          0.7027027010917664,
          0.7117117047309875,
          0.7297297120094299,
          0.707207202911377,
          0.6981981992721558,
          0.7117117047309875,
          0.7342342138290405,
          0.7477477192878723,
          0.7297297120094299,
          0.7342342138290405,
          0.7162162065505981,
          0.7342342138290405,
          0.707207202911377,
          0.7297297120094299,
          0.7342342138290405,
          0.7027027010917664,
          0.7162162065505981,
          0.7207207083702087,
          0.7207207083702087,
          0.6936936974525452,
          0.707207202911377,
          0.7342342138290405,
          0.7207207083702087,
          0.7162162065505981,
          0.707207202911377,
          0.7432432174682617,
          0.7027027010917664,
          0.7207207083702087,
          0.7297297120094299,
          0.7027027010917664,
          0.7117117047309875,
          0.7162162065505981,
          0.7387387156486511,
          0.7297297120094299,
          0.7387387156486511,
          0.7387387156486511,
          0.7342342138290405,
          0.7657657861709595,
          0.7342342138290405,
          0.7612612843513489,
          0.7477477192878723,
          0.7522522807121277,
          0.7207207083702087,
          0.7207207083702087,
          0.7432432174682617,
          0.7477477192878723,
          0.6981981992721558,
          0.7477477192878723,
          0.7297297120094299,
          0.7297297120094299,
          0.7477477192878723,
          0.7297297120094299,
          0.7432432174682617,
          0.7162162065505981,
          0.7252252101898193,
          0.7522522807121277,
          0.7387387156486511,
          0.7522522807121277,
          0.7387387156486511,
          0.7342342138290405,
          0.7477477192878723,
          0.7657657861709595,
          0.7252252101898193,
          0.7567567825317383,
          0.7567567825317383,
          0.7297297120094299,
          0.7612612843513489,
          0.7567567825317383,
          0.7522522807121277,
          0.7747747898101807,
          0.7522522807121277,
          0.7432432174682617,
          0.7477477192878723,
          0.7522522807121277,
          0.7522522807121277,
          0.7432432174682617,
          0.7567567825317383,
          0.7522522807121277,
          0.7612612843513489,
          0.7477477192878723,
          0.7567567825317383,
          0.7567567825317383,
          0.7522522807121277,
          0.7297297120094299,
          0.7657657861709595,
          0.7702702879905701,
          0.7522522807121277,
          0.7567567825317383,
          0.7477477192878723,
          0.7387387156486511,
          0.7477477192878723,
          0.7567567825317383,
          0.7567567825317383,
          0.7792792916297913,
          0.7477477192878723,
          0.7747747898101807,
          0.7567567825317383,
          0.7477477192878723,
          0.7837837934494019,
          0.7477477192878723,
          0.7657657861709595,
          0.7657657861709595,
          0.7612612843513489,
          0.7387387156486511,
          0.7657657861709595,
          0.7567567825317383,
          0.7657657861709595,
          0.7747747898101807,
          0.7747747898101807,
          0.7657657861709595,
          0.7657657861709595,
          0.7387387156486511,
          0.7702702879905701,
          0.7657657861709595,
          0.7657657861709595,
          0.7432432174682617,
          0.7567567825317383,
          0.7567567825317383,
          0.7747747898101807,
          0.7612612843513489,
          0.7567567825317383,
          0.7657657861709595,
          0.792792797088623,
          0.7747747898101807,
          0.7837837934494019,
          0.7657657861709595,
          0.7477477192878723,
          0.7477477192878723,
          0.7522522807121277,
          0.7567567825317383,
          0.7567567825317383,
          0.7612612843513489,
          0.7657657861709595,
          0.7522522807121277,
          0.7657657861709595,
          0.7747747898101807,
          0.7477477192878723,
          0.7387387156486511,
          0.7567567825317383,
          0.7567567825317383,
          0.7477477192878723,
          0.7657657861709595,
          0.7747747898101807,
          0.7342342138290405,
          0.7567567825317383,
          0.7657657861709595,
          0.7567567825317383,
          0.7477477192878723,
          0.7657657861709595,
          0.7747747898101807,
          0.7657657861709595,
          0.7387387156486511,
          0.7657657861709595,
          0.7657657861709595,
          0.7747747898101807,
          0.7612612843513489,
          0.7657657861709595,
          0.7657657861709595,
          0.7747747898101807,
          0.7567567825317383,
          0.7612612843513489,
          0.7747747898101807,
          0.7837837934494019,
          0.7837837934494019,
          0.7657657861709595,
          0.7657657861709595,
          0.7747747898101807,
          0.7567567825317383,
          0.7747747898101807,
          0.7657657861709595,
          0.7747747898101807,
          0.7477477192878723,
          0.7837837934494019,
          0.7747747898101807,
          0.7657657861709595,
          0.7477477192878723,
          0.7657657861709595,
          0.7612612843513489,
          0.7522522807121277,
          0.7477477192878723,
          0.7657657861709595,
          0.7477477192878723,
          0.7477477192878723,
          0.7657657861709595,
          0.7837837934494019,
          0.7747747898101807,
          0.7747747898101807,
          0.7567567825317383,
          0.7747747898101807,
          0.7567567825317383,
          0.7657657861709595,
          0.7837837934494019,
          0.7747747898101807,
          0.7837837934494019,
          0.7747747898101807,
          0.7657657861709595,
          0.7477477192878723,
          0.7837837934494019,
          0.7747747898101807,
          0.7657657861709595,
          0.7657657861709595,
          0.7567567825317383,
          0.7657657861709595,
          0.7567567825317383,
          0.7747747898101807,
          0.7567567825317383,
          0.7657657861709595,
          0.7567567825317383,
          0.7657657861709595,
          0.7612612843513489,
          0.7837837934494019,
          0.7837837934494019,
          0.7747747898101807,
          0.7567567825317383,
          0.7387387156486511,
          0.7567567825317383,
          0.7477477192878723,
          0.7657657861709595,
          0.7657657861709595,
          0.7747747898101807,
          0.7387387156486511,
          0.7567567825317383,
          0.7567567825317383,
          0.7477477192878723,
          0.7657657861709595,
          0.7567567825317383,
          0.7657657861709595,
          0.7477477192878723,
          0.7657657861709595,
          0.7837837934494019,
          0.7657657861709595,
          0.7657657861709595,
          0.7567567825317383,
          0.7567567825317383,
          0.7567567825317383,
          0.7657657861709595,
          0.7747747898101807,
          0.7837837934494019,
          0.7567567825317383,
          0.7657657861709595,
          0.7747747898101807,
          0.7477477192878723,
          0.7747747898101807,
          0.7567567825317383,
          0.7657657861709595,
          0.7747747898101807,
          0.7747747898101807,
          0.7747747898101807,
          0.7657657861709595,
          0.7837837934494019,
          0.7837837934494019,
          0.7837837934494019,
          0.7837837934494019,
          0.792792797088623,
          0.7657657861709595,
          0.7657657861709595,
          0.7747747898101807,
          0.7747747898101807,
          0.7747747898101807,
          0.7747747898101807,
          0.7747747898101807,
          0.7837837934494019,
          0.7747747898101807,
          0.7657657861709595,
          0.7747747898101807,
          0.7837837934494019,
          0.7837837934494019,
          0.7837837934494019,
          0.7747747898101807,
          0.7837837934494019,
          0.7837837934494019,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.7837837934494019,
          0.792792797088623,
          0.7747747898101807,
          0.7837837934494019,
          0.7837837934494019,
          0.792792797088623,
          0.7837837934494019,
          0.792792797088623,
          0.8018018007278442,
          0.8018018007278442,
          0.792792797088623,
          0.792792797088623,
          0.7657657861709595,
          0.7747747898101807,
          0.7747747898101807,
          0.7837837934494019,
          0.7747747898101807,
          0.7747747898101807,
          0.7837837934494019,
          0.7657657861709595,
          0.7747747898101807,
          0.8018018007278442,
          0.792792797088623,
          0.792792797088623,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.792792797088623,
          0.7747747898101807,
          0.792792797088623,
          0.7837837934494019,
          0.792792797088623,
          0.792792797088623,
          0.792792797088623,
          0.792792797088623,
          0.7837837934494019,
          0.792792797088623,
          0.7747747898101807,
          0.7837837934494019,
          0.7837837934494019,
          0.7747747898101807,
          0.7837837934494019,
          0.7837837934494019,
          0.7747747898101807,
          0.7837837934494019,
          0.7837837934494019,
          0.792792797088623,
          0.792792797088623,
          0.792792797088623,
          0.792792797088623,
          0.7837837934494019,
          0.792792797088623,
          0.8018018007278442,
          0.7837837934494019,
          0.7747747898101807,
          0.792792797088623,
          0.8018018007278442,
          0.7837837934494019,
          0.792792797088623,
          0.7747747898101807,
          0.7747747898101807,
          0.8108108043670654,
          0.7837837934494019,
          0.7747747898101807,
          0.792792797088623,
          0.792792797088623,
          0.792792797088623,
          0.792792797088623,
          0.7972972989082336,
          0.792792797088623,
          0.792792797088623,
          0.7747747898101807,
          0.7747747898101807,
          0.8108108043670654,
          0.8018018007278442,
          0.7747747898101807,
          0.7747747898101807,
          0.7657657861709595,
          0.7837837934494019,
          0.8018018007278442,
          0.7837837934494019,
          0.792792797088623,
          0.8108108043670654,
          0.7747747898101807,
          0.8018018007278442,
          0.792792797088623,
          0.792792797088623,
          0.8198198080062866,
          0.792792797088623,
          0.792792797088623,
          0.8198198080062866,
          0.8198198080062866,
          0.8018018007278442,
          0.7837837934494019,
          0.7747747898101807,
          0.7837837934494019,
          0.8108108043670654,
          0.8108108043670654,
          0.792792797088623,
          0.8018018007278442,
          0.8018018007278442,
          0.7837837934494019,
          0.8018018007278442,
          0.8198198080062866,
          0.792792797088623,
          0.7747747898101807,
          0.792792797088623,
          0.792792797088623,
          0.8108108043670654,
          0.7882882952690125,
          0.792792797088623,
          0.8018018007278442,
          0.7837837934494019,
          0.7747747898101807,
          0.792792797088623,
          0.7837837934494019,
          0.792792797088623,
          0.792792797088623,
          0.792792797088623,
          0.792792797088623,
          0.792792797088623,
          0.792792797088623,
          0.7972972989082336,
          0.792792797088623,
          0.8018018007278442,
          0.792792797088623,
          0.7747747898101807,
          0.8198198080062866,
          0.8063063025474548,
          0.792792797088623,
          0.792792797088623,
          0.792792797088623,
          0.792792797088623,
          0.8018018007278442,
          0.7837837934494019,
          0.7837837934494019,
          0.7837837934494019,
          0.7747747898101807,
          0.7747747898101807,
          0.7747747898101807,
          0.7837837934494019,
          0.7837837934494019,
          0.7837837934494019,
          0.7657657861709595,
          0.7837837934494019,
          0.7837837934494019,
          0.7747747898101807,
          0.7837837934494019,
          0.7837837934494019,
          0.7837837934494019,
          0.7837837934494019,
          0.7837837934494019,
          0.792792797088623,
          0.8198198080062866,
          0.792792797088623,
          0.8108108043670654,
          0.7837837934494019,
          0.8018018007278442,
          0.7882882952690125,
          0.8108108043670654,
          0.792792797088623,
          0.8108108043670654,
          0.8108108043670654,
          0.792792797088623,
          0.8108108043670654,
          0.792792797088623,
          0.8198198080062866,
          0.8108108043670654,
          0.792792797088623,
          0.8108108043670654,
          0.7972972989082336,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8108108043670654,
          0.792792797088623,
          0.7837837934494019,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8108108043670654,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.792792797088623,
          0.8018018007278442,
          0.8108108043670654,
          0.8198198080062866,
          0.815315306186676,
          0.8198198080062866,
          0.8108108043670654,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8198198080062866,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8018018007278442,
          0.8108108043670654,
          0.8108108043670654,
          0.792792797088623,
          0.8198198080062866,
          0.792792797088623,
          0.8198198080062866,
          0.8198198080062866,
          0.8108108043670654,
          0.8018018007278442,
          0.8198198080062866,
          0.8198198080062866,
          0.7972972989082336,
          0.7972972989082336,
          0.8018018007278442,
          0.8198198080062866,
          0.792792797088623,
          0.8108108043670654,
          0.8018018007278442,
          0.8108108043670654,
          0.815315306186676,
          0.8108108043670654,
          0.8018018007278442,
          0.8063063025474548,
          0.8108108043670654,
          0.792792797088623,
          0.8108108043670654,
          0.8018018007278442,
          0.8108108043670654,
          0.8108108043670654,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.792792797088623,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8018018007278442,
          0.792792797088623,
          0.792792797088623,
          0.792792797088623,
          0.792792797088623,
          0.8108108043670654,
          0.8108108043670654,
          0.8018018007278442,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8198198080062866,
          0.8108108043670654,
          0.792792797088623,
          0.8108108043670654,
          0.8198198080062866,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8018018007278442,
          0.8018018007278442,
          0.8108108043670654,
          0.8198198080062866,
          0.8108108043670654,
          0.8018018007278442,
          0.8018018007278442,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8018018007278442,
          0.8108108043670654,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8018018007278442,
          0.8018018007278442,
          0.792792797088623,
          0.8018018007278442,
          0.8198198080062866,
          0.8108108043670654,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8108108043670654,
          0.8018018007278442,
          0.8108108043670654,
          0.8018018007278442,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8108108043670654,
          0.8198198080062866,
          0.8108108043670654,
          0.8108108043670654,
          0.792792797088623,
          0.792792797088623,
          0.792792797088623,
          0.8108108043670654,
          0.8198198080062866,
          0.792792797088623,
          0.8108108043670654,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8108108043670654,
          0.8018018007278442,
          0.8018018007278442,
          0.792792797088623,
          0.8018018007278442,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8018018007278442,
          0.8198198080062866,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8108108043670654,
          0.792792797088623,
          0.8108108043670654,
          0.792792797088623,
          0.8198198080062866,
          0.8198198080062866,
          0.7972972989082336,
          0.8018018007278442,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8018018007278442,
          0.8018018007278442,
          0.8108108043670654,
          0.8108108043670654,
          0.8018018007278442,
          0.792792797088623,
          0.8108108043670654,
          0.8108108043670654,
          0.815315306186676,
          0.8018018007278442,
          0.8063063025474548,
          0.8108108043670654,
          0.8198198080062866,
          0.8108108043670654,
          0.8018018007278442,
          0.8018018007278442,
          0.8198198080062866,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8108108043670654,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8108108043670654,
          0.8198198080062866,
          0.8108108043670654,
          0.8018018007278442,
          0.8198198080062866,
          0.8018018007278442,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.7837837934494019,
          0.8018018007278442,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8018018007278442,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8108108043670654,
          0.8198198080062866,
          0.8108108043670654,
          0.8108108043670654,
          0.815315306186676,
          0.8108108043670654,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.815315306186676,
          0.8198198080062866,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8108108043670654,
          0.8108108043670654,
          0.8063063025474548,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8018018007278442,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8018018007278442,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8018018007278442,
          0.8018018007278442,
          0.8063063025474548,
          0.8108108043670654,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8108108043670654,
          0.8108108043670654,
          0.8198198080062866,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8108108043670654,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866,
          0.8198198080062866
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Accuracy MLP"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Accuracy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train F1 score",
         "type": "scatter",
         "y": [
          0.5110042095184326,
          0.45043063163757324,
          0.39336398243904114,
          0.3898705244064331,
          0.3609192669391632,
          0.37545788288116455,
          0.34956151247024536,
          0.37806758284568787,
          0.4717881679534912,
          0.5042478442192078,
          0.5598379969596863,
          0.5987104177474976,
          0.5891208648681641,
          0.5862774848937988,
          0.6315308213233948,
          0.6305062174797058,
          0.6208101511001587,
          0.6271727085113525,
          0.6340848803520203,
          0.6255252361297607,
          0.6677224636077881,
          0.6244784593582153,
          0.6713688373565674,
          0.6412656903266907,
          0.6477093696594238,
          0.6658217310905457,
          0.6698970794677734,
          0.6895129680633545,
          0.6646091938018799,
          0.652550220489502,
          0.6566711664199829,
          0.6661556363105774,
          0.6828591823577881,
          0.6628354787826538,
          0.6947216987609863,
          0.6982611417770386,
          0.6828015446662903,
          0.7036200761795044,
          0.6823580265045166,
          0.6874039769172668,
          0.6811235547065735,
          0.6951735615730286,
          0.7108678817749023,
          0.6983904242515564,
          0.6795492172241211,
          0.6922962069511414,
          0.697321891784668,
          0.6803480386734009,
          0.7007080912590027,
          0.7209547162055969,
          0.6972140073776245,
          0.7055500149726868,
          0.6993800401687622,
          0.6975929737091064,
          0.703519344329834,
          0.6908236742019653,
          0.7080075740814209,
          0.7167671918869019,
          0.7073745727539062,
          0.7087435722351074,
          0.6886160373687744,
          0.7128028869628906,
          0.7238262891769409,
          0.7253271341323853,
          0.7230418920516968,
          0.7127065658569336,
          0.7263206839561462,
          0.7276785373687744,
          0.7344883680343628,
          0.733009934425354,
          0.7146387100219727,
          0.701001763343811,
          0.7229512333869934,
          0.7328938245773315,
          0.7270998954772949,
          0.7270725965499878,
          0.7334355115890503,
          0.7402937412261963,
          0.7435709834098816,
          0.7461956143379211,
          0.7315641641616821,
          0.7204606533050537,
          0.7203818559646606,
          0.7316622734069824,
          0.726299524307251,
          0.7150168418884277,
          0.7433507442474365,
          0.7434934973716736,
          0.7396484017372131,
          0.7353825569152832,
          0.7118157148361206,
          0.7253271341323853,
          0.7162470817565918,
          0.727695882320404,
          0.7352672815322876,
          0.7477829456329346,
          0.7537079453468323,
          0.7289750576019287,
          0.7292994260787964,
          0.7439268231391907,
          0.7443358898162842,
          0.7548008561134338,
          0.7432994842529297,
          0.7546610832214355,
          0.7313733100891113,
          0.7695280909538269,
          0.7324057817459106,
          0.7796363830566406,
          0.7535022497177124,
          0.7565554976463318,
          0.7609062194824219,
          0.7415151596069336,
          0.7407627105712891,
          0.7493632435798645,
          0.74098801612854,
          0.7411105036735535,
          0.7470585107803345,
          0.7781528234481812,
          0.748888373374939,
          0.7473419904708862,
          0.768068790435791,
          0.763411819934845,
          0.7611638307571411,
          0.7415562868118286,
          0.7676010131835938,
          0.7649201154708862,
          0.7550287246704102,
          0.7569020986557007,
          0.7729915380477905,
          0.7312255501747131,
          0.7526418566703796,
          0.7800407409667969,
          0.7820019721984863,
          0.7696660161018372,
          0.7811110019683838,
          0.7522627115249634,
          0.7688316106796265,
          0.7589194774627686,
          0.765326738357544,
          0.7647573947906494,
          0.7738617658615112,
          0.7604555487632751,
          0.7912371158599854,
          0.7655801773071289,
          0.7674560546875,
          0.7760995626449585,
          0.7729027271270752,
          0.7716427445411682,
          0.782031238079071,
          0.7809453010559082,
          0.7810361385345459,
          0.7962881922721863,
          0.7565991282463074,
          0.7830260396003723,
          0.7538070678710938,
          0.7971809506416321,
          0.7748884558677673,
          0.7707778215408325,
          0.7737069129943848,
          0.7879953980445862,
          0.7658205628395081,
          0.7725844383239746,
          0.78285813331604,
          0.7759765386581421,
          0.7920335531234741,
          0.7552684545516968,
          0.80010986328125,
          0.7879617214202881,
          0.7760995626449585,
          0.7849831581115723,
          0.7892783284187317,
          0.7889178991317749,
          0.7811857461929321,
          0.7808756828308105,
          0.7857888340950012,
          0.790268063545227,
          0.8032592535018921,
          0.7862347364425659,
          0.8011038303375244,
          0.792270302772522,
          0.7970653772354126,
          0.7706096172332764,
          0.7754942178726196,
          0.7888405323028564,
          0.7989815473556519,
          0.7891085147857666,
          0.7738957405090332,
          0.778989315032959,
          0.787723183631897,
          0.7768728733062744,
          0.787090539932251,
          0.7893040180206299,
          0.7882367968559265,
          0.7798840403556824,
          0.7868598103523254,
          0.7890810966491699,
          0.7727973461151123,
          0.7939856052398682,
          0.7948206067085266,
          0.79696124792099,
          0.785607099533081,
          0.7870627641677856,
          0.785233199596405,
          0.7868198156356812,
          0.7829959988594055,
          0.7888405323028564,
          0.7821084260940552,
          0.7819709777832031,
          0.7880859375,
          0.7850146293640137,
          0.7922170758247375,
          0.7951078414916992,
          0.7840204834938049,
          0.7683337926864624,
          0.7920945286750793,
          0.798943817615509,
          0.7809453010559082,
          0.795232892036438,
          0.7970322966575623,
          0.7929577827453613,
          0.7796822786331177,
          0.7822729349136353,
          0.7778275012969971,
          0.7992676496505737,
          0.7982690334320068,
          0.7861651182174683,
          0.8212672472000122,
          0.7992676496505737,
          0.788711428642273,
          0.785976231098175,
          0.8062499761581421,
          0.78516685962677,
          0.7898707389831543,
          0.812279462814331,
          0.8032816648483276,
          0.7865433096885681,
          0.7920647859573364,
          0.7746058702468872,
          0.78565514087677,
          0.8013384342193604,
          0.8031824827194214,
          0.7809773087501526,
          0.8031231164932251,
          0.7899460792541504,
          0.7838490009307861,
          0.8012640476226807,
          0.7958930730819702,
          0.7941665649414062,
          0.7860085368156433,
          0.7850444316864014,
          0.8215317726135254,
          0.7829306125640869,
          0.8151091933250427,
          0.7692599296569824,
          0.817517876625061,
          0.792841374874115,
          0.79696124792099,
          0.7950127124786377,
          0.8052993416786194,
          0.7816109657287598,
          0.7941908240318298,
          0.8008849620819092,
          0.7990846633911133,
          0.8103535771369934,
          0.7857888340950012,
          0.7891342639923096,
          0.7987306118011475,
          0.8019936084747314,
          0.8154414892196655,
          0.8011913299560547,
          0.7807578444480896,
          0.8081238269805908,
          0.8091498613357544,
          0.8133257627487183,
          0.8063383102416992,
          0.80622398853302,
          0.8042795658111572,
          0.8214403390884399,
          0.8041170239448547,
          0.7941405773162842,
          0.7972288131713867,
          0.7846676111221313,
          0.8163859844207764,
          0.813188910484314,
          0.800168514251709,
          0.8144678473472595,
          0.7955198287963867,
          0.8002204895019531,
          0.8080901503562927,
          0.7858297824859619,
          0.7941405773162842,
          0.7970967292785645,
          0.7980587482452393,
          0.7951878309249878,
          0.7878096699714661,
          0.7970653772354126,
          0.7947770953178406,
          0.803209662437439,
          0.8123902082443237,
          0.8123283386230469,
          0.7923997640609741,
          0.8133257627487183,
          0.7859422564506531,
          0.787723183631897,
          0.8092104196548462,
          0.8059183359146118,
          0.8133014440536499,
          0.7981500029563904,
          0.796330451965332,
          0.8142139911651611,
          0.7979896664619446,
          0.7930588722229004,
          0.7908203601837158,
          0.8061040639877319,
          0.8154249787330627,
          0.7961845397949219,
          0.8142980337142944,
          0.7980908155441284,
          0.7910076379776001,
          0.8023679256439209,
          0.8023221492767334,
          0.8009644746780396,
          0.8052771687507629,
          0.7977426052093506,
          0.7879953980445862,
          0.8123046159744263,
          0.8030573129653931,
          0.8153663873672485,
          0.8111693859100342,
          0.8254115581512451,
          0.7947770953178406,
          0.8244361877441406,
          0.8113721013069153,
          0.8081238269805908,
          0.8069887161254883,
          0.8007990121841431,
          0.805997371673584,
          0.8063878417015076,
          0.8112831115722656,
          0.8082424402236938,
          0.8051725625991821,
          0.8052771687507629,
          0.8243904709815979,
          0.8052279949188232,
          0.8030220866203308,
          0.7980587482452393,
          0.8082683086395264,
          0.8070626854896545,
          0.7960716485977173,
          0.8041480779647827,
          0.805077314376831,
          0.822230875492096,
          0.8204214572906494,
          0.8084396719932556,
          0.7990175485610962,
          0.8098780512809753,
          0.8143229484558105,
          0.8018749952316284,
          0.8069887161254883,
          0.825564980506897,
          0.8009644746780396,
          0.8111032247543335,
          0.8033024668693542,
          0.8090097904205322,
          0.8009644746780396,
          0.8040140867233276,
          0.8051424026489258,
          0.8142139911651611,
          0.806136429309845,
          0.8020645380020142,
          0.8164061307907104,
          0.8233429789543152,
          0.8121632933616638,
          0.818488597869873,
          0.8072949647903442,
          0.8122527003288269,
          0.7917665243148804,
          0.8101102709770203,
          0.8021869659423828,
          0.8295288681983948,
          0.7991156578063965,
          0.819475531578064,
          0.8263305425643921,
          0.8061963319778442,
          0.8134077787399292,
          0.8063383102416992,
          0.8062499761581421,
          0.8073561191558838,
          0.7999348044395447,
          0.8031535744667053,
          0.8202423453330994,
          0.8172280788421631,
          0.8071302175521851,
          0.8123283386230469,
          0.8254755735397339,
          0.8161705732345581,
          0.8133895397186279,
          0.8069493770599365,
          0.8154699802398682,
          0.7961314916610718,
          0.8184590339660645,
          0.8162902593612671,
          0.8173381686210632,
          0.8051424026489258,
          0.8103103637695312,
          0.8091498613357544,
          0.814243495464325,
          0.7961024641990662,
          0.8171297907829285,
          0.8103905916213989,
          0.8032352328300476,
          0.81613689661026,
          0.8184236288070679,
          0.8193800449371338,
          0.8245390057563782,
          0.8172280788421631,
          0.8092647790908813,
          0.8009254932403564,
          0.800168514251709,
          0.8254556655883789,
          0.8040499687194824,
          0.8233688473701477,
          0.824365496635437,
          0.8223958015441895,
          0.8002204895019531,
          0.8141504526138306,
          0.8235368728637695,
          0.8041170239448547,
          0.8061963319778442,
          0.8121946454048157,
          0.7980587482452393,
          0.7990175485610962,
          0.8017877340316772,
          0.8112831115722656,
          0.8109523057937622,
          0.8120959997177124,
          0.8073732852935791,
          0.8080901503562927,
          0.8212963342666626,
          0.8022135496139526,
          0.8029851913452148,
          0.8233429789543152,
          0.8202423453330994,
          0.8285593390464783,
          0.8154069185256958,
          0.8152385950088501,
          0.835665225982666,
          0.813188910484314,
          0.8184037208557129,
          0.8173619508743286,
          0.8163164854049683,
          0.8221642971038818,
          0.8011637926101685,
          0.8254343271255493,
          0.8111370801925659,
          0.8008849620819092,
          0.8273541927337646,
          0.8174242973327637,
          0.8111032247543335,
          0.8053199052810669,
          0.8123046159744263,
          0.8120599389076233,
          0.8110678195953369,
          0.806372880935669,
          0.8111032247543335,
          0.8000097274780273,
          0.8285447359085083,
          0.8184037208557129,
          0.8042055368423462,
          0.8041776418685913,
          0.8346606492996216,
          0.8245390057563782,
          0.8223466873168945,
          0.8121632933616638,
          0.81634122133255,
          0.8224767446517944,
          0.823121190071106,
          0.814116358757019,
          0.8365633487701416,
          0.8041480779647827,
          0.8143229484558105,
          0.8254556655883789,
          0.8183091878890991,
          0.8254940509796143,
          0.8112000823020935,
          0.8223466873168945,
          0.8203775882720947,
          0.8154414892196655,
          0.8202423453330994,
          0.7978735566139221,
          0.8111693859100342,
          0.8112293481826782,
          0.8131569623947144,
          0.8090472221374512,
          0.8295454978942871,
          0.8295288681983948,
          0.8181906938552856,
          0.8027762770652771,
          0.8202108144760132,
          0.8120959997177124,
          0.814116358757019,
          0.8385810852050781,
          0.8275913000106812,
          0.8119006156921387,
          0.8272655010223389,
          0.8234938383102417,
          0.8354566097259521,
          0.8070626854896545,
          0.8001400232315063,
          0.8122527003288269,
          0.8244361877441406,
          0.8140808343887329,
          0.8068660497665405,
          0.8224391937255859,
          0.8140436410903931,
          0.8081238269805908,
          0.8325080871582031,
          0.7966597080230713,
          0.8133485317230225,
          0.8213239908218384,
          0.812279462814331,
          0.8108227252960205,
          0.7978315949440002,
          0.8173129558563232,
          0.8038966059684753,
          0.812451183795929,
          0.8172578811645508,
          0.8162626028060913,
          0.8296161890029907,
          0.8223719596862793,
          0.8356118202209473,
          0.836371660232544,
          0.8264541029930115,
          0.8414888381958008,
          0.8133014440536499,
          0.8254343271255493,
          0.8295108079910278,
          0.8204002380371094,
          0.8213239908218384,
          0.831577718257904,
          0.8204002380371094,
          0.8101435899734497,
          0.8294483423233032,
          0.7897886037826538,
          0.8213748931884766,
          0.824365496635437,
          0.8111032247543335,
          0.8244761228561401,
          0.8100011348724365,
          0.8284924030303955,
          0.8121632933616638,
          0.8215317726135254,
          0.8159028887748718,
          0.8132482767105103,
          0.8152385950088501,
          0.8223466873168945,
          0.8406324982643127,
          0.8244940638542175,
          0.8215070962905884,
          0.8113076686859131,
          0.8214199542999268,
          0.8343805074691772,
          0.8273541927337646,
          0.818359375,
          0.8355449438095093,
          0.821204662322998,
          0.8224586248397827,
          0.8113306760787964,
          0.8415985107421875,
          0.8163642883300781,
          0.8078107833862305,
          0.8110308647155762,
          0.8336204886436462,
          0.8171968460083008,
          0.815319836139679,
          0.8295108079910278,
          0.8224767446517944,
          0.8232868313789368,
          0.8314434289932251,
          0.8081558346748352,
          0.8131235241889954,
          0.8204002380371094,
          0.8152085542678833,
          0.8070971965789795,
          0.8184236288070679,
          0.8284721374511719,
          0.8144972920417786,
          0.8416758179664612,
          0.8333847522735596,
          0.825245201587677,
          0.8051106333732605,
          0.8252766132354736,
          0.8335636854171753,
          0.8192770481109619,
          0.8090472221374512,
          0.8326177597045898,
          0.8223466873168945,
          0.8103103637695312,
          0.8194020986557007,
          0.8173129558563232,
          0.8253064155578613,
          0.8174585103988647,
          0.8154249787330627,
          0.8090828657150269,
          0.8355034589767456,
          0.8221642971038818,
          0.8253874778747559,
          0.8244361877441406,
          0.8335270881652832,
          0.8111370801925659,
          0.8335636854171753,
          0.8080182075500488,
          0.8068660497665405,
          0.8162626028060913,
          0.814243495464325,
          0.8159869909286499,
          0.8235368728637695,
          0.8384726643562317,
          0.8142715692520142,
          0.8467236757278442,
          0.8243110775947571,
          0.8151769042015076,
          0.8111370801925659,
          0.8201779127120972,
          0.8406163454055786,
          0.8152085542678833,
          0.8344830274581909,
          0.8365813493728638,
          0.8275114297866821,
          0.812279462814331,
          0.8294705152511597,
          0.8161705732345581,
          0.818488597869873,
          0.8171297907829285,
          0.8141831159591675,
          0.804084300994873,
          0.8262392282485962,
          0.8416628837585449,
          0.8131235241889954,
          0.8234162330627441,
          0.8224391937255859,
          0.8152085542678833,
          0.8081558346748352,
          0.8284721374511719,
          0.8223466873168945,
          0.8029466271400452,
          0.8243904709815979,
          0.830396294593811,
          0.828528642654419,
          0.827406108379364,
          0.8060345649719238,
          0.83552485704422,
          0.8174242973327637,
          0.8203535079956055,
          0.8162626028060913,
          0.8164248466491699,
          0.8018749952316284,
          0.831577718257904,
          0.8224391937255859,
          0.8346102237701416,
          0.8292204141616821,
          0.8173129558563232,
          0.8193315863609314,
          0.8172578811645508,
          0.8243110775947571,
          0.8334851264953613,
          0.8263842463493347,
          0.8324153423309326,
          0.80385422706604,
          0.8365633487701416,
          0.830528736114502,
          0.8385987877845764,
          0.8224391937255859,
          0.813219428062439,
          0.8050057888031006,
          0.8182225227355957,
          0.819216787815094,
          0.8143463134765625,
          0.8385416865348816,
          0.8162027597427368,
          0.8315284252166748,
          0.8182528614997864,
          0.8406940698623657,
          0.8230447769165039,
          0.824365496635437,
          0.8183091878890991,
          0.827430009841919,
          0.8130137920379639,
          0.8123046159744263,
          0.8164061307907104,
          0.8355636596679688,
          0.8313926458358765,
          0.8214199542999268,
          0.8426498174667358,
          0.8406474590301514,
          0.8254556655883789,
          0.8335460424423218,
          0.8222620487213135,
          0.8296078443527222,
          0.8285447359085083,
          0.8255409002304077,
          0.8395401835441589,
          0.8254115581512451,
          0.821398138999939,
          0.8324406147003174,
          0.8101102709770203,
          0.8396706581115723,
          0.8254940509796143,
          0.8223199248313904,
          0.8335067629814148,
          0.8141504526138306,
          0.818157434463501,
          0.8254755735397339,
          0.824365496635437,
          0.8344830274581909,
          0.8173381686210632,
          0.8325462341308594,
          0.8324869871139526,
          0.8204002380371094,
          0.821204662322998,
          0.8405384421348572,
          0.8284505009651184,
          0.8192477226257324,
          0.8335270881652832,
          0.8152672052383423,
          0.8386303186416626,
          0.8315284252166748,
          0.8233688473701477,
          0.8174421787261963,
          0.8373960256576538,
          0.8071914315223694,
          0.8415365815162659,
          0.8082424402236938,
          0.8279569745063782,
          0.8130885362625122,
          0.8355034589767456,
          0.8141831159591675,
          0.8254115581512451,
          0.8193050622940063,
          0.813219428062439,
          0.8133014440536499,
          0.827406108379364,
          0.8193315863609314,
          0.8211711645126343,
          0.8162626028060913,
          0.8131235241889954,
          0.8192477226257324,
          0.8152672052383423,
          0.8184037208557129,
          0.8192770481109619,
          0.8366131782531738,
          0.8324153423309326,
          0.8285844326019287,
          0.8212672472000122,
          0.8316147327423096,
          0.8213502168655396,
          0.8447422981262207,
          0.8224586248397827,
          0.8477004170417786,
          0.8335270881652832,
          0.8171968460083008,
          0.8323887586593628,
          0.8242818117141724,
          0.8416487574577332,
          0.8260976076126099,
          0.827430009841919,
          0.8324153423309326,
          0.8264937400817871,
          0.8184037208557129,
          0.8173619508743286,
          0.8149130344390869,
          0.8376288414001465,
          0.8244569301605225,
          0.8194417953491211,
          0.8194020986557007,
          0.8224767446517944,
          0.8333847522735596,
          0.8080901503562927,
          0.8234938383102417,
          0.8222917318344116,
          0.8132482767105103,
          0.8212963342666626,
          0.8415133953094482,
          0.8152672052383423,
          0.8201779127120972,
          0.8366398215293884,
          0.8192770481109619,
          0.8222917318344116,
          0.8263015747070312,
          0.8100011348724365,
          0.836427628993988,
          0.8295108079910278,
          0.8254115581512451,
          0.8475925326347351,
          0.828528642654419,
          0.824365496635437,
          0.8232566118240356,
          0.8283771872520447,
          0.8335270881652832,
          0.8233932256698608,
          0.8365014791488647,
          0.8395805954933167,
          0.8212963342666626,
          0.8446342945098877,
          0.8171968460083008,
          0.8527740836143494,
          0.825245201587677,
          0.8385987877845764,
          0.829491376876831,
          0.8304687738418579,
          0.8282911777496338,
          0.8284505009651184,
          0.8243390321731567,
          0.8223958015441895,
          0.8314434289932251,
          0.8304460048675537,
          0.8314886689186096,
          0.827430009841919,
          0.8283213376998901,
          0.8224767446517944,
          0.8325278759002686,
          0.8415791392326355,
          0.8264937400817871,
          0.8202108144760132,
          0.8385987877845764,
          0.8325932621955872,
          0.8111370801925659,
          0.8223199248313904,
          0.8314667344093323,
          0.821204662322998,
          0.8395611047744751,
          0.8396458625793457,
          0.8255409002304077,
          0.813188910484314,
          0.8275436758995056,
          0.8345637917518616,
          0.8293455839157104,
          0.8212367296218872,
          0.8295606970787048,
          0.8148239850997925,
          0.818157434463501,
          0.8415133953094482,
          0.8261712789535522,
          0.8244569301605225,
          0.8395805954933167,
          0.8275436758995056,
          0.836651086807251,
          0.8264937400817871,
          0.8405384421348572,
          0.8375627994537354,
          0.8152385950088501,
          0.8131569623947144,
          0.8264089822769165,
          0.8355970978736877,
          0.8202108144760132,
          0.81634122133255,
          0.8118115663528442,
          0.8253348469734192,
          0.8202722072601318,
          0.8153438568115234,
          0.8354566097259521,
          0.8112569451332092,
          0.8456680774688721,
          0.8184745907783508,
          0.819184422492981,
          0.8204411864280701,
          0.8223199248313904,
          0.8119426965713501,
          0.8345456123352051,
          0.8314886689186096,
          0.8294247388839722,
          0.8324153423309326,
          0.8214925527572632,
          0.8245104551315308,
          0.8324869871139526,
          0.8263842463493347,
          0.8323314189910889,
          0.8234162330627441,
          0.8386861085891724,
          0.8140436410903931,
          0.8436163663864136,
          0.8111693859100342,
          0.8345260620117188,
          0.8315092325210571,
          0.8446342945098877,
          0.8386303186416626,
          0.8152672052383423,
          0.8264089822769165,
          0.8194901943206787,
          0.8294483423233032,
          0.8241851329803467,
          0.8174585103988647,
          0.8365234732627869,
          0.8223719596862793,
          0.815319836139679,
          0.8235368728637695,
          0.8253874778747559,
          0.8315914869308472,
          0.8213502168655396,
          0.820327877998352,
          0.813188910484314,
          0.8393872380256653,
          0.8263305425643921,
          0.834434449672699,
          0.8333847522735596,
          0.8345806002616882,
          0.8284924030303955,
          0.8142980337142944,
          0.8234162330627441,
          0.8336204886436462,
          0.8153438568115234,
          0.8122244477272034,
          0.8314667344093323,
          0.8293455839157104,
          0.8325080871582031,
          0.8264541029930115,
          0.8283499479293823,
          0.8305894136428833,
          0.8344594240188599,
          0.8273807764053345,
          0.8446159362792969,
          0.8244761228561401,
          0.8374239206314087,
          0.8252766132354736,
          0.8265423774719238,
          0.8386151790618896,
          0.831577718257904,
          0.8365014791488647,
          0.8163164854049683,
          0.8162902593612671,
          0.80607008934021,
          0.8252766132354736,
          0.8243110775947571,
          0.8385987877845764,
          0.8180862665176392,
          0.8234162330627441,
          0.827430009841919,
          0.833594799041748,
          0.8414074778556824,
          0.8335636854171753,
          0.8264322280883789,
          0.810205340385437,
          0.8140050768852234,
          0.8193565607070923,
          0.8264541029930115,
          0.819216787815094,
          0.8232566118240356,
          0.8212672472000122,
          0.834434449672699,
          0.8345637917518616,
          0.8375430107116699,
          0.8355636596679688,
          0.8264541029930115,
          0.8205287456512451,
          0.830396294593811,
          0.8386565446853638,
          0.8284505009651184,
          0.8233932256698608,
          0.8193800449371338,
          0.8293733596801758,
          0.834408164024353,
          0.8353466391563416,
          0.8162626028060913,
          0.8232249021530151,
          0.8275282382965088,
          0.8171968460083008,
          0.813219428062439,
          0.8264541029930115,
          0.8324644565582275,
          0.8315284252166748,
          0.8212672472000122,
          0.827406108379364,
          0.8333847522735596,
          0.8234766125679016,
          0.826474666595459,
          0.830396294593811,
          0.8172578811645508,
          0.8293997645378113,
          0.8304460048675537,
          0.829491376876831,
          0.814407467842102,
          0.8345260620117188,
          0.827430009841919,
          0.8467342853546143,
          0.8273259997367859,
          0.8425780534744263,
          0.8335270881652832,
          0.830545961856842,
          0.8425780534744263,
          0.8456831574440002,
          0.8131569623947144,
          0.8486697673797607,
          0.8252124190330505,
          0.8405598402023315,
          0.8333562612533569,
          0.8304460048675537,
          0.8252766132354736,
          0.8304901719093323,
          0.8202722072601318,
          0.8314667344093323
         ]
        },
        {
         "name": "Validation F1 score",
         "type": "scatter",
         "y": [
          0.49721255898475647,
          0.3392857015132904,
          0.3392857015132904,
          0.3392857015132904,
          0.3392857015132904,
          0.3392857015132904,
          0.3392857015132904,
          0.3392857015132904,
          0.3392857015132904,
          0.3392857015132904,
          0.7196969985961914,
          0.4950553774833679,
          0.6717917919158936,
          0.7206300497055054,
          0.6503192186355591,
          0.6314176321029663,
          0.7105605602264404,
          0.675438642501831,
          0.7115010023117065,
          0.693469762802124,
          0.702316164970398,
          0.6842746734619141,
          0.693469762802124,
          0.7011504173278809,
          0.7098039388656616,
          0.7286505699157715,
          0.7467405796051025,
          0.7192624807357788,
          0.7279411554336548,
          0.7105606198310852,
          0.7366009950637817,
          0.7442396879196167,
          0.7199022769927979,
          0.7460784912109375,
          0.6930708885192871,
          0.7373745441436768,
          0.7366009950637817,
          0.7026060819625854,
          0.7184354066848755,
          0.7270492315292358,
          0.7452459335327148,
          0.7105606198310852,
          0.7018315196037292,
          0.7270492315292358,
          0.725970983505249,
          0.7286505699157715,
          0.7098039388656616,
          0.7192624807357788,
          0.7105606198310852,
          0.702316164970398,
          0.7270492315292358,
          0.7203575372695923,
          0.7442396879196167,
          0.7467405796051025,
          0.7279411554336548,
          0.7174180746078491,
          0.7279411554336548,
          0.7286505699157715,
          0.7199022769927979,
          0.7192624807357788,
          0.7279411554336548,
          0.7366009950637817,
          0.7279411554336548,
          0.7386538982391357,
          0.737973153591156,
          0.7192624807357788,
          0.7002700567245483,
          0.7105606198310852,
          0.7192624807357788,
          0.7386538982391357,
          0.7386538982391357,
          0.7279411554336548,
          0.7207207083702087,
          0.7098039388656616,
          0.7472348213195801,
          0.7162061929702759,
          0.7206300497055054,
          0.7115010023117065,
          0.7295321822166443,
          0.7295321822166443,
          0.6936688423156738,
          0.7206300497055054,
          0.7115010023117065,
          0.7295321822166443,
          0.7295321822166443,
          0.7297078371047974,
          0.7111256122589111,
          0.7105606198310852,
          0.7297078371047974,
          0.7111256122589111,
          0.7203575372695923,
          0.7192624807357788,
          0.7383990287780762,
          0.7199022769927979,
          0.7116883397102356,
          0.7383990287780762,
          0.7027027010917664,
          0.7206300497055054,
          0.7203575372695923,
          0.7366009950637817,
          0.7027027010917664,
          0.7027027010917664,
          0.7291802167892456,
          0.7116883397102356,
          0.7116882801055908,
          0.7026060819625854,
          0.7373745441436768,
          0.7472348213195801,
          0.7373745441436768,
          0.7279411554336548,
          0.7115010023117065,
          0.7383990287780762,
          0.7026061415672302,
          0.7452459335327148,
          0.747563362121582,
          0.7018315196037292,
          0.7206300497055054,
          0.7199022769927979,
          0.7206300497055054,
          0.693469762802124,
          0.7115010023117065,
          0.7270492315292358,
          0.7373745441436768,
          0.7116882801055908,
          0.7026060819625854,
          0.737973153591156,
          0.7027027010917664,
          0.7203575372695923,
          0.7291802167892456,
          0.702316164970398,
          0.7027027010917664,
          0.7115010023117065,
          0.747563362121582,
          0.737973153591156,
          0.7467405796051025,
          0.7373745441436768,
          0.7295321822166443,
          0.7652895450592041,
          0.7366009950637817,
          0.7564404606819153,
          0.7373745441436768,
          0.7554867267608643,
          0.7206300497055054,
          0.7206300497055054,
          0.7467405796051025,
          0.7460784912109375,
          0.693668782711029,
          0.7467405796051025,
          0.7295321822166443,
          0.7295321822166443,
          0.7472348213195801,
          0.7295321822166443,
          0.737973153591156,
          0.7116882801055908,
          0.7206300497055054,
          0.7564404606819153,
          0.737973153591156,
          0.7472348213195801,
          0.7383990287780762,
          0.7295321226119995,
          0.7477272748947144,
          0.7657467126846313,
          0.7297078371047974,
          0.7554867267608643,
          0.7564404606819153,
          0.7279411554336548,
          0.7554867267608643,
          0.7560439109802246,
          0.7460784912109375,
          0.7735987901687622,
          0.7538802027702332,
          0.747563362121582,
          0.7467405796051025,
          0.7467405796051025,
          0.7467405796051025,
          0.7387387156486511,
          0.7564404606819153,
          0.7560439109802246,
          0.7652895450592041,
          0.7452459335327148,
          0.7547663450241089,
          0.7547663450241089,
          0.7567567825317383,
          0.7295321822166443,
          0.7652895450592041,
          0.7648305892944336,
          0.7467405796051025,
          0.7560439109802246,
          0.7467405796051025,
          0.7386538982391357,
          0.7472348213195801,
          0.7564404606819153,
          0.7566778063774109,
          0.7829204797744751,
          0.7472348213195801,
          0.7735987901687622,
          0.7560439109802246,
          0.7452459335327148,
          0.7829204797744751,
          0.7460784912109375,
          0.7648305892944336,
          0.7648305892944336,
          0.7560439109802246,
          0.7387387156486511,
          0.7648305892944336,
          0.7564404606819153,
          0.7655945420265198,
          0.7744818925857544,
          0.774114727973938,
          0.7655945420265198,
          0.7655945420265198,
          0.7386538982391357,
          0.7648305892944336,
          0.7655945420265198,
          0.7652895450592041,
          0.7452459335327148,
          0.7567567825317383,
          0.7547663450241089,
          0.774114727973938,
          0.7652895450592041,
          0.7547663450241089,
          0.7655945420265198,
          0.7925233840942383,
          0.7747016549110413,
          0.7833442091941833,
          0.7648305892944336,
          0.7477272748947144,
          0.7472348213195801,
          0.7564404606819153,
          0.7564404606819153,
          0.7564404606819153,
          0.7560439109802246,
          0.7648305892944336,
          0.747563362121582,
          0.7652895450592041,
          0.7744818925857544,
          0.7467405796051025,
          0.7373745441436768,
          0.7567567825317383,
          0.7560439109802246,
          0.7477272748947144,
          0.7652895450592041,
          0.774114727973938,
          0.7279411554336548,
          0.7566778063774109,
          0.7655945420265198,
          0.7560439109802246,
          0.7472348213195801,
          0.7655945420265198,
          0.7747016549110413,
          0.7655945420265198,
          0.7366009950637817,
          0.7655945420265198,
          0.7655945420265198,
          0.7744818925857544,
          0.7652895450592041,
          0.7642157077789307,
          0.7648305892944336,
          0.774114727973938,
          0.7567567825317383,
          0.7642157077789307,
          0.7744818925857544,
          0.7833442091941833,
          0.7833442091941833,
          0.7655945420265198,
          0.7655945420265198,
          0.7744818925857544,
          0.7564404606819153,
          0.7747016549110413,
          0.7648305892944336,
          0.7744818925857544,
          0.7467405796051025,
          0.7833442091941833,
          0.7744818925857544,
          0.7655945420265198,
          0.7460784912109375,
          0.7652895450592041,
          0.7652895450592041,
          0.7460784912109375,
          0.747563362121582,
          0.7648305892944336,
          0.7452459335327148,
          0.7467405796051025,
          0.7657467126846313,
          0.7836257219314575,
          0.7744818925857544,
          0.774114727973938,
          0.7554867267608643,
          0.7744818925857544,
          0.7564404606819153,
          0.7648305892944336,
          0.7833442091941833,
          0.774114727973938,
          0.7829204797744751,
          0.774114727973938,
          0.7648305892944336,
          0.7452459335327148,
          0.7836257219314575,
          0.7747747302055359,
          0.7648305892944336,
          0.7648305892944336,
          0.7560439109802246,
          0.7652895450592041,
          0.7560439109802246,
          0.7744818925857544,
          0.7560439109802246,
          0.7648305892944336,
          0.7564404606819153,
          0.7648305892944336,
          0.7648305892944336,
          0.7829204797744751,
          0.7829204797744751,
          0.7744818925857544,
          0.7554867267608643,
          0.7373745441436768,
          0.7560439109802246,
          0.7467405796051025,
          0.7652895450592041,
          0.7652895450592041,
          0.7747016549110413,
          0.737973153591156,
          0.7560439109802246,
          0.7560439109802246,
          0.7472348213195801,
          0.7648305892944336,
          0.7560439109802246,
          0.7648305892944336,
          0.7467405796051025,
          0.7655945420265198,
          0.7836257219314575,
          0.7652895450592041,
          0.7655945420265198,
          0.7554867267608643,
          0.7554867267608643,
          0.7560439109802246,
          0.7652895450592041,
          0.774114727973938,
          0.7836257219314575,
          0.7564404606819153,
          0.7648305892944336,
          0.774114727973938,
          0.7467405796051025,
          0.7744818925857544,
          0.7560439109802246,
          0.7648305892944336,
          0.774114727973938,
          0.774114727973938,
          0.774114727973938,
          0.7655945420265198,
          0.7836257219314575,
          0.7833442091941833,
          0.7829204797744751,
          0.7836257219314575,
          0.7927254438400269,
          0.7652895450592041,
          0.7652895450592041,
          0.774114727973938,
          0.774114727973938,
          0.774114727973938,
          0.774114727973938,
          0.774114727973938,
          0.7833442091941833,
          0.774114727973938,
          0.7648305892944336,
          0.774114727973938,
          0.7836257219314575,
          0.7836257219314575,
          0.7836257219314575,
          0.7744818925857544,
          0.7833442091941833,
          0.7836257219314575,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.7833442091941833,
          0.7927254438400269,
          0.7744818925857544,
          0.7836257219314575,
          0.7836257219314575,
          0.7927254438400269,
          0.7833442091941833,
          0.7925233840942383,
          0.8016568422317505,
          0.8017857074737549,
          0.7925233840942383,
          0.7927254438400269,
          0.7652895450592041,
          0.7744818925857544,
          0.7744818925857544,
          0.7836257219314575,
          0.7744818925857544,
          0.7744818925857544,
          0.7833442091941833,
          0.7652895450592041,
          0.7747016549110413,
          0.8017857074737549,
          0.7927254438400269,
          0.7927254438400269,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.7927254438400269,
          0.7744818925857544,
          0.7927254438400269,
          0.7836257219314575,
          0.7925233840942383,
          0.7927254438400269,
          0.7927254438400269,
          0.7927254438400269,
          0.7836257219314575,
          0.7925233840942383,
          0.7744818925857544,
          0.7836257219314575,
          0.7836257219314575,
          0.7744818925857544,
          0.7836257219314575,
          0.7833442091941833,
          0.7744818925857544,
          0.7836257219314575,
          0.7836257219314575,
          0.7927254438400269,
          0.7927254438400269,
          0.7925233840942383,
          0.7927254438400269,
          0.7836257219314575,
          0.7925233840942383,
          0.8017857074737549,
          0.7836257219314575,
          0.774114727973938,
          0.7927254438400269,
          0.8013988137245178,
          0.7833442091941833,
          0.7925233840942383,
          0.7744818925857544,
          0.7744818925857544,
          0.8108108043670654,
          0.7836257219314575,
          0.7744818925857544,
          0.7927254438400269,
          0.7927254438400269,
          0.7927254438400269,
          0.7927254438400269,
          0.7927254438400269,
          0.7927254438400269,
          0.7927254438400269,
          0.7744818925857544,
          0.7744818925857544,
          0.8108108043670654,
          0.8017857074737549,
          0.7744818925857544,
          0.7744818925857544,
          0.7652895450592041,
          0.7836257219314575,
          0.8016568422317505,
          0.7836257219314575,
          0.7927254438400269,
          0.8107492923736572,
          0.7744818925857544,
          0.8016568422317505,
          0.7925233840942383,
          0.7925233840942383,
          0.8198052048683167,
          0.7925233840942383,
          0.7921856641769409,
          0.8198052048683167,
          0.8198052048683167,
          0.8016568422317505,
          0.7833442091941833,
          0.774114727973938,
          0.7836257219314575,
          0.8108108043670654,
          0.8107492923736572,
          0.7927254438400269,
          0.8017857074737549,
          0.8016568422317505,
          0.7833442091941833,
          0.8017857074737549,
          0.8198052048683167,
          0.7925233840942383,
          0.774114727973938,
          0.7925233840942383,
          0.7925233840942383,
          0.8107492923736572,
          0.7833442091941833,
          0.7925233840942383,
          0.8016568422317505,
          0.7836257219314575,
          0.7744818925857544,
          0.7927254438400269,
          0.7836257219314575,
          0.7925233840942383,
          0.7925233840942383,
          0.7925233840942383,
          0.7925233840942383,
          0.7925233840942383,
          0.7925233840942383,
          0.8016568422317505,
          0.7925233840942383,
          0.8016568422317505,
          0.7925233840942383,
          0.774114727973938,
          0.8198052048683167,
          0.8108108043670654,
          0.7925233840942383,
          0.7925233840942383,
          0.7925233840942383,
          0.7925233840942383,
          0.8017857074737549,
          0.7833442091941833,
          0.7836257219314575,
          0.7836257219314575,
          0.7744818925857544,
          0.7744818925857544,
          0.7744818925857544,
          0.7836257219314575,
          0.7836257219314575,
          0.7836257219314575,
          0.7652895450592041,
          0.7836257219314575,
          0.7833442091941833,
          0.7744818925857544,
          0.7833442091941833,
          0.7833442091941833,
          0.7833442091941833,
          0.7833442091941833,
          0.7833442091941833,
          0.7925233840942383,
          0.8198052048683167,
          0.7925233840942383,
          0.8107492923736572,
          0.7833442091941833,
          0.8016568422317505,
          0.7833442091941833,
          0.8107492923736572,
          0.7925233840942383,
          0.8107492923736572,
          0.8107492923736572,
          0.7925233840942383,
          0.8107492923736572,
          0.7925233840942383,
          0.8198052048683167,
          0.8107492923736572,
          0.7925233840942383,
          0.8107492923736572,
          0.7925233840942383,
          0.8016568422317505,
          0.8016568422317505,
          0.8016568422317505,
          0.8016568422317505,
          0.8107492923736572,
          0.7925233840942383,
          0.7833442091941833,
          0.8016568422317505,
          0.8016568422317505,
          0.8016568422317505,
          0.8107492923736572,
          0.8016568422317505,
          0.8016568422317505,
          0.8016568422317505,
          0.8016568422317505,
          0.7925233840942383,
          0.8016568422317505,
          0.8107492923736572,
          0.8198052048683167,
          0.8107492923736572,
          0.8198052048683167,
          0.8107492923736572,
          0.8016568422317505,
          0.8016568422317505,
          0.8016568422317505,
          0.8016568422317505,
          0.8198052048683167,
          0.8107492923736572,
          0.8107492923736572,
          0.8107492923736572,
          0.8107492923736572,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8016568422317505,
          0.8107492923736572,
          0.8107492923736572,
          0.7925233840942383,
          0.8198052048683167,
          0.7925233840942383,
          0.8198052048683167,
          0.8198052048683167,
          0.8107492923736572,
          0.8016568422317505,
          0.8198052048683167,
          0.8198052048683167,
          0.7925233840942383,
          0.7925233840942383,
          0.8016568422317505,
          0.8198052048683167,
          0.7925233840942383,
          0.8107492923736572,
          0.8016568422317505,
          0.8107492923736572,
          0.8107492923736572,
          0.8107492923736572,
          0.8016568422317505,
          0.8016568422317505,
          0.8107492923736572,
          0.7925233840942383,
          0.8107492923736572,
          0.8016568422317505,
          0.8107492923736572,
          0.8107492923736572,
          0.8016568422317505,
          0.8016568422317505,
          0.8016568422317505,
          0.8016568422317505,
          0.8016568422317505,
          0.7925233840942383,
          0.8107492923736572,
          0.8198052048683167,
          0.8198052048683167,
          0.8016568422317505,
          0.7925233840942383,
          0.7925233840942383,
          0.7925233840942383,
          0.7925233840942383,
          0.8107492923736572,
          0.8107492923736572,
          0.8016568422317505,
          0.8107492923736572,
          0.8107492923736572,
          0.8107492923736572,
          0.8107492923736572,
          0.8198052048683167,
          0.8107492923736572,
          0.7925233840942383,
          0.8107492923736572,
          0.8198052048683167,
          0.8107492923736572,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8016568422317505,
          0.8016568422317505,
          0.8107492923736572,
          0.8198052048683167,
          0.8107492923736572,
          0.8016568422317505,
          0.8016568422317505,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8016568422317505,
          0.8107492923736572,
          0.8107492923736572,
          0.8198052048683167,
          0.8198052048683167,
          0.8016568422317505,
          0.8016568422317505,
          0.7925233840942383,
          0.8016568422317505,
          0.8198052048683167,
          0.8107492923736572,
          0.8107492923736572,
          0.8198052048683167,
          0.8198052048683167,
          0.8016568422317505,
          0.8016568422317505,
          0.8016568422317505,
          0.8107492923736572,
          0.8016568422317505,
          0.8107492923736572,
          0.8016568422317505,
          0.8107492923736572,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8107492923736572,
          0.8198052048683167,
          0.8107492923736572,
          0.8107492923736572,
          0.7925233840942383,
          0.7925233840942383,
          0.7925233840942383,
          0.8107492923736572,
          0.8198052048683167,
          0.7925233840942383,
          0.8107492923736572,
          0.8016568422317505,
          0.8016568422317505,
          0.8016568422317505,
          0.8107492923736572,
          0.8016568422317505,
          0.8016568422317505,
          0.7925233840942383,
          0.8016568422317505,
          0.8107492923736572,
          0.8107492923736572,
          0.8107492923736572,
          0.8016568422317505,
          0.8198052048683167,
          0.8107492923736572,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8107492923736572,
          0.7925233840942383,
          0.8107492923736572,
          0.7925233840942383,
          0.8198052048683167,
          0.8198052048683167,
          0.7925233840942383,
          0.8016568422317505,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8016568422317505,
          0.8016568422317505,
          0.8107492923736572,
          0.8107492923736572,
          0.8016568422317505,
          0.7925233840942383,
          0.8107492923736572,
          0.8107492923736572,
          0.8107492923736572,
          0.8016568422317505,
          0.8107492923736572,
          0.8107492923736572,
          0.8198052048683167,
          0.8107492923736572,
          0.8016568422317505,
          0.8016568422317505,
          0.8198052048683167,
          0.8107492923736572,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8107492923736572,
          0.8107492923736572,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8107492923736572,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8107492923736572,
          0.8198052048683167,
          0.8107492923736572,
          0.8016568422317505,
          0.8198052048683167,
          0.8016568422317505,
          0.8107492923736572,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8016568422317505,
          0.8016568422317505,
          0.8016568422317505,
          0.7833442091941833,
          0.8016568422317505,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8016568422317505,
          0.8107492923736572,
          0.8198052048683167,
          0.8198052048683167,
          0.8107492923736572,
          0.8198052048683167,
          0.8107492923736572,
          0.8107492923736572,
          0.8198052048683167,
          0.8107492923736572,
          0.8107492923736572,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8107492923736572,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8107492923736572,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8107492923736572,
          0.8107492923736572,
          0.8107492923736572,
          0.8107492923736572,
          0.8107492923736572,
          0.8107492923736572,
          0.8016568422317505,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8107492923736572,
          0.8107492923736572,
          0.8107492923736572,
          0.8107492923736572,
          0.8107492923736572,
          0.8016568422317505,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8107492923736572,
          0.8107492923736572,
          0.8107492923736572,
          0.8107492923736572,
          0.8107492923736572,
          0.8016568422317505,
          0.8016568422317505,
          0.8107492923736572,
          0.8107492923736572,
          0.8107492923736572,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8107492923736572,
          0.8107492923736572,
          0.8198052048683167,
          0.8107492923736572,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8108108043670654,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8107492923736572,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167,
          0.8198052048683167
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Accuracy MLP"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "F1 score"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'..\\\\..\\\\Outputs\\\\COVID\\\\LR\\\\NuovoTest_Not_Kfold\\\\GRAPHS\\\\training MLP F1 score.html'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLOT HISTORY\n",
    "plt.clf()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['loss'],\n",
    "                    name='Train loss'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_loss'],\n",
    "                    name='Validation loss'))\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Loss MLP',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='Loss')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training MLP loss.html\"))), auto_open=False)\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    # y=history.history['categorical_accuracy'],\n",
    "                    y=history.history['binary_accuracy'],\n",
    "                    name='Train accuracy'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    # y=history.history['val_categorical_accuracy'],\n",
    "                    y=history.history['val_binary_accuracy'],\n",
    "                    name='Validation accuracy'))\n",
    "\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Accuracy MLP',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='Accuracy')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training MLP accuracy.html\"))), auto_open=False)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['f1_score'],\n",
    "                    name='Train F1 score'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_f1_score'],\n",
    "                    name='Validation F1 score'))\n",
    "\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Accuracy MLP',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='F1 score')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training MLP F1 score.html\"))), auto_open=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth of the validation\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4779 - binary_accuracy: 0.8198 - f1_score: 0.8198\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "[0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1]\n",
      "[0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1]\n",
      "accuracy:  0.8198198198198198\n",
      "log_loss:  0.47826043830194453\n",
      "[[46  8]\n",
      " [12 45]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.79310   0.85185   0.82143        54\n",
      "           1    0.84906   0.78947   0.81818        57\n",
      "\n",
      "    accuracy                        0.81982       111\n",
      "   macro avg    0.82108   0.82066   0.81981       111\n",
      "weighted avg    0.82184   0.81982   0.81976       111\n",
      "\n",
      "{'loss': 0.47793662548065186, 'binary_accuracy': 0.8198198080062866, 'f1_score': 0.8198052048683167}\n",
      "Ground truth of the validation\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4779 - binary_accuracy: 0.8198 - f1_score: 0.8198\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "[0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1]\n",
      "[0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1]\n",
      "accuracy:  0.8198198198198198\n",
      "log_loss:  0.47826043830194453\n",
      "[[46  8]\n",
      " [12 45]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.79310   0.85185   0.82143        54\n",
      "           1    0.84906   0.78947   0.81818        57\n",
      "\n",
      "    accuracy                        0.81982       111\n",
      "   macro avg    0.82108   0.82066   0.81981       111\n",
      "weighted avg    0.82184   0.81982   0.81976       111\n",
      "\n",
      "{'loss': 0.47793662548065186, 'binary_accuracy': 0.8198198080062866, 'f1_score': 0.8198052048683167}\n"
     ]
    }
   ],
   "source": [
    "## EVALUATION\n",
    "gc.collect()\n",
    "\n",
    "bestPt = checkpointPath / Path('MLP_Classification_F1')\n",
    "model = tf.keras.models.load_model(bestPt)\n",
    "\n",
    "print(\"Ground truth of the validation\")\n",
    "\n",
    "evaluation = model.evaluate(x = X_val, y = Y_val)\n",
    "prediction = model.predict(X_val)\n",
    "\n",
    "ypreds = np.argmax(prediction, axis=1)\n",
    "yground = np.argmax(Y_val.to_numpy(), axis=1)\n",
    "# yground = np.argmax(targets, axis=1)\n",
    "\n",
    "print(ypreds)\n",
    "print(yground)\n",
    "\n",
    "\n",
    "\n",
    "score = accuracy_score(yground, ypreds)\n",
    "print('accuracy: ', score)\n",
    "\n",
    "loss = log_loss(yground, prediction)\n",
    "print('log_loss: ', loss)\n",
    "\n",
    "# Print the confusion matrix\n",
    "cmatrix = confusion_matrix(yground, ypreds)\n",
    "print(cmatrix)\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "creport = classification_report(yground, ypreds, digits=5)\n",
    "print(creport)\n",
    "\n",
    "\n",
    "evaluation = dict(zip(model.metrics_names,evaluation))\n",
    "\n",
    "# FF_f1_values.append(evaluation['f1_score'])\n",
    "\n",
    "\n",
    "\n",
    "print(evaluation)\n",
    "with open(str(evalspath) + 'MLP_F1_EVALUATION.txt', 'w') as f:\n",
    "    f.write('MLP FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    for k, v in evaluation.items():\n",
    "        f.write(str(k) + ' >>> '+ str(v) + '\\n\\n')\n",
    "    f.write('CNN FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    f.write('SKLEARN METRICS:')\n",
    "    f.write(str('Loss') + ' >>> '+ str(loss) + '\\n\\n')\n",
    "    f.write(str('Accuracy') + ' >>> '+ str(score) + '\\n\\n')\n",
    "    f.write(str('Confusion Matrix') + ' >>> \\n'+ str(cmatrix) + '\\n\\n')\n",
    "    f.write(str('CLASSIFICATION REPORT WITH CLASSES') + ' >>> \\n'+ str(creport) + '\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bestPt = checkpointPath / Path('MLP_Classification_acc')\n",
    "model = tf.keras.models.load_model(bestPt)\n",
    "\n",
    "print(\"Ground truth of the validation\")\n",
    "\n",
    "evaluation = model.evaluate(x = X_val, y = Y_val)\n",
    "prediction = model.predict(X_val)\n",
    "\n",
    "ypreds = np.argmax(prediction, axis=1)\n",
    "yground = np.argmax(Y_val.to_numpy(), axis=1)\n",
    "# yground = np.argmax(targets, axis=1)\n",
    "\n",
    "print(ypreds)\n",
    "print(yground)\n",
    "\n",
    "\n",
    "\n",
    "score = accuracy_score(yground, ypreds)\n",
    "print('accuracy: ', score)\n",
    "\n",
    "loss = log_loss(yground, prediction)\n",
    "print('log_loss: ', loss)\n",
    "\n",
    "\n",
    "# Print the confusion matrix\n",
    "cmatrix = confusion_matrix(yground, ypreds)\n",
    "print(cmatrix)\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "creport = classification_report(yground, ypreds, digits=5)\n",
    "print(creport)\n",
    "\n",
    "\n",
    "# FF_accuracy_values.append(score)\n",
    "\n",
    "evaluation = dict(zip(model.metrics_names,evaluation))\n",
    "print(evaluation)\n",
    "with open(str(evalspath) + 'MLP_acc_EVALUATION.txt', 'w') as f:\n",
    "    f.write('MLP FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    for k, v in evaluation.items():\n",
    "        f.write(str(k) + ' >>> '+ str(v) + '\\n\\n')\n",
    "    f.write('MLP FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    f.write('SKLEARN METRICS:')\n",
    "    f.write(str('Loss') + ' >>> '+ str(loss) + '\\n\\n')\n",
    "    f.write(str('Accuracy') + ' >>> '+ str(score) + '\\n\\n')\n",
    "    f.write(str('Confusion Matrix') + ' >>> \\n'+ str(cmatrix) + '\\n\\n')\n",
    "    f.write(str('CLASSIFICATION REPORT WITH CLASSES') + ' >>> \\n'+ str(creport) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decision_Multimodal(MLP, CNN, mlpOutL = -2, cnnOutL = -3):\n",
    "\n",
    "    outMLP = MLP.layers[cnnOutL].output\n",
    "    outCNN = CNN.layers[mlpOutL].output\n",
    "\n",
    "    concat_decisions = Concatenate()([outMLP,outCNN])    \n",
    "\n",
    "    new_decision = Dense(4,activation='relu')(concat_decisions)\n",
    "    new_decision = BatchNormalization()(new_decision)\n",
    "    \n",
    "    output = Dense(2, activation='sigmoid')(new_decision)\n",
    "\n",
    "    model = Model(inputs=[CNN.input,MLP.input], outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT\n",
      "Epoch 1/200\n",
      " 6/31 [====>.........................] - ETA: 1s - loss: 0.7063 - binary_accuracy: 0.6094 - f1_score: 0.5914WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0136s vs `on_train_batch_end` time: 0.0642s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0136s vs `on_train_batch_end` time: 0.0642s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/31 [============================>.] - ETA: 0s - loss: 0.6457 - binary_accuracy: 0.6510 - f1_score: 0.6410"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 33s 993ms/step - loss: 0.6448 - binary_accuracy: 0.6527 - f1_score: 0.6426 - val_loss: 0.6281 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7199\n",
      "Epoch 2/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6299 - binary_accuracy: 0.6537 - f1_score: 0.6437"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 29s 966ms/step - loss: 0.6299 - binary_accuracy: 0.6537 - f1_score: 0.6437 - val_loss: 0.6084 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7260\n",
      "Epoch 3/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.6294 - binary_accuracy: 0.6552 - f1_score: 0.6500 - val_loss: 0.5905 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7162\n",
      "Epoch 4/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.5788 - binary_accuracy: 0.6976 - f1_score: 0.6942 - val_loss: 0.5765 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7148\n",
      "Epoch 5/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.5690 - binary_accuracy: 0.7016 - f1_score: 0.6922 - val_loss: 0.5630 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7232\n",
      "Epoch 6/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.5476 - binary_accuracy: 0.7177 - f1_score: 0.7100 - val_loss: 0.5510 - val_binary_accuracy: 0.7207 - val_f1_score: 0.6927\n",
      "Epoch 7/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.5090 - binary_accuracy: 0.7566 - f1_score: 0.7540 - val_loss: 0.5403 - val_binary_accuracy: 0.7207 - val_f1_score: 0.6927\n",
      "Epoch 8/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.5418 - binary_accuracy: 0.7137 - f1_score: 0.7092 - val_loss: 0.5338 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7010\n",
      "Epoch 9/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.5332 - binary_accuracy: 0.7384 - f1_score: 0.7348 - val_loss: 0.5267 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7113\n",
      "Epoch 10/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.5284 - binary_accuracy: 0.7248 - f1_score: 0.7177 - val_loss: 0.5248 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7113\n",
      "Epoch 11/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.5024 - binary_accuracy: 0.7550 - f1_score: 0.7535 - val_loss: 0.5175 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7216\n",
      "Epoch 12/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4876 - binary_accuracy: 0.7631 - f1_score: 0.7643"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 29s 957ms/step - loss: 0.4876 - binary_accuracy: 0.7631 - f1_score: 0.7643 - val_loss: 0.5106 - val_binary_accuracy: 0.7432 - val_f1_score: 0.7417\n",
      "Epoch 13/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4910 - binary_accuracy: 0.7686 - f1_score: 0.7694 - val_loss: 0.5046 - val_binary_accuracy: 0.7432 - val_f1_score: 0.7417\n",
      "Epoch 14/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4742 - binary_accuracy: 0.7767 - f1_score: 0.7756 - val_loss: 0.4987 - val_binary_accuracy: 0.7432 - val_f1_score: 0.7332\n",
      "Epoch 15/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4952 - binary_accuracy: 0.7661 - f1_score: 0.7683"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 513ms/step - loss: 0.4952 - binary_accuracy: 0.7661 - f1_score: 0.7683 - val_loss: 0.4953 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7431\n",
      "Epoch 16/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4614 - binary_accuracy: 0.8009 - f1_score: 0.8003"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 489ms/step - loss: 0.4614 - binary_accuracy: 0.8009 - f1_score: 0.8003 - val_loss: 0.4882 - val_binary_accuracy: 0.7477 - val_f1_score: 0.7260\n",
      "Epoch 17/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4732 - binary_accuracy: 0.7777 - f1_score: 0.7783"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 29s 953ms/step - loss: 0.4732 - binary_accuracy: 0.7777 - f1_score: 0.7783 - val_loss: 0.4833 - val_binary_accuracy: 0.7523 - val_f1_score: 0.7461\n",
      "Epoch 18/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4645 - binary_accuracy: 0.7848 - f1_score: 0.7844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 30s 983ms/step - loss: 0.4645 - binary_accuracy: 0.7848 - f1_score: 0.7844 - val_loss: 0.4798 - val_binary_accuracy: 0.7568 - val_f1_score: 0.7648\n",
      "Epoch 19/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4654 - binary_accuracy: 0.7898 - f1_score: 0.7913"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 499ms/step - loss: 0.4654 - binary_accuracy: 0.7898 - f1_score: 0.7913 - val_loss: 0.4738 - val_binary_accuracy: 0.7613 - val_f1_score: 0.7648\n",
      "Epoch 20/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4507 - binary_accuracy: 0.7828 - f1_score: 0.7841"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 491ms/step - loss: 0.4507 - binary_accuracy: 0.7828 - f1_score: 0.7841 - val_loss: 0.4707 - val_binary_accuracy: 0.7613 - val_f1_score: 0.7653\n",
      "Epoch 21/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4364 - binary_accuracy: 0.7969 - f1_score: 0.8003"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 506ms/step - loss: 0.4364 - binary_accuracy: 0.7969 - f1_score: 0.8003 - val_loss: 0.4678 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7653\n",
      "Epoch 22/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4319 - binary_accuracy: 0.8135 - f1_score: 0.8133 - val_loss: 0.4663 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7564\n",
      "Epoch 23/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4420 - binary_accuracy: 0.8130 - f1_score: 0.8135 - val_loss: 0.4659 - val_binary_accuracy: 0.7658 - val_f1_score: 0.7564\n",
      "Epoch 24/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4519 - binary_accuracy: 0.7818 - f1_score: 0.7789"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 504ms/step - loss: 0.4519 - binary_accuracy: 0.7818 - f1_score: 0.7789 - val_loss: 0.4655 - val_binary_accuracy: 0.7613 - val_f1_score: 0.7657\n",
      "Epoch 25/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4495 - binary_accuracy: 0.8155 - f1_score: 0.8133"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 486ms/step - loss: 0.4495 - binary_accuracy: 0.8155 - f1_score: 0.8133 - val_loss: 0.4651 - val_binary_accuracy: 0.7703 - val_f1_score: 0.7657\n",
      "Epoch 26/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4954 - binary_accuracy: 0.7752 - f1_score: 0.7752"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 16s 514ms/step - loss: 0.4954 - binary_accuracy: 0.7752 - f1_score: 0.7752 - val_loss: 0.4668 - val_binary_accuracy: 0.7703 - val_f1_score: 0.7748\n",
      "Epoch 27/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4420 - binary_accuracy: 0.8070 - f1_score: 0.8075 - val_loss: 0.4651 - val_binary_accuracy: 0.7703 - val_f1_score: 0.7748\n",
      "Epoch 28/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4261 - binary_accuracy: 0.8070 - f1_score: 0.8094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 501ms/step - loss: 0.4261 - binary_accuracy: 0.8070 - f1_score: 0.8094 - val_loss: 0.4630 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7748\n",
      "Epoch 29/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4266 - binary_accuracy: 0.8251 - f1_score: 0.8246 - val_loss: 0.4640 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7748\n",
      "Epoch 30/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4564 - binary_accuracy: 0.7939 - f1_score: 0.7933 - val_loss: 0.4635 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7748\n",
      "Epoch 31/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4378 - binary_accuracy: 0.8004 - f1_score: 0.7993 - val_loss: 0.4640 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7748\n",
      "Epoch 32/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4215 - binary_accuracy: 0.8155 - f1_score: 0.8174 - val_loss: 0.4630 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7748\n",
      "Epoch 33/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4359 - binary_accuracy: 0.7984 - f1_score: 0.7974"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 496ms/step - loss: 0.4359 - binary_accuracy: 0.7984 - f1_score: 0.7974 - val_loss: 0.4633 - val_binary_accuracy: 0.7793 - val_f1_score: 0.7748\n",
      "Epoch 34/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4208 - binary_accuracy: 0.8226 - f1_score: 0.8256 - val_loss: 0.4633 - val_binary_accuracy: 0.7793 - val_f1_score: 0.7748\n",
      "Epoch 35/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4443 - binary_accuracy: 0.8090 - f1_score: 0.8085 - val_loss: 0.4614 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7748\n",
      "Epoch 36/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4299 - binary_accuracy: 0.8185 - f1_score: 0.8175"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 29s 980ms/step - loss: 0.4299 - binary_accuracy: 0.8185 - f1_score: 0.8175 - val_loss: 0.4606 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7838\n",
      "Epoch 37/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4345 - binary_accuracy: 0.8075 - f1_score: 0.8074 - val_loss: 0.4603 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7838\n",
      "Epoch 38/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4101 - binary_accuracy: 0.8367 - f1_score: 0.8367"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 510ms/step - loss: 0.4101 - binary_accuracy: 0.8367 - f1_score: 0.8367 - val_loss: 0.4606 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7838\n",
      "Epoch 39/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4364 - binary_accuracy: 0.8080 - f1_score: 0.8094 - val_loss: 0.4613 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7838\n",
      "Epoch 40/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4387 - binary_accuracy: 0.8216 - f1_score: 0.8235 - val_loss: 0.4610 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7748\n",
      "Epoch 41/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4241 - binary_accuracy: 0.8105 - f1_score: 0.8095 - val_loss: 0.4613 - val_binary_accuracy: 0.7748 - val_f1_score: 0.7748\n",
      "Epoch 42/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4411 - binary_accuracy: 0.8135 - f1_score: 0.8125 - val_loss: 0.4617 - val_binary_accuracy: 0.7793 - val_f1_score: 0.7748\n",
      "Epoch 43/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4227 - binary_accuracy: 0.8196 - f1_score: 0.8185 - val_loss: 0.4614 - val_binary_accuracy: 0.7793 - val_f1_score: 0.7748\n",
      "Epoch 44/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4136 - binary_accuracy: 0.8246 - f1_score: 0.8256 - val_loss: 0.4610 - val_binary_accuracy: 0.7793 - val_f1_score: 0.7748\n",
      "Epoch 45/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4344 - binary_accuracy: 0.8044 - f1_score: 0.8024 - val_loss: 0.4605 - val_binary_accuracy: 0.7793 - val_f1_score: 0.7748\n",
      "Epoch 46/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4573 - binary_accuracy: 0.8024 - f1_score: 0.8054 - val_loss: 0.4594 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7838\n",
      "Epoch 47/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4178 - binary_accuracy: 0.8261 - f1_score: 0.8276 - val_loss: 0.4592 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7838\n",
      "Epoch 48/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4350 - binary_accuracy: 0.8115 - f1_score: 0.8104 - val_loss: 0.4599 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7838\n",
      "Epoch 49/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4194 - binary_accuracy: 0.8216 - f1_score: 0.8215 - val_loss: 0.4602 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7838\n",
      "Epoch 50/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4453 - binary_accuracy: 0.8009 - f1_score: 0.8004 - val_loss: 0.4605 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7838\n",
      "Epoch 51/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4666 - binary_accuracy: 0.7969 - f1_score: 0.8004 - val_loss: 0.4603 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7838\n",
      "Epoch 52/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4533 - binary_accuracy: 0.8145 - f1_score: 0.8155"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 511ms/step - loss: 0.4533 - binary_accuracy: 0.8145 - f1_score: 0.8155 - val_loss: 0.4608 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 53/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4335 - binary_accuracy: 0.8049 - f1_score: 0.8053 - val_loss: 0.4601 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7838\n",
      "Epoch 54/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4435 - binary_accuracy: 0.8080 - f1_score: 0.8095 - val_loss: 0.4605 - val_binary_accuracy: 0.7793 - val_f1_score: 0.7838\n",
      "Epoch 55/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4393 - binary_accuracy: 0.8039 - f1_score: 0.8065 - val_loss: 0.4606 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7838\n",
      "Epoch 56/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4013 - binary_accuracy: 0.8261 - f1_score: 0.8235 - val_loss: 0.4613 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7838\n",
      "Epoch 57/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4368 - binary_accuracy: 0.8100 - f1_score: 0.8125 - val_loss: 0.4612 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7838\n",
      "Epoch 58/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4419 - binary_accuracy: 0.8125 - f1_score: 0.8145"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 497ms/step - loss: 0.4419 - binary_accuracy: 0.8125 - f1_score: 0.8145 - val_loss: 0.4607 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7928\n",
      "Epoch 59/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4514 - binary_accuracy: 0.8024 - f1_score: 0.8034 - val_loss: 0.4607 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7838\n",
      "Epoch 60/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4469 - binary_accuracy: 0.7969 - f1_score: 0.7972"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 508ms/step - loss: 0.4469 - binary_accuracy: 0.7969 - f1_score: 0.7972 - val_loss: 0.4602 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7928\n",
      "Epoch 61/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4453 - binary_accuracy: 0.8090 - f1_score: 0.8094 - val_loss: 0.4594 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 62/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4343 - binary_accuracy: 0.7979 - f1_score: 0.7991 - val_loss: 0.4601 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7928\n",
      "Epoch 63/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4353 - binary_accuracy: 0.8024 - f1_score: 0.7984 - val_loss: 0.4598 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 64/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4434 - binary_accuracy: 0.8004 - f1_score: 0.8003 - val_loss: 0.4594 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 65/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4215 - binary_accuracy: 0.8261 - f1_score: 0.8276 - val_loss: 0.4595 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 66/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4186 - binary_accuracy: 0.8125 - f1_score: 0.8113 - val_loss: 0.4597 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 67/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4260 - binary_accuracy: 0.8049 - f1_score: 0.8044 - val_loss: 0.4599 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7928\n",
      "Epoch 68/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4372 - binary_accuracy: 0.7908 - f1_score: 0.7893 - val_loss: 0.4602 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7838\n",
      "Epoch 69/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4223 - binary_accuracy: 0.8251 - f1_score: 0.8246 - val_loss: 0.4603 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7928\n",
      "Epoch 70/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4351 - binary_accuracy: 0.8191 - f1_score: 0.8195 - val_loss: 0.4604 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7928\n",
      "Epoch 71/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4078 - binary_accuracy: 0.8357 - f1_score: 0.8357 - val_loss: 0.4601 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7928\n",
      "Epoch 72/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4498 - binary_accuracy: 0.7994 - f1_score: 0.8014 - val_loss: 0.4600 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7928\n",
      "Epoch 73/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4338 - binary_accuracy: 0.8070 - f1_score: 0.8054 - val_loss: 0.4602 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7928\n",
      "Epoch 74/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4593 - binary_accuracy: 0.7984 - f1_score: 0.7981 - val_loss: 0.4603 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7928\n",
      "Epoch 75/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4201 - binary_accuracy: 0.8266 - f1_score: 0.8286 - val_loss: 0.4606 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7928\n",
      "Epoch 76/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4266 - binary_accuracy: 0.8185 - f1_score: 0.8195 - val_loss: 0.4595 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7928\n",
      "Epoch 77/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4227 - binary_accuracy: 0.8196 - f1_score: 0.8194"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 495ms/step - loss: 0.4227 - binary_accuracy: 0.8196 - f1_score: 0.8194 - val_loss: 0.4605 - val_binary_accuracy: 0.7973 - val_f1_score: 0.8018\n",
      "Epoch 78/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4360 - binary_accuracy: 0.8009 - f1_score: 0.7991 - val_loss: 0.4609 - val_binary_accuracy: 0.7973 - val_f1_score: 0.8018\n",
      "Epoch 79/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4444 - binary_accuracy: 0.8090 - f1_score: 0.8084 - val_loss: 0.4611 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 80/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4329 - binary_accuracy: 0.8130 - f1_score: 0.8145 - val_loss: 0.4606 - val_binary_accuracy: 0.7928 - val_f1_score: 0.8018\n",
      "Epoch 81/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4343 - binary_accuracy: 0.8165 - f1_score: 0.8165 - val_loss: 0.4604 - val_binary_accuracy: 0.7973 - val_f1_score: 0.8018\n",
      "Epoch 82/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4401 - binary_accuracy: 0.7974 - f1_score: 0.8002 - val_loss: 0.4602 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 83/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4022 - binary_accuracy: 0.8317 - f1_score: 0.8326 - val_loss: 0.4592 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7928\n",
      "Epoch 84/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4242 - binary_accuracy: 0.8251 - f1_score: 0.8246 - val_loss: 0.4586 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 85/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4330 - binary_accuracy: 0.8276 - f1_score: 0.8286 - val_loss: 0.4592 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7928\n",
      "Epoch 86/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4164 - binary_accuracy: 0.8276 - f1_score: 0.8276 - val_loss: 0.4595 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7928\n",
      "Epoch 87/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4295 - binary_accuracy: 0.8110 - f1_score: 0.8115 - val_loss: 0.4594 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 88/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4529 - binary_accuracy: 0.7939 - f1_score: 0.7933 - val_loss: 0.4593 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 89/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4539 - binary_accuracy: 0.7984 - f1_score: 0.7973 - val_loss: 0.4595 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 90/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4551 - binary_accuracy: 0.7878 - f1_score: 0.7871 - val_loss: 0.4595 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7928\n",
      "Epoch 91/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4384 - binary_accuracy: 0.8165 - f1_score: 0.8144 - val_loss: 0.4589 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 92/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4270 - binary_accuracy: 0.8115 - f1_score: 0.8135 - val_loss: 0.4593 - val_binary_accuracy: 0.7838 - val_f1_score: 0.7928\n",
      "Epoch 93/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4607 - binary_accuracy: 0.8004 - f1_score: 0.7994 - val_loss: 0.4590 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 94/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4506 - binary_accuracy: 0.8049 - f1_score: 0.8044 - val_loss: 0.4602 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7928\n",
      "Epoch 95/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4081 - binary_accuracy: 0.8271 - f1_score: 0.8275 - val_loss: 0.4606 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7928\n",
      "Epoch 96/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4270 - binary_accuracy: 0.8145 - f1_score: 0.8155 - val_loss: 0.4592 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 97/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4430 - binary_accuracy: 0.8004 - f1_score: 0.8004 - val_loss: 0.4595 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7838\n",
      "Epoch 98/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4410 - binary_accuracy: 0.8100 - f1_score: 0.8094 - val_loss: 0.4595 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 99/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4422 - binary_accuracy: 0.8034 - f1_score: 0.8023 - val_loss: 0.4594 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 100/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4541 - binary_accuracy: 0.8034 - f1_score: 0.8034 - val_loss: 0.4597 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 101/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4493 - binary_accuracy: 0.7989 - f1_score: 0.7983 - val_loss: 0.4596 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7838\n",
      "Epoch 102/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4404 - binary_accuracy: 0.8130 - f1_score: 0.8134 - val_loss: 0.4604 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 103/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4102 - binary_accuracy: 0.8372 - f1_score: 0.8377 - val_loss: 0.4606 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7928\n",
      "Epoch 104/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4360 - binary_accuracy: 0.8140 - f1_score: 0.8124 - val_loss: 0.4603 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7928\n",
      "Epoch 105/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4136 - binary_accuracy: 0.8306 - f1_score: 0.8305 - val_loss: 0.4602 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7928\n",
      "Epoch 106/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4323 - binary_accuracy: 0.8155 - f1_score: 0.8145 - val_loss: 0.4599 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 107/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4304 - binary_accuracy: 0.8196 - f1_score: 0.8184 - val_loss: 0.4598 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7928\n",
      "Epoch 108/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4472 - binary_accuracy: 0.8019 - f1_score: 0.8003 - val_loss: 0.4595 - val_binary_accuracy: 0.7883 - val_f1_score: 0.7838\n",
      "Epoch 109/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4328 - binary_accuracy: 0.8155 - f1_score: 0.8175 - val_loss: 0.4600 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7928\n",
      "Epoch 110/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4526 - binary_accuracy: 0.8054 - f1_score: 0.8043 - val_loss: 0.4606 - val_binary_accuracy: 0.7973 - val_f1_score: 0.8018\n",
      "Epoch 111/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4321 - binary_accuracy: 0.8075 - f1_score: 0.8074 - val_loss: 0.4607 - val_binary_accuracy: 0.7973 - val_f1_score: 0.8018\n",
      "Epoch 112/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4227 - binary_accuracy: 0.8185 - f1_score: 0.8185 - val_loss: 0.4602 - val_binary_accuracy: 0.7973 - val_f1_score: 0.8018\n",
      "Epoch 113/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4434 - binary_accuracy: 0.8110 - f1_score: 0.8094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 494ms/step - loss: 0.4434 - binary_accuracy: 0.8110 - f1_score: 0.8094 - val_loss: 0.4606 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 114/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4421 - binary_accuracy: 0.8070 - f1_score: 0.8054 - val_loss: 0.4608 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 115/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4319 - binary_accuracy: 0.8130 - f1_score: 0.8135 - val_loss: 0.4602 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 116/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4163 - binary_accuracy: 0.8191 - f1_score: 0.8194 - val_loss: 0.4595 - val_binary_accuracy: 0.7973 - val_f1_score: 0.8018\n",
      "Epoch 117/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4196 - binary_accuracy: 0.8206 - f1_score: 0.8205 - val_loss: 0.4588 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 118/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4245 - binary_accuracy: 0.8105 - f1_score: 0.8094 - val_loss: 0.4588 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 119/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4247 - binary_accuracy: 0.8115 - f1_score: 0.8103 - val_loss: 0.4597 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 120/200\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4288 - binary_accuracy: 0.8140 - f1_score: 0.8134"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_CONCAT_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 515ms/step - loss: 0.4288 - binary_accuracy: 0.8140 - f1_score: 0.8134 - val_loss: 0.4595 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8108\n",
      "Epoch 121/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4518 - binary_accuracy: 0.8024 - f1_score: 0.8023 - val_loss: 0.4599 - val_binary_accuracy: 0.7973 - val_f1_score: 0.8018\n",
      "Epoch 122/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4445 - binary_accuracy: 0.8115 - f1_score: 0.8094 - val_loss: 0.4603 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 123/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4408 - binary_accuracy: 0.8160 - f1_score: 0.8175 - val_loss: 0.4606 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 124/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4188 - binary_accuracy: 0.8206 - f1_score: 0.8184 - val_loss: 0.4596 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 125/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4180 - binary_accuracy: 0.8306 - f1_score: 0.8296 - val_loss: 0.4588 - val_binary_accuracy: 0.7973 - val_f1_score: 0.8018\n",
      "Epoch 126/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4706 - binary_accuracy: 0.7944 - f1_score: 0.7932 - val_loss: 0.4588 - val_binary_accuracy: 0.7973 - val_f1_score: 0.8018\n",
      "Epoch 127/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4249 - binary_accuracy: 0.8130 - f1_score: 0.8145 - val_loss: 0.4596 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 128/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4272 - binary_accuracy: 0.8029 - f1_score: 0.8032 - val_loss: 0.4591 - val_binary_accuracy: 0.7973 - val_f1_score: 0.8018\n",
      "Epoch 129/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4394 - binary_accuracy: 0.8039 - f1_score: 0.8022 - val_loss: 0.4588 - val_binary_accuracy: 0.7973 - val_f1_score: 0.8018\n",
      "Epoch 130/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4342 - binary_accuracy: 0.8155 - f1_score: 0.8154 - val_loss: 0.4586 - val_binary_accuracy: 0.7973 - val_f1_score: 0.8018\n",
      "Epoch 131/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4395 - binary_accuracy: 0.8100 - f1_score: 0.8103 - val_loss: 0.4595 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 132/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4349 - binary_accuracy: 0.8100 - f1_score: 0.8104 - val_loss: 0.4597 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 133/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4077 - binary_accuracy: 0.8155 - f1_score: 0.8154 - val_loss: 0.4590 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 134/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4438 - binary_accuracy: 0.7888 - f1_score: 0.7913 - val_loss: 0.4587 - val_binary_accuracy: 0.7973 - val_f1_score: 0.8018\n",
      "Epoch 135/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4284 - binary_accuracy: 0.8034 - f1_score: 0.8003 - val_loss: 0.4598 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 136/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4315 - binary_accuracy: 0.8034 - f1_score: 0.8033 - val_loss: 0.4598 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 137/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4341 - binary_accuracy: 0.8034 - f1_score: 0.8044 - val_loss: 0.4597 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 138/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4190 - binary_accuracy: 0.8211 - f1_score: 0.8204 - val_loss: 0.4593 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 139/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4421 - binary_accuracy: 0.8049 - f1_score: 0.8044 - val_loss: 0.4593 - val_binary_accuracy: 0.7973 - val_f1_score: 0.8018\n",
      "Epoch 140/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4423 - binary_accuracy: 0.8014 - f1_score: 0.8003 - val_loss: 0.4593 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 141/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4374 - binary_accuracy: 0.8044 - f1_score: 0.8024 - val_loss: 0.4587 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 142/200\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.4375 - binary_accuracy: 0.8090 - f1_score: 0.8074 - val_loss: 0.4590 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 143/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4429 - binary_accuracy: 0.7949 - f1_score: 0.7943 - val_loss: 0.4586 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 144/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4299 - binary_accuracy: 0.8085 - f1_score: 0.8104 - val_loss: 0.4583 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 145/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4300 - binary_accuracy: 0.8080 - f1_score: 0.8074 - val_loss: 0.4585 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 146/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4244 - binary_accuracy: 0.7974 - f1_score: 0.7973 - val_loss: 0.4583 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 147/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4324 - binary_accuracy: 0.8175 - f1_score: 0.8165 - val_loss: 0.4584 - val_binary_accuracy: 0.7973 - val_f1_score: 0.8018\n",
      "Epoch 148/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4390 - binary_accuracy: 0.8009 - f1_score: 0.8014 - val_loss: 0.4587 - val_binary_accuracy: 0.7973 - val_f1_score: 0.8018\n",
      "Epoch 149/200\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.4334 - binary_accuracy: 0.8125 - f1_score: 0.8125 - val_loss: 0.4594 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 150/200\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.4280 - binary_accuracy: 0.8140 - f1_score: 0.8135 - val_loss: 0.4593 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 151/200\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.4392 - binary_accuracy: 0.8090 - f1_score: 0.8093 - val_loss: 0.4600 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 152/200\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.4496 - binary_accuracy: 0.8024 - f1_score: 0.8023 - val_loss: 0.4598 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7927\n",
      "Epoch 153/200\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.4227 - binary_accuracy: 0.8135 - f1_score: 0.8143 - val_loss: 0.4601 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 154/200\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.4557 - binary_accuracy: 0.8009 - f1_score: 0.8003 - val_loss: 0.4600 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 155/200\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.4270 - binary_accuracy: 0.8196 - f1_score: 0.8206 - val_loss: 0.4594 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 156/200\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.4261 - binary_accuracy: 0.8160 - f1_score: 0.8165 - val_loss: 0.4591 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 157/200\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.4186 - binary_accuracy: 0.8110 - f1_score: 0.8104 - val_loss: 0.4583 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 158/200\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.4324 - binary_accuracy: 0.8054 - f1_score: 0.8044 - val_loss: 0.4582 - val_binary_accuracy: 0.7973 - val_f1_score: 0.7927\n",
      "Epoch 159/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4325 - binary_accuracy: 0.8211 - f1_score: 0.8225 - val_loss: 0.4582 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 160/200\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.4379 - binary_accuracy: 0.8070 - f1_score: 0.8064 - val_loss: 0.4583 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 161/200\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.4257 - binary_accuracy: 0.8221 - f1_score: 0.8224 - val_loss: 0.4582 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 162/200\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.4046 - binary_accuracy: 0.8301 - f1_score: 0.8296 - val_loss: 0.4582 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 163/200\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.4251 - binary_accuracy: 0.8170 - f1_score: 0.8174 - val_loss: 0.4583 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 164/200\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.4190 - binary_accuracy: 0.8226 - f1_score: 0.8235 - val_loss: 0.4582 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 165/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4650 - binary_accuracy: 0.7964 - f1_score: 0.7964 - val_loss: 0.4575 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 166/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4256 - binary_accuracy: 0.8180 - f1_score: 0.8175 - val_loss: 0.4572 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 167/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4282 - binary_accuracy: 0.8170 - f1_score: 0.8165 - val_loss: 0.4571 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 168/200\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.4065 - binary_accuracy: 0.8206 - f1_score: 0.8215 - val_loss: 0.4566 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 169/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4248 - binary_accuracy: 0.8180 - f1_score: 0.8185 - val_loss: 0.4566 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 170/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4386 - binary_accuracy: 0.7974 - f1_score: 0.7973 - val_loss: 0.4567 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 171/200\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.4494 - binary_accuracy: 0.7954 - f1_score: 0.7973 - val_loss: 0.4569 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 172/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4190 - binary_accuracy: 0.8085 - f1_score: 0.8084 - val_loss: 0.4571 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 173/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4330 - binary_accuracy: 0.8145 - f1_score: 0.8145 - val_loss: 0.4579 - val_binary_accuracy: 0.7928 - val_f1_score: 0.7927\n",
      "Epoch 174/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4411 - binary_accuracy: 0.8070 - f1_score: 0.8065 - val_loss: 0.4580 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 175/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4123 - binary_accuracy: 0.8206 - f1_score: 0.8206 - val_loss: 0.4574 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 176/200\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.4380 - binary_accuracy: 0.8039 - f1_score: 0.8034 - val_loss: 0.4579 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 177/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4527 - binary_accuracy: 0.7903 - f1_score: 0.7893 - val_loss: 0.4587 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 178/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4402 - binary_accuracy: 0.8100 - f1_score: 0.8095 - val_loss: 0.4588 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 179/200\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.4283 - binary_accuracy: 0.8090 - f1_score: 0.8104 - val_loss: 0.4588 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 180/200\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.4049 - binary_accuracy: 0.8236 - f1_score: 0.8235 - val_loss: 0.4588 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 181/200\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.4095 - binary_accuracy: 0.8231 - f1_score: 0.8235 - val_loss: 0.4587 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 182/200\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.4182 - binary_accuracy: 0.8261 - f1_score: 0.8256 - val_loss: 0.4587 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 183/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.4245 - binary_accuracy: 0.8125 - f1_score: 0.8125 - val_loss: 0.4590 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 184/200\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.4274 - binary_accuracy: 0.8165 - f1_score: 0.8154 - val_loss: 0.4587 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 185/200\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.4393 - binary_accuracy: 0.8100 - f1_score: 0.8094 - val_loss: 0.4588 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 186/200\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.4315 - binary_accuracy: 0.8029 - f1_score: 0.8023 - val_loss: 0.4578 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 187/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4211 - binary_accuracy: 0.8150 - f1_score: 0.8144 - val_loss: 0.4578 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 188/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4330 - binary_accuracy: 0.8115 - f1_score: 0.8114 - val_loss: 0.4587 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 189/200\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.4376 - binary_accuracy: 0.8039 - f1_score: 0.8034 - val_loss: 0.4584 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 190/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4382 - binary_accuracy: 0.8110 - f1_score: 0.8104 - val_loss: 0.4581 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 191/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4242 - binary_accuracy: 0.8115 - f1_score: 0.8114 - val_loss: 0.4581 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 192/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4137 - binary_accuracy: 0.8150 - f1_score: 0.8145 - val_loss: 0.4581 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 193/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4429 - binary_accuracy: 0.8145 - f1_score: 0.8145 - val_loss: 0.4581 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 194/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4151 - binary_accuracy: 0.8286 - f1_score: 0.8286 - val_loss: 0.4580 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 195/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4197 - binary_accuracy: 0.8125 - f1_score: 0.8115 - val_loss: 0.4585 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 196/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4275 - binary_accuracy: 0.8070 - f1_score: 0.8083 - val_loss: 0.4581 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 197/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4371 - binary_accuracy: 0.8044 - f1_score: 0.8043 - val_loss: 0.4581 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 198/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4277 - binary_accuracy: 0.8130 - f1_score: 0.8125 - val_loss: 0.4578 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 199/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4426 - binary_accuracy: 0.8044 - f1_score: 0.8044 - val_loss: 0.4585 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n",
      "Epoch 200/200\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.4183 - binary_accuracy: 0.8150 - f1_score: 0.8145 - val_loss: 0.4577 - val_binary_accuracy: 0.8018 - val_f1_score: 0.8018\n"
     ]
    }
   ],
   "source": [
    "bestPt = checkpointPath / Path('efficientnet_Classification_acc')\n",
    "CNNmod = tf.keras.models.load_model(bestPt)\n",
    "CNNmod = changeModelLayers(CNNmod,'mlFusionC')\n",
    "CNNmod.trainable = False\n",
    "\n",
    "bestPt = checkpointPath / Path('MLP_Classification_acc')\n",
    "MLPmod = tf.keras.models.load_model(bestPt)\n",
    "MLPmod = changeModelLayers(MLPmod,'_MLP')\n",
    "MLPmod.trainable = False\n",
    "    \n",
    "\n",
    "# concatMultimodal = create_decision_Multimodal(MLPmod,CNNmod,-1,-1)\n",
    "concatMultimodal = create_decision_Multimodal(MLPmod,CNNmod)\n",
    "\n",
    "plot_model(concatMultimodal, to_file=plotpath / Path('MULTIMODAL CONCAT_only.png'), show_shapes=True, show_layer_names=True)\n",
    "\n",
    "\n",
    "initial_learning_rate = 1e-3  # migliore come accuracy massima  0.69\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=100, decay_rate=0.99, staircase=False)\n",
    "\n",
    "concatMultimodal.compile(\n",
    "        # optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-2, decay=1e-4),   #FUSION 2022\n",
    "        optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "        # optimizer=tf.keras.optimizers.Adam(0.0011),\n",
    "        # optimizer = tf.keras.optimizers.Adam(1e-2),\n",
    "        # optimizer=tf.keras.optimizers.Adam(1.1e-3),\n",
    "        # loss=\"categorical_crossentropy\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        # loss=\"categorical_crossentropy\",\n",
    "        # metrics=[\"accuracy\",dice_coef,\"categorical_accuracy\"],\n",
    "        # metrics=[\"categorical_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=4)],\n",
    "        # metrics=[\"categorical_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=2)],\n",
    "        metrics=[\"binary_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=2)],\n",
    "        # metrics=[\"categorical_accuracy\"],\n",
    ")\n",
    "# CREATE CALLBACKS\n",
    "checkpointClassification = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        checkpointPath / Path('MULTIMODAL_CONCAT_Classification_acc'),\n",
    "                        monitor='val_binary_accuracy', verbose=0, \n",
    "                        # monitor='val_dm_pignistic_16_categorical_accuracy', verbose=0, \n",
    "                        save_best_only=True, mode='max')\n",
    "                                                # CREATE CALLBACKS\n",
    "\n",
    "\n",
    "checkpointClassificationF1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        checkpointPath / Path('MULTIMODAL_CONCAT_Classification_F1'), \n",
    "                        monitor='val_f1_score', verbose=0, \n",
    "                        save_best_only=True, mode='max')\n",
    "                                                # CREATE CALLBACKS\n",
    "\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "\n",
    "    \n",
    "# lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "\n",
    "cb = TimingCallback()\n",
    "\n",
    "print(checkpointPath / Path('MULTIMODAL_CONCAT'))\n",
    "\n",
    "# callbacks_list = [checkpointClassification]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb, schedule_lr,lrate]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb, schedule_lr]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, cb, schedule_lr]\n",
    "callbacks_list = [checkpointClassification, checkpointClassificationF1, cb]\n",
    "# callbacks_list = [checkpointClassification, cb]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb]\n",
    "# callbacks_list = [checkpointClassificationF1, earlyStopping]\n",
    "\n",
    "history = concatMultimodal.fit(\n",
    "        # x = X_train_IMGS,\n",
    "        # trainset,\n",
    "        x = [X_train_IMGS,X_train],\n",
    "        # y = [Y_train,Y_tr_unc],\n",
    "        y = Y_train,\n",
    "        # epochs = 1,\n",
    "        epochs = 200,\n",
    "        validation_data = [[X_val_IMGS,X_val],Y_val],\n",
    "        # validation_data = [X_val_IMGS,[Y_val,Y_val_unc]],\n",
    "        # validation_data = validset,\n",
    "        callbacks = callbacks_list,    #some problems with DS layers and callbacks\n",
    "        batch_size=32,\n",
    "        # batch_size=1,\n",
    "        # class_weight=c_w,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train loss",
         "type": "scatter",
         "y": [
          0.7217127680778503,
          0.6783889532089233,
          0.6351072788238525,
          0.6008964776992798,
          0.5653701424598694,
          0.5356369018554688,
          0.5124208927154541,
          0.4740014672279358,
          0.43583589792251587,
          0.4355274736881256,
          0.4521560072898865,
          0.43783465027809143,
          0.43300938606262207,
          0.42901611328125,
          0.41975200176239014,
          0.4310772120952606,
          0.42484408617019653,
          0.41828861832618713,
          0.43856972455978394,
          0.42186108231544495,
          0.44346776604652405,
          0.42466652393341064,
          0.43722084164619446,
          0.4249841570854187,
          0.4321867823600769,
          0.41401833295822144,
          0.4351832866668701,
          0.4186629056930542,
          0.42034754157066345,
          0.43193745613098145,
          0.4214518964290619,
          0.40823525190353394,
          0.4250415563583374,
          0.43408045172691345,
          0.42562273144721985,
          0.4229738712310791,
          0.4283638596534729,
          0.4485872685909271,
          0.428633451461792,
          0.42984461784362793,
          0.41998034715652466,
          0.4359414279460907,
          0.41852274537086487,
          0.41969048976898193,
          0.44861888885498047,
          0.42848294973373413,
          0.4242444634437561,
          0.421200692653656,
          0.42720532417297363,
          0.44034644961357117,
          0.4318840503692627,
          0.4445515275001526,
          0.40815964341163635,
          0.39830780029296875,
          0.42429178953170776,
          0.42115816473960876,
          0.42361053824424744,
          0.4186866581439972,
          0.4231038987636566,
          0.46017399430274963,
          0.45049813389778137,
          0.4135165512561798,
          0.4288803040981293,
          0.42664873600006104,
          0.4170648455619812,
          0.42713019251823425,
          0.41146785020828247,
          0.4202505648136139,
          0.4431760013103485,
          0.4203824996948242,
          0.4512310326099396,
          0.4274943172931671,
          0.44541075825691223,
          0.43593814969062805,
          0.4360087811946869,
          0.40035709738731384,
          0.4271693527698517,
          0.4223865270614624,
          0.46358737349510193,
          0.4139162003993988,
          0.39243191480636597,
          0.41721364855766296,
          0.4282079339027405,
          0.43202298879623413,
          0.4238368570804596,
          0.46827563643455505,
          0.4056861102581024,
          0.4409533143043518,
          0.43307772278785706,
          0.41418007016181946,
          0.42770326137542725,
          0.43018925189971924,
          0.4304434061050415,
          0.40561002492904663,
          0.4078008532524109,
          0.43270593881607056,
          0.42359262704849243,
          0.4362281560897827,
          0.4288342297077179,
          0.4316076934337616,
          0.43515312671661377,
          0.4450635015964508,
          0.4383445382118225,
          0.43096959590911865,
          0.44335588812828064,
          0.4330670237541199,
          0.41589850187301636,
          0.4214385151863098,
          0.4343623220920563,
          0.43166011571884155,
          0.4041685163974762,
          0.41788601875305176,
          0.4432220160961151,
          0.4357037842273712,
          0.44185924530029297,
          0.4069787859916687,
          0.43101438879966736,
          0.4538920521736145,
          0.4429171085357666,
          0.42370662093162537,
          0.42415982484817505,
          0.40668395161628723,
          0.4082850515842438,
          0.4349289834499359,
          0.43430161476135254,
          0.4300355315208435,
          0.4399650990962982,
          0.4157634377479553,
          0.4513365924358368,
          0.41799941658973694,
          0.4078475832939148,
          0.4298567473888397,
          0.44116339087486267,
          0.43020033836364746,
          0.4177227020263672,
          0.4263182580471039,
          0.4038624167442322,
          0.43353012204170227,
          0.4074755609035492,
          0.4337565302848816,
          0.42840391397476196,
          0.41686683893203735,
          0.4248473644256592,
          0.4361988604068756,
          0.4330359399318695,
          0.420437216758728,
          0.4042409360408783,
          0.4328661561012268,
          0.46326640248298645,
          0.4256953001022339,
          0.4320015013217926,
          0.4268225431442261,
          0.43310612440109253,
          0.42836901545524597,
          0.4332796633243561,
          0.42095720767974854,
          0.40499866008758545,
          0.43301162123680115,
          0.4516631066799164,
          0.4217602610588074,
          0.4157879054546356,
          0.4279954433441162,
          0.4276736378669739,
          0.41498082876205444,
          0.4371158182621002,
          0.41668784618377686,
          0.43411022424697876,
          0.42253929376602173,
          0.415417343378067,
          0.4174143075942993,
          0.43483269214630127,
          0.4308744966983795,
          0.43582937121391296,
          0.4472353458404541,
          0.4168187379837036,
          0.43085259199142456,
          0.44510117173194885,
          0.45502543449401855,
          0.42654111981391907,
          0.44406190514564514,
          0.4375883638858795,
          0.44073453545570374,
          0.41790130734443665,
          0.4400852620601654,
          0.4511241912841797,
          0.4249199628829956,
          0.4292418956756592,
          0.4520874619483948,
          0.4107781648635864,
          0.43721097707748413,
          0.40621697902679443,
          0.4190347492694855,
          0.4091174900531769,
          0.41521263122558594,
          0.424331396818161,
          0.4337660074234009,
          0.424403578042984,
          0.4025769829750061,
          0.43231791257858276,
          0.39777234196662903,
          0.411251425743103,
          0.4375075697898865,
          0.4291541576385498,
          0.4320901334285736,
          0.4431051015853882,
          0.4271717667579651,
          0.43786829710006714,
          0.44580623507499695,
          0.42511284351348877,
          0.42440980672836304,
          0.4110880494117737,
          0.4346521198749542,
          0.41938498616218567,
          0.4034915566444397,
          0.4231259226799011,
          0.4188360869884491,
          0.4051891267299652,
          0.4298001527786255,
          0.4051192104816437,
          0.4255038797855377,
          0.4401804804801941,
          0.42611128091812134,
          0.429136723279953,
          0.4318675696849823,
          0.4287814199924469,
          0.4334283173084259,
          0.4196881651878357,
          0.43153470754623413,
          0.39518532156944275,
          0.4270791709423065,
          0.40197864174842834,
          0.4205837547779083,
          0.44198131561279297,
          0.4191102981567383,
          0.41292351484298706,
          0.4262092113494873,
          0.432610422372818,
          0.4348681569099426,
          0.4376504719257355,
          0.4144597351551056,
          0.41578182578086853,
          0.4196555018424988,
          0.44591057300567627,
          0.4365941286087036,
          0.4177758991718292,
          0.4321431815624237,
          0.42943257093429565,
          0.4066075384616852,
          0.42280057072639465,
          0.4248768389225006,
          0.42136409878730774,
          0.4256848990917206,
          0.4301071763038635,
          0.4326469302177429,
          0.41235047578811646,
          0.4340474605560303,
          0.42867526412010193,
          0.41304728388786316,
          0.4301019012928009,
          0.4037571847438812,
          0.4516053795814514,
          0.42417460680007935,
          0.42050865292549133,
          0.4180890917778015,
          0.4159056544303894,
          0.43188798427581787,
          0.39987263083457947,
          0.431804895401001,
          0.4208167791366577,
          0.46251633763313293,
          0.44456571340560913,
          0.4291785955429077,
          0.4137752652168274,
          0.4245102107524872,
          0.4278172254562378,
          0.4216427206993103,
          0.4262685775756836,
          0.43375423550605774,
          0.43786075711250305,
          0.4063941538333893,
          0.421933114528656,
          0.4292886555194855,
          0.4455339014530182,
          0.44974905252456665,
          0.4077567756175995,
          0.4494287669658661,
          0.4603644609451294,
          0.4320375621318817,
          0.4272853136062622,
          0.4249730706214905,
          0.42173558473587036,
          0.41125771403312683,
          0.42889338731765747,
          0.3958119750022888,
          0.39812666177749634,
          0.43451517820358276,
          0.4045581519603729,
          0.4171718955039978,
          0.42879560589790344,
          0.40982919931411743,
          0.41450393199920654,
          0.4253961741924286,
          0.43487265706062317,
          0.44287168979644775,
          0.41538748145103455,
          0.4350200891494751,
          0.44411662220954895,
          0.43102091550827026,
          0.4267982840538025,
          0.42439982295036316,
          0.44444069266319275,
          0.43492990732192993,
          0.45951953530311584,
          0.4470050632953644,
          0.4397142231464386,
          0.4379822313785553,
          0.4170643091201782,
          0.436076283454895,
          0.4467335343360901,
          0.43774378299713135,
          0.43412619829177856,
          0.4114237427711487,
          0.4307669699192047,
          0.43475446105003357,
          0.4080842435359955,
          0.4245683252811432,
          0.417826384305954,
          0.4422346353530884,
          0.42357006669044495,
          0.4237876236438751,
          0.3810964524745941,
          0.4291937053203583,
          0.4417261779308319,
          0.41642463207244873,
          0.4401208460330963,
          0.4486694037914276,
          0.42405083775520325,
          0.42511847615242004,
          0.426828533411026,
          0.4333728849887848,
          0.4466351270675659,
          0.4153929054737091,
          0.423453152179718,
          0.42784491181373596,
          0.438406765460968,
          0.4570885896682739,
          0.4197065532207489,
          0.43697240948677063,
          0.40460434556007385,
          0.4261828362941742,
          0.436626672744751,
          0.4359511733055115,
          0.43212732672691345,
          0.4186030626296997,
          0.4320143461227417,
          0.4366243779659271,
          0.40652352571487427,
          0.46402448415756226,
          0.4257071018218994,
          0.4286615550518036,
          0.4329583942890167,
          0.43709683418273926,
          0.4272758662700653,
          0.4306435286998749,
          0.41820213198661804,
          0.4535350203514099,
          0.43009600043296814,
          0.4489869475364685,
          0.4455420970916748,
          0.4554302990436554,
          0.4399447739124298,
          0.39622700214385986,
          0.4174463748931885,
          0.43806543946266174,
          0.41918060183525085,
          0.4133436381816864,
          0.447848379611969,
          0.4273087978363037,
          0.4290982186794281,
          0.4315595030784607,
          0.41699138283729553,
          0.4169180989265442,
          0.4357598125934601,
          0.4105779528617859,
          0.43898648023605347,
          0.42989495396614075,
          0.4178268313407898,
          0.4423447251319885,
          0.4282064437866211,
          0.42940616607666016,
          0.42687228322029114,
          0.4274462163448334,
          0.4478740096092224,
          0.42165249586105347,
          0.40332335233688354,
          0.4125573933124542,
          0.44510743021965027,
          0.4202239215373993,
          0.42486512660980225,
          0.4354153573513031,
          0.42919567227363586,
          0.43804532289505005,
          0.45461446046829224,
          0.42886683344841003,
          0.43919283151626587,
          0.4286823868751526,
          0.4281831383705139,
          0.4193912744522095,
          0.43096891045570374,
          0.4342438876628876,
          0.4334852695465088,
          0.44159749150276184,
          0.4342918395996094,
          0.43493011593818665,
          0.4163493514060974,
          0.44584280252456665,
          0.4208410382270813,
          0.4275907576084137,
          0.4380033016204834,
          0.43460598587989807,
          0.4450249969959259,
          0.4272584617137909,
          0.4207805097103119,
          0.43150484561920166,
          0.4229053556919098,
          0.4393288493156433,
          0.41325411200523376,
          0.41581177711486816,
          0.43111497163772583,
          0.4543898105621338,
          0.4205182194709778,
          0.42814216017723083,
          0.42484739422798157,
          0.4168184995651245,
          0.4284796118736267,
          0.4093846380710602,
          0.4081704020500183,
          0.43342897295951843,
          0.40803655982017517,
          0.4000585973262787,
          0.4521491527557373,
          0.4348267912864685,
          0.4207189083099365,
          0.429930180311203,
          0.43123042583465576,
          0.44388604164123535,
          0.43223121762275696,
          0.42093625664711,
          0.41745126247406006,
          0.4518852233886719,
          0.43376830220222473,
          0.4280799627304077,
          0.4420093297958374,
          0.4471072852611542,
          0.4342649579048157,
          0.44067642092704773,
          0.43549904227256775,
          0.4201318025588989,
          0.44233548641204834,
          0.45333558320999146,
          0.4393238425254822,
          0.408090204000473,
          0.42244842648506165,
          0.43321093916893005,
          0.43148642778396606,
          0.444011390209198,
          0.44591787457466125,
          0.42709583044052124,
          0.45739102363586426,
          0.39560744166374207,
          0.43203356862068176,
          0.42418545484542847,
          0.42325395345687866,
          0.4174126982688904,
          0.4289073646068573,
          0.41250982880592346,
          0.431761234998703,
          0.4226512908935547,
          0.42373916506767273,
          0.4172534942626953,
          0.42422398924827576,
          0.414863258600235,
          0.4410998523235321,
          0.4338335692882538,
          0.42308324575424194,
          0.41916424036026,
          0.41862308979034424,
          0.4223867654800415,
          0.4288876950740814,
          0.413838267326355,
          0.43681320548057556,
          0.4381639063358307,
          0.4328378140926361,
          0.4647660553455353,
          0.424326092004776,
          0.4375791847705841,
          0.44003742933273315,
          0.41027939319610596,
          0.4588847756385803,
          0.4447072446346283,
          0.43342578411102295,
          0.43644165992736816,
          0.409971684217453,
          0.43745291233062744,
          0.41390344500541687,
          0.4084950089454651,
          0.4439767301082611,
          0.4497946500778198,
          0.44399628043174744,
          0.442496120929718,
          0.424528568983078,
          0.4475008249282837,
          0.4280990660190582,
          0.4204660654067993,
          0.43735605478286743,
          0.41040128469467163,
          0.42578157782554626,
          0.40343931317329407,
          0.45023512840270996,
          0.4089217782020569,
          0.4164116084575653,
          0.4077700078487396,
          0.43449220061302185,
          0.44137370586395264,
          0.45041653513908386,
          0.4283764958381653,
          0.43584075570106506,
          0.42446285486221313,
          0.4376308023929596,
          0.43008726835250854,
          0.4257591962814331,
          0.4214913547039032,
          0.41615501046180725,
          0.4293498396873474,
          0.43171626329421997,
          0.45678219199180603,
          0.4235362708568573,
          0.4288778007030487,
          0.43149957060813904,
          0.43816062808036804,
          0.43181121349334717,
          0.4349524676799774,
          0.43875715136528015,
          0.41801905632019043,
          0.41179677844047546,
          0.4409911334514618,
          0.44786572456359863,
          0.4209824502468109,
          0.41102248430252075,
          0.4276355803012848,
          0.44717860221862793,
          0.427486389875412,
          0.44574326276779175,
          0.41719701886177063,
          0.43167203664779663,
          0.42739394307136536,
          0.4060947895050049,
          0.42242172360420227,
          0.4194977283477783,
          0.4281409680843353,
          0.4490942060947418,
          0.42742598056793213,
          0.43989112973213196,
          0.4387443959712982,
          0.4192449450492859,
          0.4476408064365387,
          0.43005993962287903,
          0.4161315858364105,
          0.4240266978740692,
          0.4267427921295166,
          0.4264460802078247,
          0.43813255429267883,
          0.43520352244377136,
          0.4271700382232666,
          0.41407090425491333,
          0.41956841945648193,
          0.4423646330833435,
          0.4375137388706207,
          0.4236408472061157,
          0.40144699811935425,
          0.42449429631233215,
          0.4145090878009796,
          0.4349896013736725,
          0.4256715178489685,
          0.4267694354057312,
          0.4389750063419342,
          0.41871902346611023,
          0.4074659049510956,
          0.4065053164958954,
          0.4486486315727234,
          0.42233502864837646,
          0.40802350640296936,
          0.44468486309051514,
          0.4116407334804535,
          0.4223998188972473,
          0.43378207087516785,
          0.42010149359703064,
          0.41972094774246216,
          0.4347175061702728,
          0.42496559023857117,
          0.42437517642974854,
          0.42322736978530884,
          0.43390655517578125,
          0.446468323469162,
          0.4441899359226227,
          0.4149235785007477,
          0.41753819584846497,
          0.4145767092704773,
          0.4398912489414215,
          0.42538440227508545,
          0.4201968014240265,
          0.42746397852897644,
          0.4348938763141632,
          0.4173136055469513,
          0.4264073073863983,
          0.41692012548446655,
          0.43470779061317444,
          0.44699522852897644,
          0.4271262288093567,
          0.4265443682670593,
          0.41652172803878784,
          0.42369693517684937,
          0.40841883420944214,
          0.4477854073047638,
          0.4183286130428314,
          0.4402238130569458,
          0.41940557956695557,
          0.42537111043930054,
          0.4365425705909729,
          0.43543604016304016,
          0.4294106662273407,
          0.4007147550582886,
          0.4017827808856964,
          0.44197648763656616,
          0.41764387488365173,
          0.4202064871788025,
          0.41926538944244385,
          0.45222610235214233,
          0.41551968455314636,
          0.42237231135368347,
          0.4231106638908386,
          0.44269296526908875,
          0.41590818762779236,
          0.4268357753753662,
          0.43320348858833313,
          0.39838430285453796,
          0.4351175129413605,
          0.4332965612411499,
          0.42244431376457214,
          0.4388664960861206,
          0.4167402684688568,
          0.4352624714374542,
          0.42765071988105774,
          0.4457736909389496,
          0.40450215339660645,
          0.4346131384372711,
          0.4158842861652374,
          0.4334704279899597,
          0.4250250458717346,
          0.42957061529159546,
          0.43936097621917725,
          0.4290458858013153,
          0.4415205717086792,
          0.41498273611068726,
          0.40126776695251465,
          0.4530471861362457,
          0.4396201968193054,
          0.420334130525589,
          0.41761019825935364,
          0.4300156235694885,
          0.4157218933105469,
          0.40891551971435547,
          0.4377874732017517,
          0.42589637637138367,
          0.46149513125419617,
          0.429504930973053,
          0.41932427883148193,
          0.43258413672447205,
          0.42969998717308044,
          0.43012896180152893,
          0.4281249940395355,
          0.43094903230667114,
          0.41307657957077026,
          0.44936466217041016,
          0.43578261137008667,
          0.4047864079475403,
          0.4390706717967987,
          0.44084686040878296,
          0.4183051288127899,
          0.42649129033088684,
          0.4291507303714752,
          0.4348328709602356,
          0.4121500849723816,
          0.43257561326026917,
          0.4328230619430542,
          0.4225856363773346,
          0.44265279173851013,
          0.42500782012939453,
          0.4346255660057068,
          0.4327203333377838,
          0.4143054485321045,
          0.43167057633399963,
          0.4263249337673187,
          0.41262269020080566,
          0.42732319235801697,
          0.4299212396144867,
          0.433028906583786,
          0.4331999123096466,
          0.42884987592697144,
          0.41581326723098755,
          0.3937591314315796,
          0.44418802857398987,
          0.4188743531703949,
          0.45377644896507263,
          0.4476894736289978,
          0.42664197087287903,
          0.43186667561531067,
          0.42657050490379333,
          0.42591720819473267,
          0.44434264302253723,
          0.42412683367729187,
          0.4144609272480011,
          0.4204879105091095,
          0.4246128797531128,
          0.40336158871650696,
          0.4470703899860382,
          0.42214763164520264,
          0.41536247730255127,
          0.43427348136901855,
          0.4231184422969818,
          0.4383021891117096,
          0.4226468801498413,
          0.4446007013320923,
          0.41881299018859863,
          0.42759817838668823,
          0.4160197675228119,
          0.4044856131076813,
          0.4278946816921234,
          0.4135723412036896,
          0.4189303517341614,
          0.41827940940856934,
          0.42125198245048523,
          0.4456835389137268,
          0.4143676161766052,
          0.42715221643447876,
          0.44064411520957947,
          0.42628759145736694,
          0.42644256353378296,
          0.4269658029079437,
          0.4306037127971649,
          0.4249335527420044,
          0.405329167842865,
          0.43640032410621643,
          0.40599149465560913,
          0.4171123802661896,
          0.44027000665664673,
          0.4207637310028076,
          0.4117622673511505,
          0.40876758098602295,
          0.42029494047164917,
          0.41485142707824707,
          0.41908761858940125,
          0.41259849071502686,
          0.4328734278678894,
          0.4382040798664093,
          0.4655970335006714,
          0.46368876099586487,
          0.43960192799568176,
          0.42445477843284607,
          0.41764765977859497,
          0.4103612005710602,
          0.4253184497356415,
          0.4160434305667877,
          0.44530996680259705,
          0.4494555592536926,
          0.4213121831417084,
          0.44043296575546265,
          0.4301869869232178,
          0.4128539562225342,
          0.4334351718425751,
          0.42652446031570435,
          0.43858465552330017,
          0.43548810482025146,
          0.4247641861438751,
          0.4277386963367462,
          0.4343923032283783,
          0.4317159056663513,
          0.4371427297592163,
          0.4357973337173462,
          0.44235745072364807,
          0.44409269094467163,
          0.40548834204673767,
          0.4262268543243408,
          0.41842666268348694,
          0.43201643228530884,
          0.4316107928752899,
          0.43087536096572876,
          0.41748759150505066,
          0.4078282415866852,
          0.44447118043899536,
          0.4270850121974945,
          0.4308076500892639,
          0.40101006627082825,
          0.4417138993740082,
          0.4338684678077698,
          0.4269564151763916,
          0.442290723323822,
          0.4114544093608856,
          0.42890048027038574,
          0.40668997168540955,
          0.4226449131965637,
          0.4257434010505676,
          0.4248034954071045,
          0.42521119117736816,
          0.4314958453178406,
          0.4285367429256439,
          0.42875415086746216,
          0.3974887430667877,
          0.4271911680698395,
          0.4128974378108978,
          0.4178943336009979,
          0.4145579934120178,
          0.4397348463535309,
          0.45264989137649536,
          0.4390946626663208,
          0.42512983083724976,
          0.44458213448524475,
          0.41885247826576233,
          0.4350014925003052,
          0.42224255204200745,
          0.42205870151519775,
          0.4158002436161041,
          0.42396026849746704,
          0.4507322609424591,
          0.4269949197769165,
          0.4471744894981384,
          0.4154171645641327,
          0.4420619308948517,
          0.41883692145347595,
          0.4432469308376312,
          0.42065170407295227,
          0.41270720958709717,
          0.42057421803474426,
          0.41929224133491516,
          0.4259965121746063,
          0.4319572150707245,
          0.4413367211818695,
          0.4169117510318756,
          0.4490932822227478,
          0.4452442526817322,
          0.43333205580711365,
          0.4366631805896759,
          0.4320102632045746,
          0.41588613390922546,
          0.42094284296035767,
          0.4181596040725708,
          0.4410463571548462,
          0.429829478263855,
          0.41577833890914917,
          0.4251234829425812,
          0.4199081063270569,
          0.4343588948249817,
          0.41196268796920776,
          0.39950045943260193,
          0.4173359274864197,
          0.4083412289619446,
          0.42783570289611816,
          0.41569703817367554,
          0.4309259057044983,
          0.4203212857246399,
          0.43767860531806946,
          0.4459381401538849,
          0.4636717438697815,
          0.42891502380371094,
          0.4317152500152588,
          0.4518865942955017,
          0.4105483591556549,
          0.42883411049842834,
          0.45437249541282654,
          0.43921342492103577,
          0.443316787481308,
          0.4233492612838745,
          0.4171772599220276,
          0.4417334794998169,
          0.42021238803863525,
          0.43221768736839294,
          0.4070363938808441,
          0.42156726121902466,
          0.44073614478111267,
          0.4203052818775177,
          0.4391513466835022,
          0.44347819685935974,
          0.41719889640808105,
          0.42308884859085083,
          0.4170042872428894,
          0.4428683817386627,
          0.43131211400032043,
          0.4307827353477478,
          0.4177871346473694,
          0.4344770014286041,
          0.43607646226882935,
          0.3878268301486969,
          0.4212062358856201,
          0.42292600870132446,
          0.43739256262779236,
          0.4195573925971985,
          0.4109431505203247,
          0.4242945909500122,
          0.45019733905792236,
          0.4182589650154114,
          0.4414128363132477,
          0.428710401058197,
          0.3959275782108307,
          0.4265281558036804,
          0.42787542939186096,
          0.4334334433078766,
          0.42762285470962524,
          0.41659119725227356,
          0.4373730421066284,
          0.45050129294395447,
          0.43555882573127747,
          0.42179203033447266,
          0.40663033723831177,
          0.4128401577472687,
          0.4185374677181244,
          0.4396502673625946,
          0.4291054606437683,
          0.4427824020385742,
          0.41959255933761597,
          0.4219450056552887,
          0.43576908111572266,
          0.4337800443172455,
          0.44873905181884766,
          0.43216416239738464,
          0.43464234471321106,
          0.43255534768104553,
          0.41296330094337463,
          0.4225723147392273,
          0.42939990758895874,
          0.42858168482780457,
          0.399828165769577,
          0.4180465638637543,
          0.4161708354949951,
          0.42962032556533813,
          0.4297158718109131,
          0.42574331164360046,
          0.41970524191856384,
          0.42996957898139954,
          0.42780226469039917,
          0.4067781865596771,
          0.41756585240364075,
          0.417178750038147,
          0.3961571455001831,
          0.45520052313804626,
          0.4117353558540344,
          0.4221419394016266,
          0.44166481494903564,
          0.43268656730651855,
          0.4357359707355499,
          0.4137493073940277,
          0.4272485375404358,
          0.3985667824745178,
          0.44194114208221436,
          0.45647260546684265,
          0.44096192717552185,
          0.43352219462394714,
          0.44594693183898926,
          0.42988863587379456,
          0.41864922642707825,
          0.43767252564430237,
          0.4441322088241577,
          0.43646225333213806,
          0.4290961027145386,
          0.4321523606777191,
          0.45125192403793335,
          0.43511149287223816,
          0.42362138628959656,
          0.444452702999115,
          0.4303794503211975,
          0.4095781445503235,
          0.4371887147426605,
          0.4118303060531616,
          0.41048869490623474,
          0.41567686200141907,
          0.4490565359592438,
          0.4156935513019562,
          0.4403823912143707,
          0.4345119893550873,
          0.4107624888420105,
          0.4239208400249481,
          0.3974088728427887,
          0.39301687479019165,
          0.4388306438922882,
          0.4503859281539917,
          0.4246862232685089,
          0.4340782165527344,
          0.42252644896507263,
          0.41402047872543335,
          0.42196258902549744,
          0.4098687767982483
         ]
        },
        {
         "name": "Validation loss",
         "type": "scatter",
         "y": [
          0.7073923945426941,
          0.6914979815483093,
          0.6708167195320129,
          0.6456699967384338,
          0.6184223890304565,
          0.5894551277160645,
          0.5614304542541504,
          0.5368661880493164,
          0.5165007710456848,
          0.5016018748283386,
          0.4917905032634735,
          0.4837985336780548,
          0.47611019015312195,
          0.4713049530982971,
          0.4677528440952301,
          0.46627452969551086,
          0.46453410387039185,
          0.46352118253707886,
          0.46334969997406006,
          0.46274620294570923,
          0.46219727396965027,
          0.4626041352748871,
          0.4633590877056122,
          0.4636596143245697,
          0.4627237617969513,
          0.4635259807109833,
          0.46317118406295776,
          0.46379756927490234,
          0.4631761908531189,
          0.46280357241630554,
          0.46352896094322205,
          0.46426331996917725,
          0.463392436504364,
          0.46308472752571106,
          0.46330881118774414,
          0.46271851658821106,
          0.46273887157440186,
          0.4628041684627533,
          0.463421106338501,
          0.46319687366485596,
          0.46305665373802185,
          0.4629693329334259,
          0.463243305683136,
          0.4635079503059387,
          0.46226778626441956,
          0.4627528488636017,
          0.46287938952445984,
          0.46311405301094055,
          0.4628411829471588,
          0.46263059973716736,
          0.463373064994812,
          0.46517422795295715,
          0.4668368101119995,
          0.46880510449409485,
          0.4652014374732971,
          0.4642273187637329,
          0.46438467502593994,
          0.4657229781150818,
          0.46604329347610474,
          0.46501344442367554,
          0.46425434947013855,
          0.46391963958740234,
          0.4640122056007385,
          0.46494725346565247,
          0.4641076624393463,
          0.46407565474510193,
          0.46465572714805603,
          0.46441471576690674,
          0.463367223739624,
          0.4629759192466736,
          0.4633384943008423,
          0.4631703495979309,
          0.4635779559612274,
          0.4633179306983948,
          0.4628726840019226,
          0.4633648991584778,
          0.46355727314949036,
          0.4634040594100952,
          0.4627749025821686,
          0.4630510210990906,
          0.46368977427482605,
          0.4642983376979828,
          0.4641202390193939,
          0.4636351764202118,
          0.4633257985115051,
          0.46253782510757446,
          0.4627893269062042,
          0.4625158905982971,
          0.46216121315956116,
          0.462954044342041,
          0.4630751609802246,
          0.4632178544998169,
          0.46291401982307434,
          0.46355772018432617,
          0.46432414650917053,
          0.4636979103088379,
          0.46432313323020935,
          0.4634800851345062,
          0.4627542793750763,
          0.46303993463516235,
          0.46267634630203247,
          0.4626656472682953,
          0.4628702700138092,
          0.4624859094619751,
          0.4622136056423187,
          0.46276697516441345,
          0.4635094404220581,
          0.4634227454662323,
          0.46333566308021545,
          0.463790625333786,
          0.46294108033180237,
          0.4632965624332428,
          0.4629996716976166,
          0.4634322226047516,
          0.4628876745700836,
          0.46336185932159424,
          0.4637351632118225,
          0.4630388021469116,
          0.46242281794548035,
          0.46292880177497864,
          0.46340060234069824,
          0.4636441767215729,
          0.46390607953071594,
          0.4634656608104706,
          0.46324023604393005,
          0.4634488523006439,
          0.4631378948688507,
          0.4633723497390747,
          0.46283894777297974,
          0.46288037300109863,
          0.4635107219219208,
          0.4630056917667389,
          0.46278801560401917,
          0.4629364609718323,
          0.4630136787891388,
          0.46288689970970154,
          0.46371665596961975,
          0.4640304744243622,
          0.4640991687774658,
          0.46405908465385437,
          0.46320223808288574,
          0.4639360010623932,
          0.46376320719718933,
          0.4629053771495819,
          0.4627879858016968,
          0.46333181858062744,
          0.4634455442428589,
          0.46325069665908813,
          0.4623428285121918,
          0.4624939262866974,
          0.46263644099235535,
          0.46284177899360657,
          0.4625852704048157,
          0.4627012312412262,
          0.4631083607673645,
          0.46280530095100403,
          0.4634544551372528,
          0.4630478322505951,
          0.46324509382247925,
          0.46300190687179565,
          0.4628353416919708,
          0.46326449513435364,
          0.4634934961795807,
          0.46347782015800476,
          0.46317538619041443,
          0.4634789824485779,
          0.46331456303596497,
          0.4634155035018921,
          0.46339401602745056,
          0.46321484446525574,
          0.4633222818374634,
          0.4630102515220642,
          0.4635314643383026,
          0.46307572722435,
          0.46359947323799133,
          0.46403375267982483,
          0.46335726976394653,
          0.4624122977256775,
          0.46241647005081177,
          0.4624355435371399,
          0.463082492351532,
          0.46223822236061096,
          0.4623802900314331,
          0.46235209703445435,
          0.4623357653617859,
          0.4634057879447937,
          0.4637294411659241,
          0.4630427062511444,
          0.4629741311073303,
          0.4631008505821228,
          0.4634750187397003,
          0.46346011757850647,
          0.4635785222053528,
          0.46354109048843384,
          0.4634184241294861,
          0.46336591243743896,
          0.4633903205394745,
          0.4635936915874481,
          0.4635927379131317,
          0.4638080894947052,
          0.46456435322761536,
          0.4639832079410553,
          0.46404993534088135,
          0.46384134888648987,
          0.4632706642150879,
          0.4634697735309601,
          0.463466614484787,
          0.46295085549354553,
          0.4628947377204895,
          0.46312326192855835,
          0.462963342666626,
          0.46272847056388855,
          0.46307483315467834,
          0.4638105034828186,
          0.46362924575805664,
          0.4634183347225189,
          0.46332332491874695,
          0.463091105222702,
          0.46358880400657654,
          0.4637191593647003,
          0.4633932411670685,
          0.4628370702266693,
          0.4627837538719177,
          0.46253716945648193,
          0.4624745547771454,
          0.4627463221549988,
          0.46322548389434814,
          0.46296435594558716,
          0.4636918604373932,
          0.4636649191379547,
          0.4637797176837921,
          0.46388980746269226,
          0.46378615498542786,
          0.46363216638565063,
          0.46365663409233093,
          0.46372199058532715,
          0.46346238255500793,
          0.46307462453842163,
          0.46300914883613586,
          0.46322569251060486,
          0.463243693113327,
          0.463290274143219,
          0.4627421498298645,
          0.4628298282623291,
          0.4632630944252014,
          0.46281394362449646,
          0.4626680016517639,
          0.4628368616104126,
          0.46300601959228516,
          0.4631444215774536,
          0.4631515145301819,
          0.4637756943702698,
          0.46280941367149353,
          0.4625099003314972,
          0.4625976085662842,
          0.46211767196655273,
          0.4621853530406952,
          0.4620055854320526,
          0.46241334080696106,
          0.4626874029636383,
          0.46246418356895447,
          0.4621710479259491,
          0.4626123309135437,
          0.4625994563102722,
          0.4626522958278656,
          0.4625740349292755,
          0.46289071440696716,
          0.46303728222846985,
          0.46319687366485596,
          0.46290019154548645,
          0.46247032284736633,
          0.46233630180358887,
          0.4625088572502136,
          0.46249449253082275,
          0.46279802918434143,
          0.46334463357925415,
          0.46305227279663086,
          0.46328336000442505,
          0.4629732072353363,
          0.46293535828590393,
          0.4629027247428894,
          0.4627830982208252,
          0.46288058161735535,
          0.4626166522502899,
          0.46243807673454285,
          0.4624534249305725,
          0.46240898966789246,
          0.4624802768230438,
          0.46263185143470764,
          0.46288394927978516,
          0.4631035625934601,
          0.46315276622772217,
          0.46322426199913025,
          0.463590532541275,
          0.46445292234420776,
          0.4644674062728882,
          0.4646155536174774,
          0.4643290638923645,
          0.46429243683815,
          0.4643523097038269,
          0.4648746848106384,
          0.4643236994743347,
          0.4640686810016632,
          0.463418185710907,
          0.4636653661727905,
          0.4637015461921692,
          0.4631993770599365,
          0.46319475769996643,
          0.4630996286869049,
          0.46308135986328125,
          0.4632066488265991,
          0.4631566107273102,
          0.46261414885520935,
          0.462446004152298,
          0.4627164304256439,
          0.4625997245311737,
          0.46266236901283264,
          0.4628625512123108,
          0.4629483222961426,
          0.46260538697242737,
          0.46249014139175415,
          0.462774395942688,
          0.4633771479129791,
          0.4633641541004181,
          0.46326419711112976,
          0.4631673991680145,
          0.46306899189949036,
          0.46288934350013733,
          0.46292540431022644,
          0.4629901051521301,
          0.46353691816329956,
          0.46365106105804443,
          0.463043749332428,
          0.46308183670043945,
          0.4629400074481964,
          0.4626134932041168,
          0.4623972773551941,
          0.46251344680786133,
          0.46284985542297363,
          0.46273067593574524,
          0.46256721019744873,
          0.4625004231929779,
          0.46246081590652466,
          0.46270376443862915,
          0.4626864790916443,
          0.46227091550827026,
          0.4623565673828125,
          0.46241655945777893,
          0.46226125955581665,
          0.46253737807273865,
          0.4623504877090454,
          0.46248990297317505,
          0.4627893269062042,
          0.4629959166049957,
          0.4632730782032013,
          0.4625825583934784,
          0.46280133724212646,
          0.4626246690750122,
          0.46244269609451294,
          0.4623842239379883,
          0.4626191258430481,
          0.462656170129776,
          0.4625311493873596,
          0.46246716380119324,
          0.46249815821647644,
          0.4624883830547333,
          0.4627229869365692,
          0.46287041902542114,
          0.4623855650424957,
          0.4619341194629669,
          0.46186891198158264,
          0.46203210949897766,
          0.4621830880641937,
          0.4622383117675781,
          0.46252450346946716,
          0.4626008868217468,
          0.46235135197639465,
          0.462166428565979,
          0.46235400438308716,
          0.4623929262161255,
          0.46276751160621643,
          0.4631725549697876,
          0.4632355868816376,
          0.46324053406715393,
          0.463253378868103,
          0.46297281980514526,
          0.4632130563259125,
          0.4630186855792999,
          0.4628642201423645,
          0.4629036784172058,
          0.4628809988498688,
          0.46301206946372986,
          0.4629790484905243,
          0.46292027831077576,
          0.4631975293159485,
          0.46334564685821533,
          0.4630301594734192,
          0.46296727657318115,
          0.4630158245563507,
          0.46309834718704224,
          0.4633512496948242,
          0.4630741775035858,
          0.4628084599971771,
          0.46276184916496277,
          0.46264195442199707,
          0.46284303069114685,
          0.4626227021217346,
          0.46280696988105774,
          0.46260568499565125,
          0.4626895785331726,
          0.4627387225627899,
          0.4627389907836914,
          0.46238693594932556,
          0.4623633027076721,
          0.4624037742614746,
          0.46248379349708557,
          0.4625400900840759,
          0.4625285267829895,
          0.4623253643512726,
          0.46244820952415466,
          0.46248629689216614,
          0.4625726640224457,
          0.46261999011039734,
          0.46303004026412964,
          0.4629615545272827,
          0.4629386365413666,
          0.4627431035041809,
          0.46306416392326355,
          0.4629782736301422,
          0.46271130442619324,
          0.46295034885406494,
          0.4630763828754425,
          0.46310877799987793,
          0.4632277190685272,
          0.4632680416107178,
          0.463335245847702,
          0.46339187026023865,
          0.46319934725761414,
          0.46315115690231323,
          0.4634590148925781,
          0.463739812374115,
          0.4636899530887604,
          0.46320170164108276,
          0.46306267380714417,
          0.4628131687641144,
          0.4625122845172882,
          0.4626299738883972,
          0.4627552330493927,
          0.4627145528793335,
          0.4627748429775238,
          0.4627814292907715,
          0.4628051817417145,
          0.4628199338912964,
          0.4626990258693695,
          0.4626295864582062,
          0.46250832080841064,
          0.4621550440788269,
          0.4622999429702759,
          0.4620874226093292,
          0.4621570408344269,
          0.46206149458885193,
          0.4620603024959564,
          0.46217966079711914,
          0.46230262517929077,
          0.46243923902511597,
          0.46245965361595154,
          0.4621051549911499,
          0.4621492624282837,
          0.46208569407463074,
          0.46205878257751465,
          0.46222978830337524,
          0.4623713493347168,
          0.4621434807777405,
          0.4622873067855835,
          0.4620331823825836,
          0.4621541202068329,
          0.4621771275997162,
          0.4622895121574402,
          0.4623374044895172,
          0.4622500240802765,
          0.4624621272087097,
          0.4622739851474762,
          0.46226170659065247,
          0.46227559447288513,
          0.46243664622306824,
          0.4624181091785431,
          0.462587833404541,
          0.46254074573516846,
          0.46245166659355164,
          0.4626117944717407,
          0.4627137780189514,
          0.4626447558403015,
          0.4626231789588928,
          0.4623810350894928,
          0.46211832761764526,
          0.4620659053325653,
          0.4620853662490845,
          0.4620712995529175,
          0.4618595838546753,
          0.46188485622406006,
          0.4617898166179657,
          0.46166807413101196,
          0.46182897686958313,
          0.4619291424751282,
          0.4621501564979553,
          0.4622558355331421,
          0.4622358977794647,
          0.462141752243042,
          0.46212565898895264,
          0.46220269799232483,
          0.4623151123523712,
          0.46224474906921387,
          0.46230077743530273,
          0.4622316360473633,
          0.46209755539894104,
          0.46211570501327515,
          0.46220457553863525,
          0.4621957540512085,
          0.46214041113853455,
          0.46228715777397156,
          0.4623886048793793,
          0.46260401606559753,
          0.4626138508319855,
          0.46272727847099304,
          0.46242257952690125,
          0.4622279703617096,
          0.4624151885509491,
          0.46231982111930847,
          0.4622845947742462,
          0.4622652232646942,
          0.46227413415908813,
          0.46247589588165283,
          0.4624270796775818,
          0.462313711643219,
          0.4622594714164734,
          0.46237120032310486,
          0.4624457061290741,
          0.46225857734680176,
          0.4621652066707611,
          0.4619181752204895,
          0.46185794472694397,
          0.4619758427143097,
          0.4620538651943207,
          0.462063729763031,
          0.4622131586074829,
          0.46232643723487854,
          0.462158739566803,
          0.4621496796607971,
          0.4622947871685028,
          0.4624977111816406,
          0.46256524324417114,
          0.4628252685070038,
          0.46265751123428345,
          0.4626041650772095,
          0.46257761120796204,
          0.46244633197784424,
          0.4624757468700409,
          0.46270543336868286,
          0.46266934275627136,
          0.4624709486961365,
          0.4625474512577057,
          0.4624097943305969,
          0.46237772703170776,
          0.4625226855278015,
          0.4625483751296997,
          0.4623194634914398,
          0.4624628722667694,
          0.46223387122154236,
          0.4625086784362793,
          0.4623943269252777,
          0.462480366230011,
          0.4625939726829529,
          0.4625335931777954,
          0.4626917839050293,
          0.4627879858016968,
          0.46297481656074524,
          0.4627736806869507,
          0.4629361629486084,
          0.4629617929458618,
          0.4628075361251831,
          0.46272051334381104,
          0.462597519159317,
          0.46240249276161194,
          0.46230989694595337,
          0.46244701743125916,
          0.46236270666122437,
          0.46256551146507263,
          0.46259889006614685,
          0.4628022015094757,
          0.46282675862312317,
          0.46276772022247314,
          0.4627467095851898,
          0.46283653378486633,
          0.4628978371620178,
          0.46287834644317627,
          0.46272462606430054,
          0.46282532811164856,
          0.4627997875213623,
          0.46301209926605225,
          0.4629227817058563,
          0.4629209637641907,
          0.46307626366615295,
          0.46289587020874023,
          0.462847501039505,
          0.4626009464263916,
          0.46247413754463196,
          0.4625881314277649,
          0.4627835750579834,
          0.46272051334381104,
          0.4629429876804352,
          0.4629875421524048,
          0.46271955966949463,
          0.46283188462257385,
          0.46276336908340454,
          0.4627310335636139,
          0.4627983570098877,
          0.4629162847995758,
          0.46296125650405884,
          0.46283525228500366,
          0.4628477990627289,
          0.46268579363822937,
          0.4628545045852661,
          0.4629034996032715,
          0.4628758728504181,
          0.462976336479187,
          0.46310845017433167,
          0.4631606638431549,
          0.4629477560520172,
          0.4627883732318878,
          0.4627653956413269,
          0.46270716190338135,
          0.46295180916786194,
          0.4630034267902374,
          0.4629608988761902,
          0.4631471335887909,
          0.4631502628326416,
          0.46306830644607544,
          0.46293890476226807,
          0.4630110561847687,
          0.4627442955970764,
          0.46285873651504517,
          0.4628615975379944,
          0.46285244822502136,
          0.4628223180770874,
          0.46271613240242004,
          0.46258434653282166,
          0.4628244936466217,
          0.4628317058086395,
          0.4630129933357239,
          0.463123083114624,
          0.46362295746803284,
          0.46335095167160034,
          0.46317628026008606,
          0.46318623423576355,
          0.463137686252594,
          0.4633604884147644,
          0.4630834460258484,
          0.4631931185722351,
          0.463309109210968,
          0.4632490575313568,
          0.4634184241294861,
          0.46323058009147644,
          0.4628661870956421,
          0.46298152208328247,
          0.4629414975643158,
          0.4631640315055847,
          0.4631301164627075,
          0.46293553709983826,
          0.46287399530410767,
          0.4627625644207001,
          0.4628104269504547,
          0.46315962076187134,
          0.46314525604248047,
          0.46304431557655334,
          0.462912917137146,
          0.4628106653690338,
          0.4628378748893738,
          0.46275681257247925,
          0.46277475357055664,
          0.4629000425338745,
          0.4628123641014099,
          0.4630485773086548,
          0.4630061388015747,
          0.4631226062774658,
          0.46296554803848267,
          0.46299096941947937,
          0.462715208530426,
          0.46277865767478943,
          0.46264734864234924,
          0.46261101961135864,
          0.46270954608917236,
          0.4626556634902954,
          0.46288007497787476,
          0.46316128969192505,
          0.463121622800827,
          0.4630187153816223,
          0.4631038010120392,
          0.46299469470977783,
          0.46308958530426025,
          0.4630974233150482,
          0.46309635043144226,
          0.4629291296005249,
          0.46302056312561035,
          0.4630098044872284,
          0.4629277288913727,
          0.46308669447898865,
          0.46310195326805115,
          0.4630856215953827,
          0.46316638588905334,
          0.4630938470363617,
          0.46302857995033264,
          0.4630579352378845,
          0.4631810188293457,
          0.46321436762809753,
          0.46325552463531494,
          0.46325138211250305,
          0.4632278382778168,
          0.46308234333992004,
          0.4628847539424896,
          0.46303460001945496,
          0.46285849809646606,
          0.46301233768463135,
          0.4629632830619812,
          0.4628717005252838,
          0.46286094188690186,
          0.4629729092121124,
          0.463003933429718,
          0.4630295932292938,
          0.4630129933357239,
          0.4628963768482208,
          0.4629213213920593,
          0.46279123425483704,
          0.46276241540908813,
          0.4626041352748871,
          0.4627649486064911,
          0.4628918170928955,
          0.4629899561405182,
          0.46327120065689087,
          0.46328166127204895,
          0.46333178877830505,
          0.4634708762168884,
          0.46328204870224,
          0.46320751309394836,
          0.4629533886909485,
          0.4629667103290558,
          0.46297523379325867,
          0.46291956305503845,
          0.4626540243625641,
          0.46274542808532715,
          0.4628629684448242,
          0.4632020592689514,
          0.46333402395248413,
          0.46311986446380615,
          0.4630121886730194,
          0.46282389760017395,
          0.4629419147968292,
          0.4630070626735687,
          0.4628446698188782,
          0.4630623757839203,
          0.463262140750885,
          0.4631950855255127,
          0.46311458945274353,
          0.46320396661758423,
          0.4631349742412567,
          0.46314966678619385,
          0.46324342489242554,
          0.46317973732948303,
          0.4629583954811096,
          0.4630564749240875,
          0.4630216658115387,
          0.46292659640312195,
          0.4629262089729309,
          0.4629358649253845,
          0.46311068534851074,
          0.46321699023246765,
          0.4632706642150879,
          0.463043749332428,
          0.46311166882514954,
          0.4630362093448639,
          0.4629640281200409,
          0.4629550278186798,
          0.46307626366615295,
          0.46307820081710815,
          0.46322906017303467,
          0.46316102147102356,
          0.462969034910202,
          0.4629743993282318,
          0.4630381166934967,
          0.4628431499004364,
          0.4628194272518158,
          0.46279463171958923,
          0.46273449063301086,
          0.46275830268859863,
          0.46270495653152466,
          0.4627715051174164,
          0.46275827288627625,
          0.4628428518772125,
          0.462613970041275,
          0.4625518023967743,
          0.4625092148780823,
          0.4625401794910431,
          0.46256712079048157,
          0.462820440530777,
          0.4627688527107239,
          0.462701678276062,
          0.46271735429763794,
          0.4629460871219635,
          0.4628409743309021,
          0.46281924843788147,
          0.4626918137073517,
          0.4627676010131836,
          0.4628169536590576,
          0.46273624897003174,
          0.4626182019710541,
          0.46270373463630676,
          0.4627450108528137,
          0.4627576470375061,
          0.4628230631351471,
          0.46276795864105225,
          0.46276572346687317,
          0.462807297706604,
          0.4629262387752533,
          0.46328336000442505,
          0.46317875385284424,
          0.46315088868141174,
          0.46316835284233093,
          0.4631977677345276,
          0.46307456493377686,
          0.4629841148853302,
          0.46267759799957275,
          0.46262696385383606,
          0.4628109633922577,
          0.46284353733062744,
          0.4628414213657379,
          0.4628162980079651,
          0.46289193630218506,
          0.4627764821052551,
          0.46288207173347473,
          0.46265703439712524,
          0.46252843737602234,
          0.4626217484474182,
          0.4625859260559082,
          0.4627057611942291,
          0.46261683106422424,
          0.4626949429512024,
          0.4628099203109741,
          0.4627365171909332,
          0.46265989542007446,
          0.4626156687736511,
          0.4627577066421509,
          0.46284201741218567,
          0.46278485655784607,
          0.4630105793476105,
          0.46283459663391113,
          0.4627223312854767,
          0.46277257800102234,
          0.46274104714393616,
          0.4627118706703186,
          0.4628300070762634,
          0.4625811278820038,
          0.4623752236366272,
          0.46236738562583923,
          0.4624864161014557,
          0.46251145005226135,
          0.46257108449935913,
          0.46264219284057617,
          0.4626724421977997,
          0.46284031867980957,
          0.4626873731613159,
          0.4627690613269806,
          0.4627281725406647,
          0.462729275226593,
          0.4627380073070526,
          0.46260321140289307,
          0.4625525176525116,
          0.46271708607673645,
          0.4626958668231964,
          0.46282199025154114,
          0.4628971815109253,
          0.46283286809921265,
          0.46274587512016296,
          0.4626612663269043,
          0.4625081717967987,
          0.4626436233520508,
          0.4627203047275543,
          0.46248677372932434,
          0.46235665678977966,
          0.46244484186172485,
          0.4626874625682831,
          0.4626430571079254,
          0.4625948667526245,
          0.46265098452568054,
          0.4625573754310608,
          0.46260738372802734,
          0.4624829888343811,
          0.4627777636051178,
          0.4628432095050812,
          0.4627520442008972,
          0.4627428948879242,
          0.4626561403274536,
          0.46275800466537476,
          0.4627467393875122,
          0.4627070426940918,
          0.46262091398239136,
          0.46265047788619995,
          0.4626292288303375,
          0.46259087324142456,
          0.46263211965560913,
          0.4627446234226227,
          0.46270978450775146,
          0.46268171072006226,
          0.46258261799812317,
          0.4626927971839905,
          0.46279510855674744,
          0.46289390325546265,
          0.4627654254436493,
          0.4628394544124603,
          0.46269750595092773,
          0.46256956458091736,
          0.462593138217926,
          0.4627774655818939,
          0.46267271041870117,
          0.4628336429595947,
          0.46298208832740784,
          0.46301785111427307,
          0.46302729845046997,
          0.46290668845176697,
          0.4629329442977905,
          0.4628569483757019,
          0.462867796421051,
          0.4628185033798218,
          0.46264901757240295,
          0.4626203179359436,
          0.46265605092048645,
          0.462872177362442,
          0.46284183859825134,
          0.46261173486709595,
          0.4625457227230072,
          0.46234750747680664,
          0.4623967707157135,
          0.46249157190322876,
          0.4625157415866852,
          0.4625440239906311,
          0.46266043186187744,
          0.46268120408058167,
          0.4626767337322235,
          0.4627978205680847,
          0.4627247750759125,
          0.46284037828445435,
          0.46265968680381775,
          0.46283724904060364,
          0.4628574252128601,
          0.463021844625473,
          0.4629655182361603,
          0.46279752254486084,
          0.46279802918434143,
          0.46297022700309753,
          0.4628862738609314,
          0.46274298429489136,
          0.4627961218357086,
          0.46304094791412354,
          0.4628920257091522,
          0.4627365171909332,
          0.4627719521522522,
          0.4629451036453247,
          0.4629119634628296,
          0.46288928389549255,
          0.4627801477909088,
          0.4626673460006714,
          0.4626973569393158,
          0.46257123351097107,
          0.46265727281570435,
          0.4628925025463104,
          0.4628979563713074,
          0.4628797769546509,
          0.4626104235649109,
          0.46265867352485657,
          0.4626842737197876,
          0.46244317293167114,
          0.46231335401535034,
          0.4625384211540222,
          0.4625171720981598,
          0.46263396739959717,
          0.4625193476676941,
          0.46243587136268616,
          0.46236345171928406,
          0.4623703956604004,
          0.4623822867870331,
          0.4624726474285126,
          0.46250101923942566,
          0.4625178277492523,
          0.46247735619544983,
          0.4626339375972748,
          0.46279752254486084,
          0.46252909302711487,
          0.46268001198768616,
          0.4624858796596527,
          0.4625789225101471,
          0.46258506178855896,
          0.4625735878944397
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Loss MULTIMODAL CONCAT"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train accuracy",
         "type": "scatter",
         "y": [
          0.5010080933570862,
          0.5,
          0.507056474685669,
          0.5836693644523621,
          0.711693525314331,
          0.7560483813285828,
          0.772681474685669,
          0.8094757795333862,
          0.8225806355476379,
          0.8200604915618896,
          0.8079637289047241,
          0.8190523982048035,
          0.8160282373428345,
          0.8190523982048035,
          0.828125,
          0.8180443644523621,
          0.8155242204666138,
          0.8245967626571655,
          0.8130040168762207,
          0.8215726017951965,
          0.805443525314331,
          0.8109878897666931,
          0.8019153475761414,
          0.8160282373428345,
          0.8119959831237793,
          0.8220766186714172,
          0.8104838728904724,
          0.8251007795333862,
          0.8119959831237793,
          0.8079637289047241,
          0.805443525314331,
          0.8356854915618896,
          0.819556474685669,
          0.805443525314331,
          0.8205645084381104,
          0.8240927457809448,
          0.8150201439857483,
          0.7958669066429138,
          0.8130040168762207,
          0.819556474685669,
          0.8271169066429138,
          0.7963709831237793,
          0.8336693644523621,
          0.8190523982048035,
          0.8034273982048035,
          0.8014112710952759,
          0.8155242204666138,
          0.8135080933570862,
          0.8125,
          0.8049395084381104,
          0.8230846524238586,
          0.8024193644523621,
          0.8286290168762207,
          0.8286290168762207,
          0.8185483813285828,
          0.8104838728904724,
          0.8145161271095276,
          0.821068525314331,
          0.8165322542190552,
          0.7913306355476379,
          0.8064516186714172,
          0.8175403475761414,
          0.8109878897666931,
          0.8165322542190552,
          0.8150201439857483,
          0.8175403475761414,
          0.8135080933570862,
          0.8240927457809448,
          0.8014112710952759,
          0.8130040168762207,
          0.8029233813285828,
          0.8114919066429138,
          0.8104838728904724,
          0.8094757795333862,
          0.8089717626571655,
          0.8321572542190552,
          0.8180443644523621,
          0.8225806355476379,
          0.796875,
          0.8311492204666138,
          0.84375,
          0.821068525314331,
          0.8084677457809448,
          0.8024193644523621,
          0.8271169066429138,
          0.7893145084381104,
          0.8286290168762207,
          0.8119959831237793,
          0.8069556355476379,
          0.8220766186714172,
          0.8074596524238586,
          0.8205645084381104,
          0.8130040168762207,
          0.8306451439857483,
          0.8291330933570862,
          0.7958669066429138,
          0.8200604915618896,
          0.8034273982048035,
          0.8079637289047241,
          0.8145161271095276,
          0.8160282373428345,
          0.8074596524238586,
          0.8094757795333862,
          0.8180443644523621,
          0.8029233813285828,
          0.8145161271095276,
          0.8220766186714172,
          0.8215726017951965,
          0.8099798560142517,
          0.8099798560142517,
          0.8266128897666931,
          0.8059476017951965,
          0.8009072542190552,
          0.8094757795333862,
          0.7998992204666138,
          0.8276209831237793,
          0.8256048560142517,
          0.7908266186714172,
          0.8074596524238586,
          0.8114919066429138,
          0.8084677457809448,
          0.8205645084381104,
          0.8245967626571655,
          0.819556474685669,
          0.8145161271095276,
          0.8064516186714172,
          0.8114919066429138,
          0.8326612710952759,
          0.7943548560142517,
          0.821068525314331,
          0.819556474685669,
          0.8145161271095276,
          0.8034273982048035,
          0.8059476017951965,
          0.8109878897666931,
          0.8135080933570862,
          0.8321572542190552,
          0.8029233813285828,
          0.8387096524238586,
          0.8094757795333862,
          0.8064516186714172,
          0.8235887289047241,
          0.8220766186714172,
          0.7983871102333069,
          0.8125,
          0.8190523982048035,
          0.8296371102333069,
          0.8114919066429138,
          0.7862903475761414,
          0.8205645084381104,
          0.8059476017951965,
          0.8235887289047241,
          0.8059476017951965,
          0.8150201439857483,
          0.8155242204666138,
          0.8170362710952759,
          0.8326612710952759,
          0.8114919066429138,
          0.8064516186714172,
          0.8155242204666138,
          0.8266128897666931,
          0.8240927457809448,
          0.8155242204666138,
          0.8301411271095276,
          0.8079637289047241,
          0.8185483813285828,
          0.8130040168762207,
          0.8175403475761414,
          0.8220766186714172,
          0.8130040168762207,
          0.8119959831237793,
          0.8114919066429138,
          0.8019153475761414,
          0.8109878897666931,
          0.8180443644523621,
          0.7983871102333069,
          0.8084677457809448,
          0.7953628897666931,
          0.8205645084381104,
          0.8079637289047241,
          0.8069556355476379,
          0.8150201439857483,
          0.8316532373428345,
          0.7998992204666138,
          0.7963709831237793,
          0.8215726017951965,
          0.8064516186714172,
          0.796875,
          0.8286290168762207,
          0.8165322542190552,
          0.8230846524238586,
          0.8185483813285828,
          0.821068525314331,
          0.8306451439857483,
          0.7973790168762207,
          0.8024193644523621,
          0.8119959831237793,
          0.8361895084381104,
          0.8185483813285828,
          0.8346773982048035,
          0.8180443644523621,
          0.8119959831237793,
          0.8089717626571655,
          0.8119959831237793,
          0.8064516186714172,
          0.8170362710952759,
          0.8119959831237793,
          0.8044354915618896,
          0.8104838728904724,
          0.8165322542190552,
          0.8185483813285828,
          0.8130040168762207,
          0.8099798560142517,
          0.8356854915618896,
          0.8135080933570862,
          0.828125,
          0.8306451439857483,
          0.8125,
          0.8321572542190552,
          0.8135080933570862,
          0.8119959831237793,
          0.8170362710952759,
          0.8114919066429138,
          0.8074596524238586,
          0.8190523982048035,
          0.8044354915618896,
          0.8220766186714172,
          0.8079637289047241,
          0.8392137289047241,
          0.8125,
          0.8311492204666138,
          0.8220766186714172,
          0.8069556355476379,
          0.8200604915618896,
          0.8205645084381104,
          0.8119959831237793,
          0.8185483813285828,
          0.8084677457809448,
          0.8084677457809448,
          0.821068525314331,
          0.8185483813285828,
          0.828125,
          0.8084677457809448,
          0.8099798560142517,
          0.8130040168762207,
          0.8170362710952759,
          0.8160282373428345,
          0.8316532373428345,
          0.8135080933570862,
          0.8190523982048035,
          0.8150201439857483,
          0.8185483813285828,
          0.8109878897666931,
          0.8155242204666138,
          0.8230846524238586,
          0.805443525314331,
          0.8009072542190552,
          0.8145161271095276,
          0.8155242204666138,
          0.8256048560142517,
          0.7963709831237793,
          0.8130040168762207,
          0.8175403475761414,
          0.8175403475761414,
          0.8266128897666931,
          0.8114919066429138,
          0.8301411271095276,
          0.8140121102333069,
          0.8235887289047241,
          0.8004032373428345,
          0.8034273982048035,
          0.8160282373428345,
          0.8220766186714172,
          0.8245967626571655,
          0.8235887289047241,
          0.8245967626571655,
          0.8175403475761414,
          0.8130040168762207,
          0.8200604915618896,
          0.8205645084381104,
          0.8125,
          0.8114919066429138,
          0.796875,
          0.7938507795333862,
          0.8336693644523621,
          0.8044354915618896,
          0.7913306355476379,
          0.7978830933570862,
          0.8286290168762207,
          0.8114919066429138,
          0.8175403475761414,
          0.8261088728904724,
          0.8140121102333069,
          0.8356854915618896,
          0.8316532373428345,
          0.8119959831237793,
          0.8240927457809448,
          0.8190523982048035,
          0.8094757795333862,
          0.8382056355476379,
          0.8271169066429138,
          0.8165322542190552,
          0.8220766186714172,
          0.8119959831237793,
          0.8180443644523621,
          0.805443525314331,
          0.8084677457809448,
          0.8114919066429138,
          0.8145161271095276,
          0.8089717626571655,
          0.8009072542190552,
          0.8155242204666138,
          0.796875,
          0.8170362710952759,
          0.8125,
          0.8064516186714172,
          0.8215726017951965,
          0.8160282373428345,
          0.803931474685669,
          0.8145161271095276,
          0.8155242204666138,
          0.8311492204666138,
          0.8104838728904724,
          0.8180443644523621,
          0.8261088728904724,
          0.8160282373428345,
          0.8170362710952759,
          0.8109878897666931,
          0.821068525314331,
          0.8145161271095276,
          0.8462701439857483,
          0.8094757795333862,
          0.805443525314331,
          0.8200604915618896,
          0.8064516186714172,
          0.7998992204666138,
          0.8130040168762207,
          0.8225806355476379,
          0.8119959831237793,
          0.8094757795333862,
          0.8069556355476379,
          0.819556474685669,
          0.8301411271095276,
          0.8200604915618896,
          0.8135080933570862,
          0.8079637289047241,
          0.8119959831237793,
          0.7973790168762207,
          0.8306451439857483,
          0.8276209831237793,
          0.8089717626571655,
          0.7988911271095276,
          0.8059476017951965,
          0.8119959831237793,
          0.8099798560142517,
          0.8109878897666931,
          0.8331653475761414,
          0.8044354915618896,
          0.8160282373428345,
          0.8125,
          0.8094757795333862,
          0.8125,
          0.803931474685669,
          0.8130040168762207,
          0.8251007795333862,
          0.8029233813285828,
          0.8094757795333862,
          0.7978830933570862,
          0.8014112710952759,
          0.796875,
          0.8064516186714172,
          0.8321572542190552,
          0.819556474685669,
          0.803931474685669,
          0.8160282373428345,
          0.8230846524238586,
          0.7988911271095276,
          0.8140121102333069,
          0.8029233813285828,
          0.8114919066429138,
          0.8180443644523621,
          0.8240927457809448,
          0.8079637289047241,
          0.836693525314331,
          0.8074596524238586,
          0.8114919066429138,
          0.8190523982048035,
          0.8160282373428345,
          0.8104838728904724,
          0.8119959831237793,
          0.8145161271095276,
          0.8109878897666931,
          0.8029233813285828,
          0.8130040168762207,
          0.8240927457809448,
          0.8331653475761414,
          0.8004032373428345,
          0.8190523982048035,
          0.8205645084381104,
          0.8099798560142517,
          0.8084677457809448,
          0.8089717626571655,
          0.7948588728904724,
          0.8099798560142517,
          0.803931474685669,
          0.8180443644523621,
          0.8084677457809448,
          0.8135080933570862,
          0.8130040168762207,
          0.8089717626571655,
          0.8220766186714172,
          0.7988911271095276,
          0.8165322542190552,
          0.8114919066429138,
          0.8336693644523621,
          0.7943548560142517,
          0.8185483813285828,
          0.819556474685669,
          0.8024193644523621,
          0.8044354915618896,
          0.8049395084381104,
          0.8135080933570862,
          0.8099798560142517,
          0.803931474685669,
          0.8150201439857483,
          0.8044354915618896,
          0.8271169066429138,
          0.8271169066429138,
          0.8145161271095276,
          0.8034273982048035,
          0.8190523982048035,
          0.8019153475761414,
          0.8135080933570862,
          0.8145161271095276,
          0.8074596524238586,
          0.8261088728904724,
          0.8165322542190552,
          0.8084677457809448,
          0.8291330933570862,
          0.8286290168762207,
          0.7943548560142517,
          0.8175403475761414,
          0.8256048560142517,
          0.8150201439857483,
          0.8155242204666138,
          0.8074596524238586,
          0.8170362710952759,
          0.8175403475761414,
          0.8266128897666931,
          0.7973790168762207,
          0.8114919066429138,
          0.8190523982048035,
          0.8044354915618896,
          0.8079637289047241,
          0.8205645084381104,
          0.8109878897666931,
          0.8140121102333069,
          0.821068525314331,
          0.8089717626571655,
          0.7953628897666931,
          0.805443525314331,
          0.8271169066429138,
          0.8155242204666138,
          0.8019153475761414,
          0.8230846524238586,
          0.8029233813285828,
          0.8014112710952759,
          0.8079637289047241,
          0.8024193644523621,
          0.8377016186714172,
          0.8130040168762207,
          0.8271169066429138,
          0.8185483813285828,
          0.8185483813285828,
          0.8185483813285828,
          0.821068525314331,
          0.8009072542190552,
          0.8119959831237793,
          0.8140121102333069,
          0.8235887289047241,
          0.8175403475761414,
          0.819556474685669,
          0.8004032373428345,
          0.8059476017951965,
          0.8215726017951965,
          0.8119959831237793,
          0.8094757795333862,
          0.8160282373428345,
          0.8160282373428345,
          0.8341733813285828,
          0.8135080933570862,
          0.8034273982048035,
          0.8170362710952759,
          0.8004032373428345,
          0.8170362710952759,
          0.8069556355476379,
          0.8099798560142517,
          0.8215726017951965,
          0.803931474685669,
          0.7998992204666138,
          0.8104838728904724,
          0.819556474685669,
          0.8165322542190552,
          0.8044354915618896,
          0.8251007795333862,
          0.828125,
          0.8024193644523621,
          0.7933467626571655,
          0.7978830933570862,
          0.7993951439857483,
          0.8150201439857483,
          0.7908266186714172,
          0.8049395084381104,
          0.8245967626571655,
          0.8019153475761414,
          0.8266128897666931,
          0.8170362710952759,
          0.8230846524238586,
          0.8125,
          0.8311492204666138,
          0.8245967626571655,
          0.8256048560142517,
          0.8245967626571655,
          0.8109878897666931,
          0.7953628897666931,
          0.8099798560142517,
          0.8084677457809448,
          0.8145161271095276,
          0.8079637289047241,
          0.8160282373428345,
          0.8190523982048035,
          0.8059476017951965,
          0.8316532373428345,
          0.8145161271095276,
          0.8170362710952759,
          0.7933467626571655,
          0.8130040168762207,
          0.8185483813285828,
          0.8240927457809448,
          0.8094757795333862,
          0.8185483813285828,
          0.8145161271095276,
          0.8064516186714172,
          0.8175403475761414,
          0.8235887289047241,
          0.8064516186714172,
          0.7983871102333069,
          0.8175403475761414,
          0.8190523982048035,
          0.8180443644523621,
          0.7938507795333862,
          0.8099798560142517,
          0.8170362710952759,
          0.8266128897666931,
          0.805443525314331,
          0.8160282373428345,
          0.8306451439857483,
          0.8155242204666138,
          0.821068525314331,
          0.8094757795333862,
          0.7903226017951965,
          0.8175403475761414,
          0.805443525314331,
          0.7953628897666931,
          0.8130040168762207,
          0.7988911271095276,
          0.8099798560142517,
          0.8225806355476379,
          0.8220766186714172,
          0.8155242204666138,
          0.8165322542190552,
          0.8079637289047241,
          0.8145161271095276,
          0.8215726017951965,
          0.8200604915618896,
          0.8150201439857483,
          0.8109878897666931,
          0.8079637289047241,
          0.803931474685669,
          0.819556474685669,
          0.8089717626571655,
          0.8291330933570862,
          0.8099798560142517,
          0.8220766186714172,
          0.8140121102333069,
          0.7983871102333069,
          0.8235887289047241,
          0.8230846524238586,
          0.8286290168762207,
          0.8034273982048035,
          0.8185483813285828,
          0.8220766186714172,
          0.8099798560142517,
          0.8240927457809448,
          0.8200604915618896,
          0.8034273982048035,
          0.8200604915618896,
          0.8336693644523621,
          0.8099798560142517,
          0.8084677457809448,
          0.8064516186714172,
          0.8205645084381104,
          0.8109878897666931,
          0.8024193644523621,
          0.7998992204666138,
          0.8251007795333862,
          0.8235887289047241,
          0.8140121102333069,
          0.8069556355476379,
          0.8135080933570862,
          0.8150201439857483,
          0.8215726017951965,
          0.805443525314331,
          0.8225806355476379,
          0.8235887289047241,
          0.8276209831237793,
          0.8089717626571655,
          0.7998992204666138,
          0.8119959831237793,
          0.8089717626571655,
          0.8256048560142517,
          0.8130040168762207,
          0.8220766186714172,
          0.8089717626571655,
          0.8130040168762207,
          0.7998992204666138,
          0.8165322542190552,
          0.8140121102333069,
          0.8094757795333862,
          0.8155242204666138,
          0.8165322542190552,
          0.8240927457809448,
          0.8326612710952759,
          0.8089717626571655,
          0.8104838728904724,
          0.8240927457809448,
          0.819556474685669,
          0.8034273982048035,
          0.8114919066429138,
          0.8150201439857483,
          0.8074596524238586,
          0.8029233813285828,
          0.8261088728904724,
          0.8150201439857483,
          0.8170362710952759,
          0.8371976017951965,
          0.8150201439857483,
          0.8114919066429138,
          0.8044354915618896,
          0.8064516186714172,
          0.8215726017951965,
          0.8004032373428345,
          0.8180443644523621,
          0.7993951439857483,
          0.8251007795333862,
          0.8099798560142517,
          0.8190523982048035,
          0.8114919066429138,
          0.8084677457809448,
          0.8135080933570862,
          0.8014112710952759,
          0.8160282373428345,
          0.8059476017951965,
          0.8205645084381104,
          0.8301411271095276,
          0.7958669066429138,
          0.7928427457809448,
          0.8200604915618896,
          0.8215726017951965,
          0.8266128897666931,
          0.8235887289047241,
          0.8205645084381104,
          0.8155242204666138,
          0.8175403475761414,
          0.8059476017951965,
          0.8180443644523621,
          0.8099798560142517,
          0.8160282373428345,
          0.8044354915618896,
          0.8160282373428345,
          0.8074596524238586,
          0.8084677457809448,
          0.8271169066429138,
          0.7963709831237793,
          0.8135080933570862,
          0.8230846524238586,
          0.8140121102333069,
          0.8059476017951965,
          0.8155242204666138,
          0.8094757795333862,
          0.8109878897666931,
          0.8049395084381104,
          0.8311492204666138,
          0.8125,
          0.8119959831237793,
          0.8160282373428345,
          0.8150201439857483,
          0.8155242204666138,
          0.8185483813285828,
          0.8200604915618896,
          0.8114919066429138,
          0.8215726017951965,
          0.8155242204666138,
          0.821068525314331,
          0.8099798560142517,
          0.8074596524238586,
          0.8059476017951965,
          0.8180443644523621,
          0.8170362710952759,
          0.8245967626571655,
          0.8321572542190552,
          0.8049395084381104,
          0.8155242204666138,
          0.7943548560142517,
          0.8014112710952759,
          0.8089717626571655,
          0.8104838728904724,
          0.8104838728904724,
          0.8114919066429138,
          0.8049395084381104,
          0.8190523982048035,
          0.8220766186714172,
          0.8240927457809448,
          0.8145161271095276,
          0.8256048560142517,
          0.8034273982048035,
          0.8155242204666138,
          0.821068525314331,
          0.8029233813285828,
          0.8104838728904724,
          0.8089717626571655,
          0.8160282373428345,
          0.7998992204666138,
          0.8185483813285828,
          0.8069556355476379,
          0.8261088728904724,
          0.8336693644523621,
          0.8155242204666138,
          0.8160282373428345,
          0.8059476017951965,
          0.8205645084381104,
          0.8316532373428345,
          0.7908266186714172,
          0.8301411271095276,
          0.8135080933570862,
          0.8009072542190552,
          0.8089717626571655,
          0.8160282373428345,
          0.8180443644523621,
          0.8104838728904724,
          0.8155242204666138,
          0.8301411271095276,
          0.8014112710952759,
          0.8230846524238586,
          0.8341733813285828,
          0.8044354915618896,
          0.8165322542190552,
          0.8316532373428345,
          0.8311492204666138,
          0.8140121102333069,
          0.828125,
          0.8220766186714172,
          0.8266128897666931,
          0.8089717626571655,
          0.8099798560142517,
          0.7953628897666931,
          0.7903226017951965,
          0.8155242204666138,
          0.8130040168762207,
          0.8125,
          0.8321572542190552,
          0.8220766186714172,
          0.8140121102333069,
          0.8029233813285828,
          0.7953628897666931,
          0.8256048560142517,
          0.8034273982048035,
          0.8125,
          0.8175403475761414,
          0.8165322542190552,
          0.8130040168762207,
          0.8109878897666931,
          0.8019153475761414,
          0.8119959831237793,
          0.8119959831237793,
          0.8064516186714172,
          0.8079637289047241,
          0.8074596524238586,
          0.7973790168762207,
          0.7983871102333069,
          0.7958669066429138,
          0.8256048560142517,
          0.8165322542190552,
          0.8235887289047241,
          0.8009072542190552,
          0.8069556355476379,
          0.8089717626571655,
          0.8170362710952759,
          0.835181474685669,
          0.8109878897666931,
          0.8145161271095276,
          0.7948588728904724,
          0.8185483813285828,
          0.8069556355476379,
          0.8099798560142517,
          0.8256048560142517,
          0.8049395084381104,
          0.8316532373428345,
          0.8245967626571655,
          0.8230846524238586,
          0.8150201439857483,
          0.8074596524238586,
          0.8185483813285828,
          0.8109878897666931,
          0.805443525314331,
          0.8205645084381104,
          0.8135080933570862,
          0.8336693644523621,
          0.8200604915618896,
          0.8245967626571655,
          0.8271169066429138,
          0.8225806355476379,
          0.805443525314331,
          0.7998992204666138,
          0.8069556355476379,
          0.8170362710952759,
          0.7908266186714172,
          0.8140121102333069,
          0.8014112710952759,
          0.819556474685669,
          0.8069556355476379,
          0.8099798560142517,
          0.8165322542190552,
          0.7993951439857483,
          0.8119959831237793,
          0.7953628897666931,
          0.8190523982048035,
          0.803931474685669,
          0.8291330933570862,
          0.8029233813285828,
          0.8150201439857483,
          0.8125,
          0.8251007795333862,
          0.8190523982048035,
          0.8230846524238586,
          0.8145161271095276,
          0.8109878897666931,
          0.8306451439857483,
          0.8064516186714172,
          0.7918346524238586,
          0.8084677457809448,
          0.7983871102333069,
          0.8014112710952759,
          0.8251007795333862,
          0.8109878897666931,
          0.8150201439857483,
          0.8114919066429138,
          0.8266128897666931,
          0.8190523982048035,
          0.8185483813285828,
          0.8180443644523621,
          0.8175403475761414,
          0.8240927457809448,
          0.8235887289047241,
          0.8190523982048035,
          0.8266128897666931,
          0.8155242204666138,
          0.8256048560142517,
          0.8079637289047241,
          0.8155242204666138,
          0.8024193644523621,
          0.8069556355476379,
          0.7958669066429138,
          0.8155242204666138,
          0.821068525314331,
          0.8029233813285828,
          0.8104838728904724,
          0.8245967626571655,
          0.7822580933570862,
          0.8079637289047241,
          0.8024193644523621,
          0.8230846524238586,
          0.8261088728904724,
          0.7933467626571655,
          0.8034273982048035,
          0.8245967626571655,
          0.8336693644523621,
          0.8220766186714172,
          0.8029233813285828,
          0.8160282373428345,
          0.8160282373428345,
          0.8029233813285828,
          0.8256048560142517,
          0.819556474685669,
          0.8185483813285828,
          0.7988911271095276,
          0.805443525314331,
          0.8059476017951965,
          0.819556474685669,
          0.796875,
          0.8059476017951965,
          0.835181474685669,
          0.8240927457809448,
          0.8245967626571655,
          0.8175403475761414,
          0.8341733813285828,
          0.819556474685669,
          0.8119959831237793,
          0.8004032373428345,
          0.8200604915618896,
          0.8119959831237793,
          0.8160282373428345,
          0.8407257795333862,
          0.8044354915618896,
          0.8094757795333862,
          0.8125,
          0.8185483813285828,
          0.8215726017951965,
          0.8079637289047241,
          0.7923387289047241,
          0.8130040168762207,
          0.8245967626571655,
          0.8377016186714172,
          0.8145161271095276,
          0.8175403475761414,
          0.8140121102333069,
          0.8109878897666931,
          0.8064516186714172,
          0.8190523982048035,
          0.8150201439857483,
          0.8160282373428345,
          0.8104838728904724,
          0.8104838728904724,
          0.7988911271095276,
          0.8130040168762207,
          0.8099798560142517,
          0.8346773982048035,
          0.8150201439857483,
          0.8145161271095276,
          0.8034273982048035,
          0.8296371102333069,
          0.8321572542190552,
          0.8251007795333862,
          0.8130040168762207,
          0.8024193644523621,
          0.8180443644523621,
          0.8190523982048035,
          0.8155242204666138,
          0.8084677457809448,
          0.8296371102333069,
          0.8099798560142517,
          0.8155242204666138,
          0.8185483813285828,
          0.8019153475761414,
          0.8245967626571655,
          0.8180443644523621,
          0.8009072542190552,
          0.8004032373428345,
          0.8064516186714172,
          0.8160282373428345,
          0.8049395084381104,
          0.8296371102333069,
          0.8009072542190552,
          0.8004032373428345,
          0.8044354915618896,
          0.8200604915618896,
          0.7938507795333862,
          0.8059476017951965,
          0.819556474685669,
          0.8135080933570862,
          0.803931474685669,
          0.8019153475761414,
          0.805443525314331,
          0.8099798560142517,
          0.7948588728904724,
          0.8200604915618896,
          0.8225806355476379,
          0.803931474685669,
          0.8099798560142517,
          0.8271169066429138,
          0.803931474685669,
          0.819556474685669,
          0.8230846524238586,
          0.8291330933570862,
          0.7923387289047241,
          0.8261088728904724,
          0.8130040168762207,
          0.8150201439857483,
          0.8185483813285828,
          0.8125,
          0.8235887289047241,
          0.8296371102333069,
          0.8094757795333862,
          0.8009072542190552,
          0.8276209831237793,
          0.8079637289047241,
          0.8316532373428345,
          0.8160282373428345,
          0.8251007795333862,
          0.8190523982048035
         ]
        },
        {
         "name": "Validation accuracy",
         "type": "scatter",
         "y": [
          0.36936935782432556,
          0.4099099040031433,
          0.5135135054588318,
          0.5945945978164673,
          0.7252252101898193,
          0.7792792916297913,
          0.792792797088623,
          0.8018018007278442,
          0.8018018007278442,
          0.8108108043670654,
          0.8063063025474548,
          0.8018018007278442,
          0.8063063025474548,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8108108043670654,
          0.8108108043670654,
          0.8063063025474548,
          0.8063063025474548,
          0.8063063025474548,
          0.8063063025474548,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8063063025474548,
          0.8063063025474548,
          0.8063063025474548,
          0.8063063025474548,
          0.8063063025474548,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8063063025474548,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8063063025474548,
          0.8063063025474548,
          0.8063063025474548,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8063063025474548,
          0.8063063025474548,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8063063025474548,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8063063025474548,
          0.8063063025474548,
          0.8063063025474548,
          0.8018018007278442,
          0.8018018007278442,
          0.8063063025474548,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8063063025474548,
          0.8018018007278442,
          0.8063063025474548,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8063063025474548,
          0.8063063025474548,
          0.8063063025474548,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8063063025474548,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8063063025474548,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8063063025474548,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442,
          0.8018018007278442
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Accuracy MULTIMODAL CONCAT"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Accuracy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train F1 score",
         "type": "scatter",
         "y": [
          0.68333899974823,
          0.8093733191490173,
          0.8255833983421326,
          0.8043392896652222,
          0.8154699802398682,
          0.808448314666748,
          0.8024064898490906,
          0.8113306760787964,
          0.823523998260498,
          0.8234162330627441,
          0.8082683086395264,
          0.8223958015441895,
          0.8164957761764526,
          0.8205615878105164,
          0.8275703191757202,
          0.8184590339660645,
          0.8144791722297668,
          0.8245617747306824,
          0.8155014514923096,
          0.8214199542999268,
          0.8053563237190247,
          0.8114764094352722,
          0.800337553024292,
          0.8162902593612671,
          0.8114365339279175,
          0.822534441947937,
          0.8124382495880127,
          0.8254343271255493,
          0.8112000823020935,
          0.8104561567306519,
          0.8073561191558838,
          0.8346606492996216,
          0.8185122609138489,
          0.8051424026489258,
          0.8204411864280701,
          0.8255962133407593,
          0.8144890069961548,
          0.795252799987793,
          0.8144890069961548,
          0.820517897605896,
          0.8275282382965088,
          0.7983666658401489,
          0.8326367139816284,
          0.8184590339660645,
          0.8053388595581055,
          0.8024121522903442,
          0.8154565095901489,
          0.8134653568267822,
          0.8124725222587585,
          0.8062499761581421,
          0.8245939612388611,
          0.8043010234832764,
          0.8295871019363403,
          0.8305763006210327,
          0.821459174156189,
          0.8093733191490173,
          0.8154820203781128,
          0.8213748931884766,
          0.8164419531822205,
          0.7923176288604736,
          0.8083555698394775,
          0.8164845705032349,
          0.8113905191421509,
          0.8173619508743286,
          0.8154414892196655,
          0.8174867630004883,
          0.8144972920417786,
          0.8266065716743469,
          0.802283525466919,
          0.8114488124847412,
          0.8033825159072876,
          0.8124809265136719,
          0.8104830980300903,
          0.8092104196548462,
          0.8094710111618042,
          0.8296078443527222,
          0.8185122609138489,
          0.8215419054031372,
          0.798346996307373,
          0.8306203484535217,
          0.8447069525718689,
          0.8193565607070923,
          0.8084179162979126,
          0.8013044595718384,
          0.8276124000549316,
          0.7902194261550903,
          0.830545961856842,
          0.812451183795929,
          0.8054275512695312,
          0.8215506076812744,
          0.8073168992996216,
          0.8194020986557007,
          0.8133485317230225,
          0.8305617570877075,
          0.8295606970787048,
          0.7951878309249878,
          0.8204411864280701,
          0.8033217191696167,
          0.8094740509986877,
          0.8145161271095276,
          0.8174049854278564,
          0.8073890209197998,
          0.8114901781082153,
          0.8175386786460876,
          0.8034223914146423,
          0.8154565095901489,
          0.8195415735244751,
          0.8205287456512451,
          0.808448314666748,
          0.80805504322052,
          0.8275997638702393,
          0.8073890209197998,
          0.800337553024292,
          0.8094198107719421,
          0.8001400232315063,
          0.8285949230194092,
          0.8234162330627441,
          0.7901554107666016,
          0.8083734512329102,
          0.8113905191421509,
          0.8073168992996216,
          0.8204593658447266,
          0.8244140148162842,
          0.8185122609138489,
          0.8144550323486328,
          0.806372880935669,
          0.8124077320098877,
          0.8326061964035034,
          0.7953277826309204,
          0.8215724229812622,
          0.8182528614997864,
          0.8144678473472595,
          0.8023221492767334,
          0.8063185811042786,
          0.8114871382713318,
          0.8154414892196655,
          0.8326061964035034,
          0.802158772945404,
          0.8386775255203247,
          0.8084552884101868,
          0.8053388595581055,
          0.8233429789543152,
          0.8204002380371094,
          0.7983739376068115,
          0.8124809265136719,
          0.818488597869873,
          0.8306114673614502,
          0.8103327751159668,
          0.7872878313064575,
          0.821557879447937,
          0.8034112453460693,
          0.822562575340271,
          0.8064233064651489,
          0.8154699802398682,
          0.8164061307907104,
          0.817517876625061,
          0.833665132522583,
          0.8113076686859131,
          0.8054275512695312,
          0.8174986243247986,
          0.8275913000106812,
          0.8235368728637695,
          0.8183822631835938,
          0.8276124000549316,
          0.8073732852935791,
          0.8185454607009888,
          0.8123505115509033,
          0.8184590339660645,
          0.822534441947937,
          0.8124382495880127,
          0.8104838728904724,
          0.8124725222587585,
          0.8044068813323975,
          0.8124878406524658,
          0.8175089955329895,
          0.799199104309082,
          0.8083898425102234,
          0.7962707877159119,
          0.8205615878105164,
          0.8074438571929932,
          0.8062744140625,
          0.8154699802398682,
          0.8306279182434082,
          0.8002204895019531,
          0.7963179349899292,
          0.820517897605896,
          0.8073732852935791,
          0.7971809506416321,
          0.8285447359085083,
          0.817517876625061,
          0.822562575340271,
          0.8184037208557129,
          0.8205054402351379,
          0.8294705152511597,
          0.7982485294342041,
          0.8023799657821655,
          0.8114227652549744,
          0.8376816511154175,
          0.8174421787261963,
          0.8346606492996216,
          0.8195254802703857,
          0.8124237060546875,
          0.8104068040847778,
          0.8114687204360962,
          0.8053864240646362,
          0.8155014514923096,
          0.8124077320098877,
          0.8033937215805054,
          0.8104345798492432,
          0.8165292739868164,
          0.8183822631835938,
          0.8124626278877258,
          0.8104068040847778,
          0.8356839418411255,
          0.8144890069961548,
          0.8275576829910278,
          0.8296231031417847,
          0.8134077787399292,
          0.8336083292961121,
          0.8133697509765625,
          0.8124626278877258,
          0.8165054321289062,
          0.8103905916213989,
          0.8084396719932556,
          0.8184590339660645,
          0.80335533618927,
          0.8235368728637695,
          0.8083555698394775,
          0.8387070894241333,
          0.8123505115509033,
          0.8305763006210327,
          0.8225222229957581,
          0.8082683086395264,
          0.8195151686668396,
          0.8225799202919006,
          0.8123505115509033,
          0.8185011744499207,
          0.8074438571929932,
          0.8093733191490173,
          0.8214199542999268,
          0.8194417953491211,
          0.8284924030303955,
          0.8084295988082886,
          0.8103535771369934,
          0.8123711347579956,
          0.8165203332901001,
          0.8165292739868164,
          0.8306341171264648,
          0.8124809265136719,
          0.8195151686668396,
          0.8144550323486328,
          0.8175252676010132,
          0.8114825487136841,
          0.8164845705032349,
          0.823523998260498,
          0.8053721189498901,
          0.8022384643554688,
          0.814513087272644,
          0.8155090808868408,
          0.8255904912948608,
          0.7961024641990662,
          0.8134077787399292,
          0.8164419531822205,
          0.8175089955329895,
          0.8265677690505981,
          0.8114595413208008,
          0.8295981884002686,
          0.8153438568115234,
          0.8245254158973694,
          0.8013044595718384,
          0.8033217191696167,
          0.8164576292037964,
          0.8225085139274597,
          0.8225085139274597,
          0.8234579563140869,
          0.8244361877441406,
          0.8164718747138977,
          0.8133697509765625,
          0.8205382823944092,
          0.8195034265518188,
          0.8124077320098877,
          0.812451183795929,
          0.795232892036438,
          0.7932356595993042,
          0.8326551914215088,
          0.803425669670105,
          0.7921224236488342,
          0.7982485294342041,
          0.8286116123199463,
          0.8093128204345703,
          0.8174867630004883,
          0.8275576829910278,
          0.8134653568267822,
          0.8366271257400513,
          0.830545961856842,
          0.8133895397186279,
          0.8245104551315308,
          0.8185417652130127,
          0.8094058632850647,
          0.8386775255203247,
          0.8264937400817871,
          0.8174049854278564,
          0.8215419054031372,
          0.8123505115509033,
          0.818488597869873,
          0.8053721189498901,
          0.8082924485206604,
          0.8114687204360962,
          0.8144890069961548,
          0.8084295988082886,
          0.8003220558166504,
          0.8155090808868408,
          0.7983838319778442,
          0.8165054321289062,
          0.8134244084358215,
          0.806444525718689,
          0.8215419054031372,
          0.8174242973327637,
          0.8043965101242065,
          0.8154925107955933,
          0.8164957761764526,
          0.8315626382827759,
          0.8113306760787964,
          0.817540168762207,
          0.8275997638702393,
          0.8155195713043213,
          0.8165136575698853,
          0.8114488124847412,
          0.820517897605896,
          0.814407467842102,
          0.8456517457962036,
          0.8093903064727783,
          0.8043392896652222,
          0.8195342421531677,
          0.8053864240646362,
          0.8003220558166504,
          0.8113905191421509,
          0.8225222229957581,
          0.8112831115722656,
          0.8094601631164551,
          0.806372880935669,
          0.820563793182373,
          0.8295606970787048,
          0.8194226622581482,
          0.8124077320098877,
          0.8073890209197998,
          0.8113521933555603,
          0.7982879281044006,
          0.8306424617767334,
          0.8276165723800659,
          0.8094322681427002,
          0.7973194718360901,
          0.8052771687507629,
          0.8124382495880127,
          0.8093547224998474,
          0.8104768991470337,
          0.8336408138275146,
          0.803209662437439,
          0.8144248723983765,
          0.8124809265136719,
          0.8112293481826782,
          0.8124077320098877,
          0.8034176230430603,
          0.8134987950325012,
          0.8255749344825745,
          0.80335533618927,
          0.810205340385437,
          0.7981212139129639,
          0.8013384342193604,
          0.7963502407073975,
          0.8052993416786194,
          0.8336204886436462,
          0.8205579519271851,
          0.8054101467132568,
          0.8154699802398682,
          0.822534441947937,
          0.7993215322494507,
          0.8144248723983765,
          0.8033217191696167,
          0.8123902082443237,
          0.8195254802703857,
          0.8235096335411072,
          0.8094601631164551,
          0.8365813493728638,
          0.8073890209197998,
          0.8114764094352722,
          0.8184590339660645,
          0.8164957761764526,
          0.8103327751159668,
          0.8124809265136719,
          0.8164957761764526,
          0.8104068040847778,
          0.8023036122322083,
          0.8134927749633789,
          0.8245254158973694,
          0.8336204886436462,
          0.800168514251709,
          0.8185122609138489,
          0.820517897605896,
          0.8094322681427002,
          0.8083360195159912,
          0.8094710111618042,
          0.7941665649414062,
          0.8094198107719421,
          0.80335533618927,
          0.8174986243247986,
          0.8093733191490173,
          0.8123902082443237,
          0.8144972920417786,
          0.8072463274002075,
          0.822562575340271,
          0.7983207106590271,
          0.8164845705032349,
          0.8113905191421509,
          0.8336678743362427,
          0.795232892036438,
          0.8184745907783508,
          0.8215419054031372,
          0.8023992776870728,
          0.8032352328300476,
          0.8033393025398254,
          0.8134077787399292,
          0.8092647790908813,
          0.8032816648483276,
          0.8165203332901001,
          0.8053563237190247,
          0.8265783786773682,
          0.8264541029930115,
          0.8165136575698853,
          0.8044322729110718,
          0.8193565607070923,
          0.8023221492767334,
          0.8142139911651611,
          0.8134653568267822,
          0.8073561191558838,
          0.8254940509796143,
          0.8165292739868164,
          0.8094198107719421,
          0.8286179304122925,
          0.828603982925415,
          0.7941405773162842,
          0.8174421787261963,
          0.8255833983421326,
          0.8154699802398682,
          0.8154820203781128,
          0.8084046840667725,
          0.8174421787261963,
          0.8185417652130127,
          0.8245104551315308,
          0.7970967292785645,
          0.8134396076202393,
          0.8185417652130127,
          0.8042317628860474,
          0.8084046840667725,
          0.8205462694168091,
          0.8104068040847778,
          0.8134927749633789,
          0.821520209312439,
          0.8073732852935791,
          0.7941908240318298,
          0.8044156432151794,
          0.8276193141937256,
          0.8133485317230225,
          0.8013529777526855,
          0.822534441947937,
          0.8023036122322083,
          0.8013771772384644,
          0.8063564300537109,
          0.8022618293762207,
          0.8376644849777222,
          0.8134077787399292,
          0.8275436758995056,
          0.8195034265518188,
          0.8174734115600586,
          0.8194901943206787,
          0.8214925527572632,
          0.800337553024292,
          0.8124077320098877,
          0.8144550323486328,
          0.8235368728637695,
          0.818359375,
          0.8184590339660645,
          0.7993052005767822,
          0.8063564300537109,
          0.8205528259277344,
          0.8124991655349731,
          0.8094710111618042,
          0.8154565095901489,
          0.8165315389633179,
          0.8346102237701416,
          0.8135033249855042,
          0.8034032583236694,
          0.8174867630004883,
          0.8003740310668945,
          0.8175252676010132,
          0.8073890209197998,
          0.8093345165252686,
          0.8215419054031372,
          0.8043965101242065,
          0.8003740310668945,
          0.8104715347290039,
          0.8195474147796631,
          0.8173129558563232,
          0.8054196238517761,
          0.8255749344825745,
          0.8276067972183228,
          0.8043965101242065,
          0.7931935787200928,
          0.799199104309082,
          0.8002204895019531,
          0.8143463134765625,
          0.7911266684532166,
          0.8032816648483276,
          0.8245903849601746,
          0.8013222217559814,
          0.8245390057563782,
          0.817517876625061,
          0.8225741386413574,
          0.8124969005584717,
          0.830545961856842,
          0.8245939612388611,
          0.8245903253555298,
          0.8245960474014282,
          0.8113521933555603,
          0.7953161001205444,
          0.8104561567306519,
          0.8083150386810303,
          0.8134851455688477,
          0.8073890209197998,
          0.8164576292037964,
          0.8195342421531677,
          0.8064233064651489,
          0.8326504230499268,
          0.8145093321800232,
          0.8174734115600586,
          0.7931935787200928,
          0.8133257627487183,
          0.8195415735244751,
          0.8224391937255859,
          0.8094663023948669,
          0.8195562362670898,
          0.8134851455688477,
          0.8073732852935791,
          0.8164061307907104,
          0.8245903253555298,
          0.8073168992996216,
          0.7983666658401489,
          0.8175089955329895,
          0.8193565607070923,
          0.8174986243247986,
          0.7941665649414062,
          0.8102339506149292,
          0.8175089955329895,
          0.8253874778747559,
          0.806401252746582,
          0.8154820203781128,
          0.8306114673614502,
          0.8164576292037964,
          0.8205054402351379,
          0.8084396719932556,
          0.7899092435836792,
          0.8194901943206787,
          0.806372880935669,
          0.7951878309249878,
          0.8123046159744263,
          0.7993786334991455,
          0.8113306760787964,
          0.8225452899932861,
          0.823523998260498,
          0.8155194520950317,
          0.8164957761764526,
          0.8063383102416992,
          0.8144972920417786,
          0.8215317726135254,
          0.8194593787193298,
          0.8152943253517151,
          0.8113076686859131,
          0.8074359893798828,
          0.8044156432151794,
          0.8195342421531677,
          0.8083734512329102,
          0.8296161890029907,
          0.8124992251396179,
          0.822562575340271,
          0.8132755756378174,
          0.7993215322494507,
          0.8234378695487976,
          0.8225452899932861,
          0.828603982925415,
          0.802283525466919,
          0.8175252676010132,
          0.8225452899932861,
          0.8113905191421509,
          0.8245254158973694,
          0.8195415735244751,
          0.8033937215805054,
          0.8193315863609314,
          0.8346531987190247,
          0.8104561567306519,
          0.8072714805603027,
          0.8063185811042786,
          0.8204214572906494,
          0.8104345798492432,
          0.8023543357849121,
          0.7993607521057129,
          0.8245617747306824,
          0.8225547075271606,
          0.8133697509765625,
          0.806444525718689,
          0.8134987950325012,
          0.8133895397186279,
          0.8224767446517944,
          0.8052533864974976,
          0.8205382823944092,
          0.823588490486145,
          0.8264937400817871,
          0.8093733191490173,
          0.7983207106590271,
          0.8114488124847412,
          0.8094058632850647,
          0.825564980506897,
          0.8133697509765625,
          0.8214925527572632,
          0.81046462059021,
          0.813476026058197,
          0.7982690334320068,
          0.8164061307907104,
          0.8144550323486328,
          0.8083898425102234,
          0.8154414892196655,
          0.8174734115600586,
          0.8235670328140259,
          0.8326504230499268,
          0.8084669709205627,
          0.8113721013069153,
          0.8245254158973694,
          0.819551944732666,
          0.8023679256439209,
          0.81046462059021,
          0.8154414892196655,
          0.8063564300537109,
          0.8023992776870728,
          0.8255749344825745,
          0.8143229484558105,
          0.8185454607009888,
          0.835665225982666,
          0.8154565095901489,
          0.8124237060546875,
          0.8041480779647827,
          0.8072949647903442,
          0.8213502168655396,
          0.8012640476226807,
          0.8185299634933472,
          0.8002862930297852,
          0.8244940638542175,
          0.8103728890419006,
          0.8185122609138489,
          0.8113306760787964,
          0.8084046840667725,
          0.8175252676010132,
          0.8012640476226807,
          0.8164419531822205,
          0.806444525718689,
          0.8214765787124634,
          0.8304687738418579,
          0.7950778007507324,
          0.7922542095184326,
          0.820517897605896,
          0.8214765787124634,
          0.8255409002304077,
          0.8235096335411072,
          0.8205054402351379,
          0.8154249787330627,
          0.8195254802703857,
          0.8054417371749878,
          0.8175089955329895,
          0.8092647790908813,
          0.8154249787330627,
          0.8023221492767334,
          0.8154699802398682,
          0.8073561191558838,
          0.8084678053855896,
          0.8265275955200195,
          0.7950127124786377,
          0.8124626278877258,
          0.8235483169555664,
          0.8135033249855042,
          0.8043010234832764,
          0.8154414892196655,
          0.8093733191490173,
          0.8113521933555603,
          0.8043710589408875,
          0.8306452035903931,
          0.8114365339279175,
          0.8114687204360962,
          0.8164718747138977,
          0.8154820203781128,
          0.8144678473472595,
          0.8184037208557129,
          0.820517897605896,
          0.8112569451332092,
          0.8215709924697876,
          0.815387487411499,
          0.8204761743545532,
          0.8084295988082886,
          0.8083898425102234,
          0.8064318895339966,
          0.8185417652130127,
          0.8173619508743286,
          0.8244361877441406,
          0.8326504230499268,
          0.8043845891952515,
          0.8154414892196655,
          0.7941908240318298,
          0.8011038303375244,
          0.8083555698394775,
          0.8114764094352722,
          0.8114687204360962,
          0.8114764094352722,
          0.8042317628860474,
          0.8185122609138489,
          0.8224767446517944,
          0.8235368728637695,
          0.8144972920417786,
          0.825564980506897,
          0.8032816648483276,
          0.8154699802398682,
          0.8214925527572632,
          0.802283525466919,
          0.8104461431503296,
          0.8073168992996216,
          0.8164576292037964,
          0.8003512620925903,
          0.8185218572616577,
          0.8074438571929932,
          0.8255535960197449,
          0.8335798978805542,
          0.814407467842102,
          0.8164718747138977,
          0.8072949647903442,
          0.8205579519271851,
          0.8316489458084106,
          0.7901784181594849,
          0.8306010961532593,
          0.8133895397186279,
          0.8023543357849121,
          0.8093128204345703,
          0.8164718747138977,
          0.8175252676010132,
          0.8102864027023315,
          0.8163642883300781,
          0.8295606970787048,
          0.8003829121589661,
          0.8235583901405334,
          0.8336489200592041,
          0.8043559789657593,
          0.8174421787261963,
          0.8316243290901184,
          0.8326061964035034,
          0.8144248723983765,
          0.828603982925415,
          0.8225806355476379,
          0.8266100883483887,
          0.8084552884101868,
          0.8094198107719421,
          0.7951878309249878,
          0.791175901889801,
          0.8154925107955933,
          0.8124878406524658,
          0.8123505115509033,
          0.8326367139816284,
          0.8225547075271606,
          0.8144890069961548,
          0.803425669670105,
          0.7952111959457397,
          0.8245788812637329,
          0.80335533618927,
          0.8123902082443237,
          0.8174867630004883,
          0.8154699802398682,
          0.8134927749633789,
          0.8114227652549744,
          0.8022618293762207,
          0.8114073872566223,
          0.8123902082443237,
          0.806401252746582,
          0.8084046840667725,
          0.8072714805603027,
          0.7982690334320068,
          0.7982263565063477,
          0.7962707877159119,
          0.8255409002304077,
          0.8165203332901001,
          0.823584258556366,
          0.8001953363418579,
          0.8083734512329102,
          0.8093128204345703,
          0.8165292739868164,
          0.8356572389602661,
          0.8104214668273926,
          0.813476026058197,
          0.7953027486801147,
          0.8185299634933472,
          0.8074265718460083,
          0.8104561567306519,
          0.8256003856658936,
          0.8053388595581055,
          0.8315914869308472,
          0.8245617747306824,
          0.8215506076812744,
          0.8153438568115234,
          0.8083898425102234,
          0.8185122609138489,
          0.8114488124847412,
          0.8043845891952515,
          0.8204214572906494,
          0.8124992251396179,
          0.8346344232559204,
          0.8205528259277344,
          0.8245711326599121,
          0.8265677690505981,
          0.8235096335411072,
          0.8051725625991821,
          0.7993362545967102,
          0.8074359893798828,
          0.8164419531822205,
          0.7901784181594849,
          0.8133895397186279,
          0.8013771772384644,
          0.8194901943206787,
          0.8074500560760498,
          0.8093345165252686,
          0.817517876625061,
          0.7981771230697632,
          0.8113306760787964,
          0.7962310314178467,
          0.8185365796089172,
          0.8033024668693542,
          0.8286116123199463,
          0.8023390769958496,
          0.8154414892196655,
          0.8124725222587585,
          0.8245390057563782,
          0.8195151686668396,
          0.8245254158973694,
          0.815387487411499,
          0.8103728890419006,
          0.8286283016204834,
          0.8074359893798828,
          0.791099488735199,
          0.8072949647903442,
          0.7982025146484375,
          0.8011637926101685,
          0.8255535960197449,
          0.8124809265136719,
          0.8164576292037964,
          0.8123046159744263,
          0.8245788812637329,
          0.8204214572906494,
          0.8195415735244751,
          0.8185299634933472,
          0.8175312280654907,
          0.8245968222618103,
          0.8234938383102417,
          0.8195415735244751,
          0.8275282382965088,
          0.8154414892196655,
          0.8254755735397339,
          0.8074359893798828,
          0.8144248723983765,
          0.8023543357849121,
          0.8064130544662476,
          0.7951628565788269,
          0.8144972920417786,
          0.8204915523529053,
          0.802283525466919,
          0.8094322681427002,
          0.8245903253555298,
          0.7809773087501526,
          0.8083555698394775,
          0.8023390769958496,
          0.8235096335411072,
          0.8255962133407593,
          0.7932356595993042,
          0.8033696413040161,
          0.8245968222618103,
          0.8336691856384277,
          0.8214199542999268,
          0.8023036122322083,
          0.8164248466491699,
          0.8165203332901001,
          0.8022384643554688,
          0.8266100883483887,
          0.8195254802703857,
          0.8164845705032349,
          0.800168514251709,
          0.806372880935669,
          0.806444525718689,
          0.8195342421531677,
          0.7963038682937622,
          0.8064130544662476,
          0.835665225982666,
          0.8235670328140259,
          0.8245903253555298,
          0.8175089955329895,
          0.8356479406356812,
          0.8195151686668396,
          0.8123902082443237,
          0.8013949394226074,
          0.819551944732666,
          0.8124382495880127,
          0.8164957761764526,
          0.8416873216629028,
          0.8033024668693542,
          0.8092895746231079,
          0.8114227652549744,
          0.8183822631835938,
          0.820517897605896,
          0.8104715347290039,
          0.79229736328125,
          0.8144407272338867,
          0.8235741853713989,
          0.8375430107116699,
          0.8145040273666382,
          0.8184745907783508,
          0.8134927749633789,
          0.8104807734489441,
          0.8072714805603027,
          0.8184745907783508,
          0.8124077320098877,
          0.8154069185256958,
          0.8104561567306519,
          0.8104461431503296,
          0.7982879281044006,
          0.8124725222587585,
          0.8093733191490173,
          0.8346102237701416,
          0.814513087272644,
          0.8144791722297668,
          0.8033024668693542,
          0.829628586769104,
          0.8315914869308472,
          0.8245511054992676,
          0.813476026058197,
          0.8022384643554688,
          0.8184421062469482,
          0.8174242973327637,
          0.814407467842102,
          0.8084046840667725,
          0.829628586769104,
          0.8092383146286011,
          0.8164248466491699,
          0.819475531578064,
          0.8013529777526855,
          0.823523998260498,
          0.8174986243247986,
          0.8002204895019531,
          0.7993362545967102,
          0.806372880935669,
          0.8174049854278564,
          0.8043209314346313,
          0.8306114673614502,
          0.8013529777526855,
          0.8002204895019531,
          0.8043559789657593,
          0.8194593787193298,
          0.7932860851287842,
          0.8053199052810669,
          0.819475531578064,
          0.8144550323486328,
          0.8033937215805054,
          0.8032816648483276,
          0.8043010234832764,
          0.8104214668273926,
          0.795252799987793,
          0.819475531578064,
          0.8224767446517944,
          0.8043965101242065,
          0.8104830980300903,
          0.8275997638702393,
          0.8032816648483276,
          0.819475531578064,
          0.8235583901405334,
          0.8285726308822632,
          0.7922542095184326,
          0.8255749344825745,
          0.8124077320098877,
          0.8144791722297668,
          0.8184745907783508,
          0.8114365339279175,
          0.8245390057563782,
          0.8285844326019287,
          0.8102864027023315,
          0.7992464303970337,
          0.8275703191757202,
          0.8062744140625,
          0.8306424617767334,
          0.8154820203781128,
          0.8244761228561401,
          0.8174986243247986
         ]
        },
        {
         "name": "Validation F1 score",
         "type": "scatter",
         "y": [
          0.3392857015132904,
          0.3392857015132904,
          0.3392857015132904,
          0.5760121941566467,
          0.7093013525009155,
          0.7836257219314575,
          0.7927255630493164,
          0.8017857074737549,
          0.8017857074737549,
          0.8108108043670654,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8108108043670654,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8108108043670654,
          0.8017857074737549,
          0.8017857074737549,
          0.8108108043670654,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8108108043670654,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8108108043670654,
          0.8108108043670654,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549,
          0.8017857074737549
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Accuracy MULTIMODAL CONCAT"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "F1 score"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'..\\\\..\\\\Outputs\\\\COVID\\\\LR\\\\NuovoTest_Not_Kfold\\\\GRAPHS\\\\training MULTIMODAL CONCAT F1 score.html'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLOT HISTORY\n",
    "plt.clf()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['loss'],\n",
    "                    name='Train loss'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_loss'],\n",
    "                    name='Validation loss'))\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Loss MULTIMODAL CONCAT',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='Loss')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training MULTIMODAL CONCAT loss.html\"))), auto_open=False)\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    # y=history.history['categorical_accuracy'],\n",
    "                    y=history.history['binary_accuracy'],\n",
    "                    name='Train accuracy'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    # y=history.history['val_categorical_accuracy'],\n",
    "                    y=history.history['val_binary_accuracy'],\n",
    "                    name='Validation accuracy'))\n",
    "\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Accuracy MULTIMODAL CONCAT',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='Accuracy')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training MULTIMODAL CONCAT accuracy.html\"))), auto_open=False)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['f1_score'],\n",
    "                    name='Train F1 score'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_f1_score'],\n",
    "                    name='Validation F1 score'))\n",
    "\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Accuracy MULTIMODAL CONCAT',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='F1 score')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training MULTIMODAL CONCAT F1 score.html\"))), auto_open=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth of the validation\n",
      "4/4 [==============================] - 1s 27ms/step - loss: 0.5016 - binary_accuracy: 0.8108 - f1_score: 0.8108\n",
      "4/4 [==============================] - 1s 27ms/step\n",
      "[0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1\n",
      " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1]\n",
      "[0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1]\n",
      "accuracy:  0.8108108108108109\n",
      "log_loss:  0.5013013702255111\n",
      "[[45  9]\n",
      " [12 45]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.78947   0.83333   0.81081        54\n",
      "           1    0.83333   0.78947   0.81081        57\n",
      "\n",
      "    accuracy                        0.81081       111\n",
      "   macro avg    0.81140   0.81140   0.81081       111\n",
      "weighted avg    0.81200   0.81081   0.81081       111\n",
      "\n",
      "{'loss': 0.5016018748283386, 'binary_accuracy': 0.8108108043670654, 'f1_score': 0.8108108043670654}\n",
      "Ground truth of the validation\n",
      "4/4 [==============================] - 1s 27ms/step - loss: 0.5016 - binary_accuracy: 0.8108 - f1_score: 0.8108\n",
      "4/4 [==============================] - 1s 26ms/step\n",
      "[0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1\n",
      " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1]\n",
      "[0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1]\n",
      "accuracy:  0.8108108108108109\n",
      "log_loss:  0.5013013702255111\n",
      "[[45  9]\n",
      " [12 45]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.78947   0.83333   0.81081        54\n",
      "           1    0.83333   0.78947   0.81081        57\n",
      "\n",
      "    accuracy                        0.81081       111\n",
      "   macro avg    0.81140   0.81140   0.81081       111\n",
      "weighted avg    0.81200   0.81081   0.81081       111\n",
      "\n",
      "{'loss': 0.5016018748283386, 'binary_accuracy': 0.8108108043670654, 'f1_score': 0.8108108043670654}\n"
     ]
    }
   ],
   "source": [
    "## EVALUATION\n",
    "gc.collect()\n",
    "\n",
    "bestPt = checkpointPath / Path('MULTIMODAL_CONCAT_Classification_F1')\n",
    "model = tf.keras.models.load_model(bestPt)\n",
    "\n",
    "print(\"Ground truth of the validation\")\n",
    "\n",
    "evaluation = model.evaluate(x = [X_val_IMGS,X_val], y = Y_val)\n",
    "prediction = model.predict([X_val_IMGS,X_val])\n",
    "\n",
    "ypreds = np.argmax(prediction, axis=1)\n",
    "yground = np.argmax(Y_val.to_numpy(), axis=1)\n",
    "# yground = np.argmax(targets, axis=1)\n",
    "\n",
    "print(ypreds)\n",
    "print(yground)\n",
    "\n",
    "\n",
    "\n",
    "score = accuracy_score(yground, ypreds)\n",
    "print('accuracy: ', score)\n",
    "\n",
    "loss = log_loss(yground, prediction)\n",
    "print('log_loss: ', loss)\n",
    "\n",
    "# Print the confusion matrix\n",
    "cmatrix = confusion_matrix(yground, ypreds)\n",
    "print(cmatrix)\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "creport = classification_report(yground, ypreds, digits=5)\n",
    "print(creport)\n",
    "\n",
    "\n",
    "evaluation = dict(zip(model.metrics_names,evaluation))\n",
    "\n",
    "# FF_f1_values.append(evaluation['f1_score'])\n",
    "\n",
    "\n",
    "\n",
    "print(evaluation)\n",
    "with open(str(evalspath) + 'MULTIMODAL_CONCAT_F1_EVALUATION.txt', 'w') as f:\n",
    "    f.write('MULTIMODAL_CONCAT FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    for k, v in evaluation.items():\n",
    "        f.write(str(k) + ' >>> '+ str(v) + '\\n\\n')\n",
    "    f.write('MULTIMODAL_CONCAT FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    f.write('SKLEARN METRICS:')\n",
    "    f.write(str('Loss') + ' >>> '+ str(loss) + '\\n\\n')\n",
    "    f.write(str('Accuracy') + ' >>> '+ str(score) + '\\n\\n')\n",
    "    f.write(str('Confusion Matrix') + ' >>> \\n'+ str(cmatrix) + '\\n\\n')\n",
    "    f.write(str('CLASSIFICATION REPORT WITH CLASSES') + ' >>> \\n'+ str(creport) + '\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bestPt = checkpointPath / Path('MULTIMODAL_CONCAT_Classification_acc')\n",
    "model = tf.keras.models.load_model(bestPt)\n",
    "\n",
    "print(\"Ground truth of the validation\")\n",
    "\n",
    "evaluation = model.evaluate(x = [X_val_IMGS,X_val], y = Y_val)\n",
    "prediction = model.predict([X_val_IMGS,X_val])\n",
    "\n",
    "ypreds = np.argmax(prediction, axis=1)\n",
    "yground = np.argmax(Y_val.to_numpy(), axis=1)\n",
    "# yground = np.argmax(targets, axis=1)\n",
    "\n",
    "print(ypreds)\n",
    "print(yground)\n",
    "\n",
    "\n",
    "\n",
    "score = accuracy_score(yground, ypreds)\n",
    "print('accuracy: ', score)\n",
    "\n",
    "loss = log_loss(yground, prediction)\n",
    "print('log_loss: ', loss)\n",
    "\n",
    "\n",
    "# Print the confusion matrix\n",
    "cmatrix = confusion_matrix(yground, ypreds)\n",
    "print(cmatrix)\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "creport = classification_report(yground, ypreds, digits=5)\n",
    "print(creport)\n",
    "\n",
    "\n",
    "# FF_accuracy_values.append(score)\n",
    "\n",
    "evaluation = dict(zip(model.metrics_names,evaluation))\n",
    "print(evaluation)\n",
    "with open(str(evalspath) + 'MULTIMODAL_CONCAT_acc_EVALUATION.txt', 'w') as f:\n",
    "    f.write('MULTIMODAL_CONCAT FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    for k, v in evaluation.items():\n",
    "        f.write(str(k) + ' >>> '+ str(v) + '\\n\\n')\n",
    "    f.write('MULTIMODAL_CONCAT FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    f.write('SKLEARN METRICS:')\n",
    "    f.write(str('Loss') + ' >>> '+ str(loss) + '\\n\\n')\n",
    "    f.write(str('Accuracy') + ' >>> '+ str(score) + '\\n\\n')\n",
    "    f.write(str('Confusion Matrix') + ' >>> \\n'+ str(cmatrix) + '\\n\\n')\n",
    "    f.write(str('CLASSIFICATION REPORT WITH CLASSES') + ' >>> \\n'+ str(creport) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"EfficientNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_17mlFusionC (InputLayer)  [(None, 224, 224, 3  0          []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " stem_convmlFusionC (Conv2D)    (None, 112, 112, 32  864         ['input_17mlFusionC[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_bnmlFusionC (BatchNormali  (None, 112, 112, 32  128        ['stem_convmlFusionC[0][0]']     \n",
      " zation)                        )                                                                 \n",
      "                                                                                                  \n",
      " stem_activationmlFusionC (Acti  (None, 112, 112, 32  0          ['stem_bnmlFusionC[0][0]']       \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " block1a_dwconvmlFusionC (Depth  (None, 112, 112, 32  288        ['stem_activationmlFusionC[0][0]'\n",
      " wiseConv2D)                    )                                ]                                \n",
      "                                                                                                  \n",
      " block1a_bnmlFusionC (BatchNorm  (None, 112, 112, 32  128        ['block1a_dwconvmlFusionC[0][0]']\n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " block1a_activationmlFusionC (A  (None, 112, 112, 32  0          ['block1a_bnmlFusionC[0][0]']    \n",
      " ctivation)                     )                                                                 \n",
      "                                                                                                  \n",
      " block1a_se_squeezemlFusionC (G  (None, 32)          0           ['block1a_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block1a_se_reshapemlFusionC (R  (None, 1, 1, 32)    0           ['block1a_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block1a_se_reducemlFusionC (Co  (None, 1, 1, 8)     264         ['block1a_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block1a_se_expandmlFusionC (Co  (None, 1, 1, 32)    288         ['block1a_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block1a_se_excitemlFusionC (Mu  (None, 112, 112, 32  0          ['block1a_activationmlFusionC[0][\n",
      " ltiply)                        )                                0]',                             \n",
      "                                                                  'block1a_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block1a_project_convmlFusionC   (None, 112, 112, 16  512        ['block1a_se_excitemlFusionC[0][0\n",
      " (Conv2D)                       )                                ]']                              \n",
      "                                                                                                  \n",
      " block1a_project_bnmlFusionC (B  (None, 112, 112, 16  64         ['block1a_project_convmlFusionC[0\n",
      " atchNormalization)             )                                ][0]']                           \n",
      "                                                                                                  \n",
      " block1b_dwconvmlFusionC (Depth  (None, 112, 112, 16  144        ['block1a_project_bnmlFusionC[0][\n",
      " wiseConv2D)                    )                                0]']                             \n",
      "                                                                                                  \n",
      " block1b_bnmlFusionC (BatchNorm  (None, 112, 112, 16  64         ['block1b_dwconvmlFusionC[0][0]']\n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " block1b_activationmlFusionC (A  (None, 112, 112, 16  0          ['block1b_bnmlFusionC[0][0]']    \n",
      " ctivation)                     )                                                                 \n",
      "                                                                                                  \n",
      " block1b_se_squeezemlFusionC (G  (None, 16)          0           ['block1b_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block1b_se_reshapemlFusionC (R  (None, 1, 1, 16)    0           ['block1b_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block1b_se_reducemlFusionC (Co  (None, 1, 1, 4)     68          ['block1b_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block1b_se_expandmlFusionC (Co  (None, 1, 1, 16)    80          ['block1b_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block1b_se_excitemlFusionC (Mu  (None, 112, 112, 16  0          ['block1b_activationmlFusionC[0][\n",
      " ltiply)                        )                                0]',                             \n",
      "                                                                  'block1b_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block1b_project_convmlFusionC   (None, 112, 112, 16  256        ['block1b_se_excitemlFusionC[0][0\n",
      " (Conv2D)                       )                                ]']                              \n",
      "                                                                                                  \n",
      " block1b_project_bnmlFusionC (B  (None, 112, 112, 16  64         ['block1b_project_convmlFusionC[0\n",
      " atchNormalization)             )                                ][0]']                           \n",
      "                                                                                                  \n",
      " block1b_dropmlFusionC (FixedDr  (None, 112, 112, 16  0          ['block1b_project_bnmlFusionC[0][\n",
      " opout)                         )                                0]']                             \n",
      "                                                                                                  \n",
      " block1b_addmlFusionC (Add)     (None, 112, 112, 16  0           ['block1b_dropmlFusionC[0][0]',  \n",
      "                                )                                 'block1a_project_bnmlFusionC[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " block2a_expand_convmlFusionC (  (None, 112, 112, 96  1536       ['block1b_addmlFusionC[0][0]']   \n",
      " Conv2D)                        )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_bnmlFusionC (Ba  (None, 112, 112, 96  384        ['block2a_expand_convmlFusionC[0]\n",
      " tchNormalization)              )                                [0]']                            \n",
      "                                                                                                  \n",
      " block2a_expand_activationmlFus  (None, 112, 112, 96  0          ['block2a_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)              )                                ]']                              \n",
      "                                                                                                  \n",
      " block2a_dwconvmlFusionC (Depth  (None, 56, 56, 96)  864         ['block2a_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block2a_bnmlFusionC (BatchNorm  (None, 56, 56, 96)  384         ['block2a_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block2a_activationmlFusionC (A  (None, 56, 56, 96)  0           ['block2a_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block2a_se_squeezemlFusionC (G  (None, 96)          0           ['block2a_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block2a_se_reshapemlFusionC (R  (None, 1, 1, 96)    0           ['block2a_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block2a_se_reducemlFusionC (Co  (None, 1, 1, 4)     388         ['block2a_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block2a_se_expandmlFusionC (Co  (None, 1, 1, 96)    480         ['block2a_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block2a_se_excitemlFusionC (Mu  (None, 56, 56, 96)  0           ['block2a_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block2a_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block2a_project_convmlFusionC   (None, 56, 56, 24)  2304        ['block2a_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block2a_project_bnmlFusionC (B  (None, 56, 56, 24)  96          ['block2a_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block2b_expand_convmlFusionC (  (None, 56, 56, 144)  3456       ['block2a_project_bnmlFusionC[0][\n",
      " Conv2D)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block2b_expand_bnmlFusionC (Ba  (None, 56, 56, 144)  576        ['block2b_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block2b_expand_activationmlFus  (None, 56, 56, 144)  0          ['block2b_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block2b_dwconvmlFusionC (Depth  (None, 56, 56, 144)  1296       ['block2b_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block2b_bnmlFusionC (BatchNorm  (None, 56, 56, 144)  576        ['block2b_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block2b_activationmlFusionC (A  (None, 56, 56, 144)  0          ['block2b_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block2b_se_squeezemlFusionC (G  (None, 144)         0           ['block2b_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block2b_se_reshapemlFusionC (R  (None, 1, 1, 144)   0           ['block2b_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block2b_se_reducemlFusionC (Co  (None, 1, 1, 6)     870         ['block2b_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block2b_se_expandmlFusionC (Co  (None, 1, 1, 144)   1008        ['block2b_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block2b_se_excitemlFusionC (Mu  (None, 56, 56, 144)  0          ['block2b_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block2b_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block2b_project_convmlFusionC   (None, 56, 56, 24)  3456        ['block2b_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block2b_project_bnmlFusionC (B  (None, 56, 56, 24)  96          ['block2b_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block2b_dropmlFusionC (FixedDr  (None, 56, 56, 24)  0           ['block2b_project_bnmlFusionC[0][\n",
      " opout)                                                          0]']                             \n",
      "                                                                                                  \n",
      " block2b_addmlFusionC (Add)     (None, 56, 56, 24)   0           ['block2b_dropmlFusionC[0][0]',  \n",
      "                                                                  'block2a_project_bnmlFusionC[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " block2c_expand_convmlFusionC (  (None, 56, 56, 144)  3456       ['block2b_addmlFusionC[0][0]']   \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block2c_expand_bnmlFusionC (Ba  (None, 56, 56, 144)  576        ['block2c_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block2c_expand_activationmlFus  (None, 56, 56, 144)  0          ['block2c_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block2c_dwconvmlFusionC (Depth  (None, 56, 56, 144)  1296       ['block2c_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block2c_bnmlFusionC (BatchNorm  (None, 56, 56, 144)  576        ['block2c_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block2c_activationmlFusionC (A  (None, 56, 56, 144)  0          ['block2c_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block2c_se_squeezemlFusionC (G  (None, 144)         0           ['block2c_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block2c_se_reshapemlFusionC (R  (None, 1, 1, 144)   0           ['block2c_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block2c_se_reducemlFusionC (Co  (None, 1, 1, 6)     870         ['block2c_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block2c_se_expandmlFusionC (Co  (None, 1, 1, 144)   1008        ['block2c_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block2c_se_excitemlFusionC (Mu  (None, 56, 56, 144)  0          ['block2c_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block2c_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block2c_project_convmlFusionC   (None, 56, 56, 24)  3456        ['block2c_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block2c_project_bnmlFusionC (B  (None, 56, 56, 24)  96          ['block2c_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block2c_dropmlFusionC (FixedDr  (None, 56, 56, 24)  0           ['block2c_project_bnmlFusionC[0][\n",
      " opout)                                                          0]']                             \n",
      "                                                                                                  \n",
      " block2c_addmlFusionC (Add)     (None, 56, 56, 24)   0           ['block2c_dropmlFusionC[0][0]',  \n",
      "                                                                  'block2b_addmlFusionC[0][0]']   \n",
      "                                                                                                  \n",
      " block3a_expand_convmlFusionC (  (None, 56, 56, 144)  3456       ['block2c_addmlFusionC[0][0]']   \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block3a_expand_bnmlFusionC (Ba  (None, 56, 56, 144)  576        ['block3a_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block3a_expand_activationmlFus  (None, 56, 56, 144)  0          ['block3a_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block3a_dwconvmlFusionC (Depth  (None, 28, 28, 144)  3600       ['block3a_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block3a_bnmlFusionC (BatchNorm  (None, 28, 28, 144)  576        ['block3a_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block3a_activationmlFusionC (A  (None, 28, 28, 144)  0          ['block3a_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block3a_se_squeezemlFusionC (G  (None, 144)         0           ['block3a_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block3a_se_reshapemlFusionC (R  (None, 1, 1, 144)   0           ['block3a_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block3a_se_reducemlFusionC (Co  (None, 1, 1, 6)     870         ['block3a_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block3a_se_expandmlFusionC (Co  (None, 1, 1, 144)   1008        ['block3a_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block3a_se_excitemlFusionC (Mu  (None, 28, 28, 144)  0          ['block3a_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block3a_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block3a_project_convmlFusionC   (None, 28, 28, 40)  5760        ['block3a_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block3a_project_bnmlFusionC (B  (None, 28, 28, 40)  160         ['block3a_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block3b_expand_convmlFusionC (  (None, 28, 28, 240)  9600       ['block3a_project_bnmlFusionC[0][\n",
      " Conv2D)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block3b_expand_bnmlFusionC (Ba  (None, 28, 28, 240)  960        ['block3b_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block3b_expand_activationmlFus  (None, 28, 28, 240)  0          ['block3b_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block3b_dwconvmlFusionC (Depth  (None, 28, 28, 240)  6000       ['block3b_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block3b_bnmlFusionC (BatchNorm  (None, 28, 28, 240)  960        ['block3b_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block3b_activationmlFusionC (A  (None, 28, 28, 240)  0          ['block3b_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block3b_se_squeezemlFusionC (G  (None, 240)         0           ['block3b_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block3b_se_reshapemlFusionC (R  (None, 1, 1, 240)   0           ['block3b_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block3b_se_reducemlFusionC (Co  (None, 1, 1, 10)    2410        ['block3b_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block3b_se_expandmlFusionC (Co  (None, 1, 1, 240)   2640        ['block3b_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block3b_se_excitemlFusionC (Mu  (None, 28, 28, 240)  0          ['block3b_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block3b_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block3b_project_convmlFusionC   (None, 28, 28, 40)  9600        ['block3b_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block3b_project_bnmlFusionC (B  (None, 28, 28, 40)  160         ['block3b_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block3b_dropmlFusionC (FixedDr  (None, 28, 28, 40)  0           ['block3b_project_bnmlFusionC[0][\n",
      " opout)                                                          0]']                             \n",
      "                                                                                                  \n",
      " block3b_addmlFusionC (Add)     (None, 28, 28, 40)   0           ['block3b_dropmlFusionC[0][0]',  \n",
      "                                                                  'block3a_project_bnmlFusionC[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " block3c_expand_convmlFusionC (  (None, 28, 28, 240)  9600       ['block3b_addmlFusionC[0][0]']   \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block3c_expand_bnmlFusionC (Ba  (None, 28, 28, 240)  960        ['block3c_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block3c_expand_activationmlFus  (None, 28, 28, 240)  0          ['block3c_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block3c_dwconvmlFusionC (Depth  (None, 28, 28, 240)  6000       ['block3c_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block3c_bnmlFusionC (BatchNorm  (None, 28, 28, 240)  960        ['block3c_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block3c_activationmlFusionC (A  (None, 28, 28, 240)  0          ['block3c_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block3c_se_squeezemlFusionC (G  (None, 240)         0           ['block3c_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block3c_se_reshapemlFusionC (R  (None, 1, 1, 240)   0           ['block3c_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block3c_se_reducemlFusionC (Co  (None, 1, 1, 10)    2410        ['block3c_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block3c_se_expandmlFusionC (Co  (None, 1, 1, 240)   2640        ['block3c_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block3c_se_excitemlFusionC (Mu  (None, 28, 28, 240)  0          ['block3c_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block3c_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block3c_project_convmlFusionC   (None, 28, 28, 40)  9600        ['block3c_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block3c_project_bnmlFusionC (B  (None, 28, 28, 40)  160         ['block3c_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block3c_dropmlFusionC (FixedDr  (None, 28, 28, 40)  0           ['block3c_project_bnmlFusionC[0][\n",
      " opout)                                                          0]']                             \n",
      "                                                                                                  \n",
      " block3c_addmlFusionC (Add)     (None, 28, 28, 40)   0           ['block3c_dropmlFusionC[0][0]',  \n",
      "                                                                  'block3b_addmlFusionC[0][0]']   \n",
      "                                                                                                  \n",
      " block4a_expand_convmlFusionC (  (None, 28, 28, 240)  9600       ['block3c_addmlFusionC[0][0]']   \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block4a_expand_bnmlFusionC (Ba  (None, 28, 28, 240)  960        ['block4a_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block4a_expand_activationmlFus  (None, 28, 28, 240)  0          ['block4a_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block4a_dwconvmlFusionC (Depth  (None, 14, 14, 240)  2160       ['block4a_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block4a_bnmlFusionC (BatchNorm  (None, 14, 14, 240)  960        ['block4a_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block4a_activationmlFusionC (A  (None, 14, 14, 240)  0          ['block4a_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block4a_se_squeezemlFusionC (G  (None, 240)         0           ['block4a_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block4a_se_reshapemlFusionC (R  (None, 1, 1, 240)   0           ['block4a_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block4a_se_reducemlFusionC (Co  (None, 1, 1, 10)    2410        ['block4a_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block4a_se_expandmlFusionC (Co  (None, 1, 1, 240)   2640        ['block4a_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block4a_se_excitemlFusionC (Mu  (None, 14, 14, 240)  0          ['block4a_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block4a_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block4a_project_convmlFusionC   (None, 14, 14, 80)  19200       ['block4a_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block4a_project_bnmlFusionC (B  (None, 14, 14, 80)  320         ['block4a_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block4b_expand_convmlFusionC (  (None, 14, 14, 480)  38400      ['block4a_project_bnmlFusionC[0][\n",
      " Conv2D)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block4b_expand_bnmlFusionC (Ba  (None, 14, 14, 480)  1920       ['block4b_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block4b_expand_activationmlFus  (None, 14, 14, 480)  0          ['block4b_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block4b_dwconvmlFusionC (Depth  (None, 14, 14, 480)  4320       ['block4b_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block4b_bnmlFusionC (BatchNorm  (None, 14, 14, 480)  1920       ['block4b_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block4b_activationmlFusionC (A  (None, 14, 14, 480)  0          ['block4b_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block4b_se_squeezemlFusionC (G  (None, 480)         0           ['block4b_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block4b_se_reshapemlFusionC (R  (None, 1, 1, 480)   0           ['block4b_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block4b_se_reducemlFusionC (Co  (None, 1, 1, 20)    9620        ['block4b_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block4b_se_expandmlFusionC (Co  (None, 1, 1, 480)   10080       ['block4b_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block4b_se_excitemlFusionC (Mu  (None, 14, 14, 480)  0          ['block4b_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block4b_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block4b_project_convmlFusionC   (None, 14, 14, 80)  38400       ['block4b_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block4b_project_bnmlFusionC (B  (None, 14, 14, 80)  320         ['block4b_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block4b_dropmlFusionC (FixedDr  (None, 14, 14, 80)  0           ['block4b_project_bnmlFusionC[0][\n",
      " opout)                                                          0]']                             \n",
      "                                                                                                  \n",
      " block4b_addmlFusionC (Add)     (None, 14, 14, 80)   0           ['block4b_dropmlFusionC[0][0]',  \n",
      "                                                                  'block4a_project_bnmlFusionC[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " block4c_expand_convmlFusionC (  (None, 14, 14, 480)  38400      ['block4b_addmlFusionC[0][0]']   \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block4c_expand_bnmlFusionC (Ba  (None, 14, 14, 480)  1920       ['block4c_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block4c_expand_activationmlFus  (None, 14, 14, 480)  0          ['block4c_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block4c_dwconvmlFusionC (Depth  (None, 14, 14, 480)  4320       ['block4c_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block4c_bnmlFusionC (BatchNorm  (None, 14, 14, 480)  1920       ['block4c_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block4c_activationmlFusionC (A  (None, 14, 14, 480)  0          ['block4c_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block4c_se_squeezemlFusionC (G  (None, 480)         0           ['block4c_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block4c_se_reshapemlFusionC (R  (None, 1, 1, 480)   0           ['block4c_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block4c_se_reducemlFusionC (Co  (None, 1, 1, 20)    9620        ['block4c_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block4c_se_expandmlFusionC (Co  (None, 1, 1, 480)   10080       ['block4c_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block4c_se_excitemlFusionC (Mu  (None, 14, 14, 480)  0          ['block4c_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block4c_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block4c_project_convmlFusionC   (None, 14, 14, 80)  38400       ['block4c_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block4c_project_bnmlFusionC (B  (None, 14, 14, 80)  320         ['block4c_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block4c_dropmlFusionC (FixedDr  (None, 14, 14, 80)  0           ['block4c_project_bnmlFusionC[0][\n",
      " opout)                                                          0]']                             \n",
      "                                                                                                  \n",
      " block4c_addmlFusionC (Add)     (None, 14, 14, 80)   0           ['block4c_dropmlFusionC[0][0]',  \n",
      "                                                                  'block4b_addmlFusionC[0][0]']   \n",
      "                                                                                                  \n",
      " block4d_expand_convmlFusionC (  (None, 14, 14, 480)  38400      ['block4c_addmlFusionC[0][0]']   \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block4d_expand_bnmlFusionC (Ba  (None, 14, 14, 480)  1920       ['block4d_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block4d_expand_activationmlFus  (None, 14, 14, 480)  0          ['block4d_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block4d_dwconvmlFusionC (Depth  (None, 14, 14, 480)  4320       ['block4d_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block4d_bnmlFusionC (BatchNorm  (None, 14, 14, 480)  1920       ['block4d_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block4d_activationmlFusionC (A  (None, 14, 14, 480)  0          ['block4d_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block4d_se_squeezemlFusionC (G  (None, 480)         0           ['block4d_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block4d_se_reshapemlFusionC (R  (None, 1, 1, 480)   0           ['block4d_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block4d_se_reducemlFusionC (Co  (None, 1, 1, 20)    9620        ['block4d_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block4d_se_expandmlFusionC (Co  (None, 1, 1, 480)   10080       ['block4d_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block4d_se_excitemlFusionC (Mu  (None, 14, 14, 480)  0          ['block4d_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block4d_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block4d_project_convmlFusionC   (None, 14, 14, 80)  38400       ['block4d_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block4d_project_bnmlFusionC (B  (None, 14, 14, 80)  320         ['block4d_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block4d_dropmlFusionC (FixedDr  (None, 14, 14, 80)  0           ['block4d_project_bnmlFusionC[0][\n",
      " opout)                                                          0]']                             \n",
      "                                                                                                  \n",
      " block4d_addmlFusionC (Add)     (None, 14, 14, 80)   0           ['block4d_dropmlFusionC[0][0]',  \n",
      "                                                                  'block4c_addmlFusionC[0][0]']   \n",
      "                                                                                                  \n",
      " block5a_expand_convmlFusionC (  (None, 14, 14, 480)  38400      ['block4d_addmlFusionC[0][0]']   \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block5a_expand_bnmlFusionC (Ba  (None, 14, 14, 480)  1920       ['block5a_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block5a_expand_activationmlFus  (None, 14, 14, 480)  0          ['block5a_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block5a_dwconvmlFusionC (Depth  (None, 14, 14, 480)  12000      ['block5a_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block5a_bnmlFusionC (BatchNorm  (None, 14, 14, 480)  1920       ['block5a_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block5a_activationmlFusionC (A  (None, 14, 14, 480)  0          ['block5a_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block5a_se_squeezemlFusionC (G  (None, 480)         0           ['block5a_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block5a_se_reshapemlFusionC (R  (None, 1, 1, 480)   0           ['block5a_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block5a_se_reducemlFusionC (Co  (None, 1, 1, 20)    9620        ['block5a_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block5a_se_expandmlFusionC (Co  (None, 1, 1, 480)   10080       ['block5a_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block5a_se_excitemlFusionC (Mu  (None, 14, 14, 480)  0          ['block5a_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block5a_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block5a_project_convmlFusionC   (None, 14, 14, 112)  53760      ['block5a_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block5a_project_bnmlFusionC (B  (None, 14, 14, 112)  448        ['block5a_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block5b_expand_convmlFusionC (  (None, 14, 14, 672)  75264      ['block5a_project_bnmlFusionC[0][\n",
      " Conv2D)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block5b_expand_bnmlFusionC (Ba  (None, 14, 14, 672)  2688       ['block5b_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block5b_expand_activationmlFus  (None, 14, 14, 672)  0          ['block5b_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block5b_dwconvmlFusionC (Depth  (None, 14, 14, 672)  16800      ['block5b_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block5b_bnmlFusionC (BatchNorm  (None, 14, 14, 672)  2688       ['block5b_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block5b_activationmlFusionC (A  (None, 14, 14, 672)  0          ['block5b_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block5b_se_squeezemlFusionC (G  (None, 672)         0           ['block5b_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block5b_se_reshapemlFusionC (R  (None, 1, 1, 672)   0           ['block5b_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block5b_se_reducemlFusionC (Co  (None, 1, 1, 28)    18844       ['block5b_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block5b_se_expandmlFusionC (Co  (None, 1, 1, 672)   19488       ['block5b_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block5b_se_excitemlFusionC (Mu  (None, 14, 14, 672)  0          ['block5b_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block5b_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block5b_project_convmlFusionC   (None, 14, 14, 112)  75264      ['block5b_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block5b_project_bnmlFusionC (B  (None, 14, 14, 112)  448        ['block5b_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block5b_dropmlFusionC (FixedDr  (None, 14, 14, 112)  0          ['block5b_project_bnmlFusionC[0][\n",
      " opout)                                                          0]']                             \n",
      "                                                                                                  \n",
      " block5b_addmlFusionC (Add)     (None, 14, 14, 112)  0           ['block5b_dropmlFusionC[0][0]',  \n",
      "                                                                  'block5a_project_bnmlFusionC[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " block5c_expand_convmlFusionC (  (None, 14, 14, 672)  75264      ['block5b_addmlFusionC[0][0]']   \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block5c_expand_bnmlFusionC (Ba  (None, 14, 14, 672)  2688       ['block5c_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block5c_expand_activationmlFus  (None, 14, 14, 672)  0          ['block5c_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block5c_dwconvmlFusionC (Depth  (None, 14, 14, 672)  16800      ['block5c_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block5c_bnmlFusionC (BatchNorm  (None, 14, 14, 672)  2688       ['block5c_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block5c_activationmlFusionC (A  (None, 14, 14, 672)  0          ['block5c_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block5c_se_squeezemlFusionC (G  (None, 672)         0           ['block5c_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block5c_se_reshapemlFusionC (R  (None, 1, 1, 672)   0           ['block5c_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block5c_se_reducemlFusionC (Co  (None, 1, 1, 28)    18844       ['block5c_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block5c_se_expandmlFusionC (Co  (None, 1, 1, 672)   19488       ['block5c_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block5c_se_excitemlFusionC (Mu  (None, 14, 14, 672)  0          ['block5c_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block5c_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block5c_project_convmlFusionC   (None, 14, 14, 112)  75264      ['block5c_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block5c_project_bnmlFusionC (B  (None, 14, 14, 112)  448        ['block5c_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block5c_dropmlFusionC (FixedDr  (None, 14, 14, 112)  0          ['block5c_project_bnmlFusionC[0][\n",
      " opout)                                                          0]']                             \n",
      "                                                                                                  \n",
      " block5c_addmlFusionC (Add)     (None, 14, 14, 112)  0           ['block5c_dropmlFusionC[0][0]',  \n",
      "                                                                  'block5b_addmlFusionC[0][0]']   \n",
      "                                                                                                  \n",
      " block5d_expand_convmlFusionC (  (None, 14, 14, 672)  75264      ['block5c_addmlFusionC[0][0]']   \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block5d_expand_bnmlFusionC (Ba  (None, 14, 14, 672)  2688       ['block5d_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block5d_expand_activationmlFus  (None, 14, 14, 672)  0          ['block5d_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block5d_dwconvmlFusionC (Depth  (None, 14, 14, 672)  16800      ['block5d_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block5d_bnmlFusionC (BatchNorm  (None, 14, 14, 672)  2688       ['block5d_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block5d_activationmlFusionC (A  (None, 14, 14, 672)  0          ['block5d_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block5d_se_squeezemlFusionC (G  (None, 672)         0           ['block5d_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block5d_se_reshapemlFusionC (R  (None, 1, 1, 672)   0           ['block5d_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block5d_se_reducemlFusionC (Co  (None, 1, 1, 28)    18844       ['block5d_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block5d_se_expandmlFusionC (Co  (None, 1, 1, 672)   19488       ['block5d_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block5d_se_excitemlFusionC (Mu  (None, 14, 14, 672)  0          ['block5d_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block5d_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block5d_project_convmlFusionC   (None, 14, 14, 112)  75264      ['block5d_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block5d_project_bnmlFusionC (B  (None, 14, 14, 112)  448        ['block5d_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block5d_dropmlFusionC (FixedDr  (None, 14, 14, 112)  0          ['block5d_project_bnmlFusionC[0][\n",
      " opout)                                                          0]']                             \n",
      "                                                                                                  \n",
      " block5d_addmlFusionC (Add)     (None, 14, 14, 112)  0           ['block5d_dropmlFusionC[0][0]',  \n",
      "                                                                  'block5c_addmlFusionC[0][0]']   \n",
      "                                                                                                  \n",
      " block6a_expand_convmlFusionC (  (None, 14, 14, 672)  75264      ['block5d_addmlFusionC[0][0]']   \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block6a_expand_bnmlFusionC (Ba  (None, 14, 14, 672)  2688       ['block6a_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block6a_expand_activationmlFus  (None, 14, 14, 672)  0          ['block6a_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block6a_dwconvmlFusionC (Depth  (None, 7, 7, 672)   16800       ['block6a_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block6a_bnmlFusionC (BatchNorm  (None, 7, 7, 672)   2688        ['block6a_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block6a_activationmlFusionC (A  (None, 7, 7, 672)   0           ['block6a_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block6a_se_squeezemlFusionC (G  (None, 672)         0           ['block6a_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block6a_se_reshapemlFusionC (R  (None, 1, 1, 672)   0           ['block6a_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block6a_se_reducemlFusionC (Co  (None, 1, 1, 28)    18844       ['block6a_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block6a_se_expandmlFusionC (Co  (None, 1, 1, 672)   19488       ['block6a_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block6a_se_excitemlFusionC (Mu  (None, 7, 7, 672)   0           ['block6a_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block6a_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block6a_project_convmlFusionC   (None, 7, 7, 192)   129024      ['block6a_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block6a_project_bnmlFusionC (B  (None, 7, 7, 192)   768         ['block6a_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block6b_expand_convmlFusionC (  (None, 7, 7, 1152)  221184      ['block6a_project_bnmlFusionC[0][\n",
      " Conv2D)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block6b_expand_bnmlFusionC (Ba  (None, 7, 7, 1152)  4608        ['block6b_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block6b_expand_activationmlFus  (None, 7, 7, 1152)  0           ['block6b_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block6b_dwconvmlFusionC (Depth  (None, 7, 7, 1152)  28800       ['block6b_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block6b_bnmlFusionC (BatchNorm  (None, 7, 7, 1152)  4608        ['block6b_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block6b_activationmlFusionC (A  (None, 7, 7, 1152)  0           ['block6b_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block6b_se_squeezemlFusionC (G  (None, 1152)        0           ['block6b_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block6b_se_reshapemlFusionC (R  (None, 1, 1, 1152)  0           ['block6b_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block6b_se_reducemlFusionC (Co  (None, 1, 1, 48)    55344       ['block6b_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block6b_se_expandmlFusionC (Co  (None, 1, 1, 1152)  56448       ['block6b_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block6b_se_excitemlFusionC (Mu  (None, 7, 7, 1152)  0           ['block6b_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block6b_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block6b_project_convmlFusionC   (None, 7, 7, 192)   221184      ['block6b_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block6b_project_bnmlFusionC (B  (None, 7, 7, 192)   768         ['block6b_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block6b_dropmlFusionC (FixedDr  (None, 7, 7, 192)   0           ['block6b_project_bnmlFusionC[0][\n",
      " opout)                                                          0]']                             \n",
      "                                                                                                  \n",
      " block6b_addmlFusionC (Add)     (None, 7, 7, 192)    0           ['block6b_dropmlFusionC[0][0]',  \n",
      "                                                                  'block6a_project_bnmlFusionC[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " block6c_expand_convmlFusionC (  (None, 7, 7, 1152)  221184      ['block6b_addmlFusionC[0][0]']   \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block6c_expand_bnmlFusionC (Ba  (None, 7, 7, 1152)  4608        ['block6c_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block6c_expand_activationmlFus  (None, 7, 7, 1152)  0           ['block6c_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block6c_dwconvmlFusionC (Depth  (None, 7, 7, 1152)  28800       ['block6c_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block6c_bnmlFusionC (BatchNorm  (None, 7, 7, 1152)  4608        ['block6c_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block6c_activationmlFusionC (A  (None, 7, 7, 1152)  0           ['block6c_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block6c_se_squeezemlFusionC (G  (None, 1152)        0           ['block6c_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block6c_se_reshapemlFusionC (R  (None, 1, 1, 1152)  0           ['block6c_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block6c_se_reducemlFusionC (Co  (None, 1, 1, 48)    55344       ['block6c_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block6c_se_expandmlFusionC (Co  (None, 1, 1, 1152)  56448       ['block6c_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block6c_se_excitemlFusionC (Mu  (None, 7, 7, 1152)  0           ['block6c_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block6c_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block6c_project_convmlFusionC   (None, 7, 7, 192)   221184      ['block6c_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block6c_project_bnmlFusionC (B  (None, 7, 7, 192)   768         ['block6c_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block6c_dropmlFusionC (FixedDr  (None, 7, 7, 192)   0           ['block6c_project_bnmlFusionC[0][\n",
      " opout)                                                          0]']                             \n",
      "                                                                                                  \n",
      " block6c_addmlFusionC (Add)     (None, 7, 7, 192)    0           ['block6c_dropmlFusionC[0][0]',  \n",
      "                                                                  'block6b_addmlFusionC[0][0]']   \n",
      "                                                                                                  \n",
      " block6d_expand_convmlFusionC (  (None, 7, 7, 1152)  221184      ['block6c_addmlFusionC[0][0]']   \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block6d_expand_bnmlFusionC (Ba  (None, 7, 7, 1152)  4608        ['block6d_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block6d_expand_activationmlFus  (None, 7, 7, 1152)  0           ['block6d_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block6d_dwconvmlFusionC (Depth  (None, 7, 7, 1152)  28800       ['block6d_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block6d_bnmlFusionC (BatchNorm  (None, 7, 7, 1152)  4608        ['block6d_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block6d_activationmlFusionC (A  (None, 7, 7, 1152)  0           ['block6d_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block6d_se_squeezemlFusionC (G  (None, 1152)        0           ['block6d_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block6d_se_reshapemlFusionC (R  (None, 1, 1, 1152)  0           ['block6d_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block6d_se_reducemlFusionC (Co  (None, 1, 1, 48)    55344       ['block6d_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block6d_se_expandmlFusionC (Co  (None, 1, 1, 1152)  56448       ['block6d_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block6d_se_excitemlFusionC (Mu  (None, 7, 7, 1152)  0           ['block6d_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block6d_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block6d_project_convmlFusionC   (None, 7, 7, 192)   221184      ['block6d_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block6d_project_bnmlFusionC (B  (None, 7, 7, 192)   768         ['block6d_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block6d_dropmlFusionC (FixedDr  (None, 7, 7, 192)   0           ['block6d_project_bnmlFusionC[0][\n",
      " opout)                                                          0]']                             \n",
      "                                                                                                  \n",
      " block6d_addmlFusionC (Add)     (None, 7, 7, 192)    0           ['block6d_dropmlFusionC[0][0]',  \n",
      "                                                                  'block6c_addmlFusionC[0][0]']   \n",
      "                                                                                                  \n",
      " block6e_expand_convmlFusionC (  (None, 7, 7, 1152)  221184      ['block6d_addmlFusionC[0][0]']   \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block6e_expand_bnmlFusionC (Ba  (None, 7, 7, 1152)  4608        ['block6e_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block6e_expand_activationmlFus  (None, 7, 7, 1152)  0           ['block6e_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block6e_dwconvmlFusionC (Depth  (None, 7, 7, 1152)  28800       ['block6e_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block6e_bnmlFusionC (BatchNorm  (None, 7, 7, 1152)  4608        ['block6e_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block6e_activationmlFusionC (A  (None, 7, 7, 1152)  0           ['block6e_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block6e_se_squeezemlFusionC (G  (None, 1152)        0           ['block6e_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block6e_se_reshapemlFusionC (R  (None, 1, 1, 1152)  0           ['block6e_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block6e_se_reducemlFusionC (Co  (None, 1, 1, 48)    55344       ['block6e_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block6e_se_expandmlFusionC (Co  (None, 1, 1, 1152)  56448       ['block6e_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block6e_se_excitemlFusionC (Mu  (None, 7, 7, 1152)  0           ['block6e_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block6e_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block6e_project_convmlFusionC   (None, 7, 7, 192)   221184      ['block6e_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block6e_project_bnmlFusionC (B  (None, 7, 7, 192)   768         ['block6e_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block6e_dropmlFusionC (FixedDr  (None, 7, 7, 192)   0           ['block6e_project_bnmlFusionC[0][\n",
      " opout)                                                          0]']                             \n",
      "                                                                                                  \n",
      " block6e_addmlFusionC (Add)     (None, 7, 7, 192)    0           ['block6e_dropmlFusionC[0][0]',  \n",
      "                                                                  'block6d_addmlFusionC[0][0]']   \n",
      "                                                                                                  \n",
      " block7a_expand_convmlFusionC (  (None, 7, 7, 1152)  221184      ['block6e_addmlFusionC[0][0]']   \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " block7a_expand_bnmlFusionC (Ba  (None, 7, 7, 1152)  4608        ['block7a_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block7a_expand_activationmlFus  (None, 7, 7, 1152)  0           ['block7a_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block7a_dwconvmlFusionC (Depth  (None, 7, 7, 1152)  10368       ['block7a_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block7a_bnmlFusionC (BatchNorm  (None, 7, 7, 1152)  4608        ['block7a_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block7a_activationmlFusionC (A  (None, 7, 7, 1152)  0           ['block7a_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block7a_se_squeezemlFusionC (G  (None, 1152)        0           ['block7a_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block7a_se_reshapemlFusionC (R  (None, 1, 1, 1152)  0           ['block7a_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block7a_se_reducemlFusionC (Co  (None, 1, 1, 48)    55344       ['block7a_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block7a_se_expandmlFusionC (Co  (None, 1, 1, 1152)  56448       ['block7a_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block7a_se_excitemlFusionC (Mu  (None, 7, 7, 1152)  0           ['block7a_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block7a_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block7a_project_convmlFusionC   (None, 7, 7, 320)   368640      ['block7a_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block7a_project_bnmlFusionC (B  (None, 7, 7, 320)   1280        ['block7a_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block7b_expand_convmlFusionC (  (None, 7, 7, 1920)  614400      ['block7a_project_bnmlFusionC[0][\n",
      " Conv2D)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block7b_expand_bnmlFusionC (Ba  (None, 7, 7, 1920)  7680        ['block7b_expand_convmlFusionC[0]\n",
      " tchNormalization)                                               [0]']                            \n",
      "                                                                                                  \n",
      " block7b_expand_activationmlFus  (None, 7, 7, 1920)  0           ['block7b_expand_bnmlFusionC[0][0\n",
      " ionC (Activation)                                               ]']                              \n",
      "                                                                                                  \n",
      " block7b_dwconvmlFusionC (Depth  (None, 7, 7, 1920)  17280       ['block7b_expand_activationmlFusi\n",
      " wiseConv2D)                                                     onC[0][0]']                      \n",
      "                                                                                                  \n",
      " block7b_bnmlFusionC (BatchNorm  (None, 7, 7, 1920)  7680        ['block7b_dwconvmlFusionC[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block7b_activationmlFusionC (A  (None, 7, 7, 1920)  0           ['block7b_bnmlFusionC[0][0]']    \n",
      " ctivation)                                                                                       \n",
      "                                                                                                  \n",
      " block7b_se_squeezemlFusionC (G  (None, 1920)        0           ['block7b_activationmlFusionC[0][\n",
      " lobalAveragePooling2D)                                          0]']                             \n",
      "                                                                                                  \n",
      " block7b_se_reshapemlFusionC (R  (None, 1, 1, 1920)  0           ['block7b_se_squeezemlFusionC[0][\n",
      " eshape)                                                         0]']                             \n",
      "                                                                                                  \n",
      " block7b_se_reducemlFusionC (Co  (None, 1, 1, 80)    153680      ['block7b_se_reshapemlFusionC[0][\n",
      " nv2D)                                                           0]']                             \n",
      "                                                                                                  \n",
      " block7b_se_expandmlFusionC (Co  (None, 1, 1, 1920)  155520      ['block7b_se_reducemlFusionC[0][0\n",
      " nv2D)                                                           ]']                              \n",
      "                                                                                                  \n",
      " block7b_se_excitemlFusionC (Mu  (None, 7, 7, 1920)  0           ['block7b_activationmlFusionC[0][\n",
      " ltiply)                                                         0]',                             \n",
      "                                                                  'block7b_se_expandmlFusionC[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block7b_project_convmlFusionC   (None, 7, 7, 320)   614400      ['block7b_se_excitemlFusionC[0][0\n",
      " (Conv2D)                                                        ]']                              \n",
      "                                                                                                  \n",
      " block7b_project_bnmlFusionC (B  (None, 7, 7, 320)   1280        ['block7b_project_convmlFusionC[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " block7b_dropmlFusionC (FixedDr  (None, 7, 7, 320)   0           ['block7b_project_bnmlFusionC[0][\n",
      " opout)                                                          0]']                             \n",
      "                                                                                                  \n",
      " block7b_addmlFusionC (Add)     (None, 7, 7, 320)    0           ['block7b_dropmlFusionC[0][0]',  \n",
      "                                                                  'block7a_project_bnmlFusionC[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " top_convmlFusionC (Conv2D)     (None, 7, 7, 1280)   409600      ['block7b_addmlFusionC[0][0]']   \n",
      "                                                                                                  \n",
      " top_bnmlFusionC (BatchNormaliz  (None, 7, 7, 1280)  5120        ['top_convmlFusionC[0][0]']      \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " top_activationmlFusionC (Activ  (None, 7, 7, 1280)  0           ['top_bnmlFusionC[0][0]']        \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " avg_poolmlFusionC (GlobalAvera  (None, 1280)        0           ['top_activationmlFusionC[0][0]']\n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_46mlFusion  (None, 1280)        5120        ['avg_poolmlFusionC[0][0]']      \n",
      " C (BatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " top_dropoutmlFusionC (Dropout)  (None, 1280)        0           ['batch_normalization_46mlFusionC\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      " dense_86mlFusionC (Dense)      (None, 512)          655872      ['top_dropoutmlFusionC[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_55mlFusionC (Dropout)  (None, 512)          0           ['dense_86mlFusionC[0][0]']      \n",
      "                                                                                                  \n",
      " dense_87mlFusionC (Dense)      (None, 128)          65664       ['dropout_55mlFusionC[0][0]']    \n",
      "                                                                                                  \n",
      " dense_88mlFusionC (Dense)      (None, 16)           2064        ['dense_87mlFusionC[0][0]']      \n",
      "                                                                                                  \n",
      " dense_89mlFusionC (Dense)      (None, 4)            68          ['dense_88mlFusionC[0][0]']      \n",
      "                                                                                                  \n",
      " predmlFusionC (Dense)          (None, 2)            10          ['dense_89mlFusionC[0][0]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,304,030\n",
      "Trainable params: 0\n",
      "Non-trainable params: 7,304,030\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNNmod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_15_MLP (Dense)        (None, 16)                768       \n",
      "                                                                 \n",
      " dropout_16_MLP (Dropout)    (None, 16)                0         \n",
      "                                                                 \n",
      " dense_16_MLP (Dense)        (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_17_MLP (Dense)        (None, 8)                 72        \n",
      "                                                                 \n",
      " dropout_17_MLP (Dropout)    (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_18_MLP (Dense)        (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_19_MLP (Dense)        (None, 2)                 10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,022\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1,022\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MLPmod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decision_Multimodal_DEMPSTER(MLP, CNN, mlpOutL = -1, cnnOutL = -1, uncertainty = False):\n",
    "\n",
    "    # outMLP = MLP.layers[cnnOutL].output\n",
    "    outMLP = MLP.output\n",
    "    # outCNN = CNN.layers[mlpOutL].output\n",
    "    outCNN = CNN.output\n",
    "\n",
    "    outMLP = BatchNormalization()(outMLP)\n",
    "    outCNN = BatchNormalization()(outCNN)\n",
    "\n",
    "    massesMLP, uncMLP = MassesWithUncertainty(outMLP,1)\n",
    "    massesCNN, uncCNN = MassesWithUncertainty(outCNN,2)\n",
    "\n",
    "    if uncertainty:\n",
    "        massesMLP = Multiply()([massesMLP,uncMLP])\n",
    "        massesCNN = Multiply()([massesCNN,uncCNN])\n",
    "\n",
    "\n",
    "    combination = Lambda(CombineMasses, name='massCombination')((massesMLP,massesCNN))\n",
    "\n",
    "    pigProb = Lambda(mass_to_pignistic, name='pigProb')(combination)\n",
    "\n",
    "    pigProb = Reshape((pigProb.shape[-1],))(pigProb)\n",
    "\n",
    "    output = pigProb\n",
    "\n",
    "\n",
    "    model = Model(inputs=[CNN.input,MLP.input], outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER\n",
      "Epoch 1/500\n",
      "(1, 2)\n",
      "(32, 2)\n",
      "(1, 2)\n",
      "(32, 2)\n",
      "(32, 1, 3)\n",
      "(32, 1, 3)\n",
      "(32, 1, 3)\n",
      "(32, 3, 1)\n",
      "(32, 1, 3)\n",
      "(1, 2)\n",
      "(32, 2)\n",
      "(1, 2)\n",
      "(32, 2)\n",
      "(32, 1, 3)\n",
      "(32, 1, 3)\n",
      "(32, 1, 3)\n",
      "(32, 3, 1)\n",
      "(32, 1, 3)\n",
      " 5/31 [===>..........................] - ETA: 0s - loss: 4.3633 - binary_accuracy: 0.5406 - f1_score: 0.5429WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0150s vs `on_train_batch_end` time: 0.0575s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0150s vs `on_train_batch_end` time: 0.0575s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - ETA: 0s - loss: 4.1514 - binary_accuracy: 0.5771 - f1_score: 0.5764(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 37s 1s/step - loss: 4.1514 - binary_accuracy: 0.5771 - f1_score: 0.5764 - val_loss: 4.2986 - val_binary_accuracy: 0.4865 - val_f1_score: 0.3273\n",
      "Epoch 2/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 4.2317 - binary_accuracy: 0.5736 - f1_score: 0.5586 - val_loss: 4.4278 - val_binary_accuracy: 0.4865 - val_f1_score: 0.3273\n",
      "Epoch 3/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 3.6926 - binary_accuracy: 0.6210 - f1_score: 0.6175(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 33s 1s/step - loss: 3.6926 - binary_accuracy: 0.6210 - f1_score: 0.6175 - val_loss: 4.3213 - val_binary_accuracy: 0.4955 - val_f1_score: 0.3784\n",
      "Epoch 4/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 3.8564 - binary_accuracy: 0.6174 - f1_score: 0.5890(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 29s 947ms/step - loss: 3.8564 - binary_accuracy: 0.6174 - f1_score: 0.5890 - val_loss: 3.8797 - val_binary_accuracy: 0.5225 - val_f1_score: 0.4912\n",
      "Epoch 5/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 3.5314 - binary_accuracy: 0.6477 - f1_score: 0.6098(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 33s 1s/step - loss: 3.5314 - binary_accuracy: 0.6477 - f1_score: 0.6098 - val_loss: 3.3995 - val_binary_accuracy: 0.5586 - val_f1_score: 0.6248\n",
      "Epoch 6/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 3.8019 - binary_accuracy: 0.6341 - f1_score: 0.5857(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 488ms/step - loss: 3.8019 - binary_accuracy: 0.6341 - f1_score: 0.5857 - val_loss: 3.3114 - val_binary_accuracy: 0.5586 - val_f1_score: 0.6513\n",
      "Epoch 7/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 3.3603 - binary_accuracy: 0.6689 - f1_score: 0.6208(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 20s 643ms/step - loss: 3.3603 - binary_accuracy: 0.6689 - f1_score: 0.6208 - val_loss: 3.0603 - val_binary_accuracy: 0.5586 - val_f1_score: 0.6754\n",
      "Epoch 8/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 3.2188 - binary_accuracy: 0.6845 - f1_score: 0.6456(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 29s 953ms/step - loss: 3.2188 - binary_accuracy: 0.6845 - f1_score: 0.6456 - val_loss: 2.4690 - val_binary_accuracy: 0.5946 - val_f1_score: 0.7287\n",
      "Epoch 9/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 2.7975 - binary_accuracy: 0.7097 - f1_score: 0.7009(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 19s 634ms/step - loss: 2.7975 - binary_accuracy: 0.7097 - f1_score: 0.7009 - val_loss: 2.4397 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7279\n",
      "Epoch 10/500\n",
      "31/31 [==============================] - 2s 38ms/step - loss: 2.6952 - binary_accuracy: 0.6850 - f1_score: 0.6835 - val_loss: 2.4904 - val_binary_accuracy: 0.6126 - val_f1_score: 0.7193\n",
      "Epoch 11/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.7637 - binary_accuracy: 0.6910 - f1_score: 0.6768 - val_loss: 2.4094 - val_binary_accuracy: 0.6126 - val_f1_score: 0.7270\n",
      "Epoch 12/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.7148 - binary_accuracy: 0.6996 - f1_score: 0.6907 - val_loss: 2.3148 - val_binary_accuracy: 0.6081 - val_f1_score: 0.7184\n",
      "Epoch 13/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 2.7781 - binary_accuracy: 0.6885 - f1_score: 0.6860(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 495ms/step - loss: 2.7781 - binary_accuracy: 0.6885 - f1_score: 0.6860 - val_loss: 2.1902 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7279\n",
      "Epoch 14/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.3682 - binary_accuracy: 0.6986 - f1_score: 0.7343 - val_loss: 2.2943 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7184\n",
      "Epoch 15/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.6050 - binary_accuracy: 0.6920 - f1_score: 0.7012 - val_loss: 2.2316 - val_binary_accuracy: 0.6216 - val_f1_score: 0.7193\n",
      "Epoch 16/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 2.6792 - binary_accuracy: 0.6754 - f1_score: 0.6651(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 32s 1s/step - loss: 2.6792 - binary_accuracy: 0.6754 - f1_score: 0.6651 - val_loss: 2.0998 - val_binary_accuracy: 0.6306 - val_f1_score: 0.7366\n",
      "Epoch 17/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.4239 - binary_accuracy: 0.6820 - f1_score: 0.7042 - val_loss: 2.1532 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 18/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 2.7330 - binary_accuracy: 0.6699 - f1_score: 0.6698(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 497ms/step - loss: 2.7330 - binary_accuracy: 0.6699 - f1_score: 0.6698 - val_loss: 2.0614 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7270\n",
      "Epoch 19/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 2.3260 - binary_accuracy: 0.6956 - f1_score: 0.7201(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 498ms/step - loss: 2.3260 - binary_accuracy: 0.6956 - f1_score: 0.7201 - val_loss: 2.0359 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7270\n",
      "Epoch 20/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.7375 - binary_accuracy: 0.6779 - f1_score: 0.6803 - val_loss: 2.0264 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7270\n",
      "Epoch 21/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.5684 - binary_accuracy: 0.6920 - f1_score: 0.7003 - val_loss: 2.3175 - val_binary_accuracy: 0.7027 - val_f1_score: 0.6937\n",
      "Epoch 22/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 2.5411 - binary_accuracy: 0.6845 - f1_score: 0.6941 - val_loss: 1.7789 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7206\n",
      "Epoch 23/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 2.3910 - binary_accuracy: 0.6799 - f1_score: 0.7082 - val_loss: 1.7705 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7206\n",
      "Epoch 24/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.3469 - binary_accuracy: 0.7082 - f1_score: 0.6904 - val_loss: 1.8087 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7206\n",
      "Epoch 25/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.2743 - binary_accuracy: 0.6971 - f1_score: 0.7097 - val_loss: 2.6814 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7115\n",
      "Epoch 26/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.5479 - binary_accuracy: 0.6744 - f1_score: 0.6720 - val_loss: 2.6136 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7297\n",
      "Epoch 27/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.4802 - binary_accuracy: 0.6840 - f1_score: 0.6969 - val_loss: 2.6142 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7115\n",
      "Epoch 28/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.5153 - binary_accuracy: 0.7107 - f1_score: 0.7151 - val_loss: 2.6813 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7018\n",
      "Epoch 29/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.4178 - binary_accuracy: 0.6900 - f1_score: 0.6953 - val_loss: 2.6166 - val_binary_accuracy: 0.7162 - val_f1_score: 0.6931\n",
      "Epoch 30/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.3843 - binary_accuracy: 0.6956 - f1_score: 0.7099 - val_loss: 2.6815 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7023\n",
      "Epoch 31/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.2755 - binary_accuracy: 0.6951 - f1_score: 0.7090 - val_loss: 2.6816 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7206\n",
      "Epoch 32/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.5301 - binary_accuracy: 0.6739 - f1_score: 0.6828 - val_loss: 1.8435 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7297\n",
      "Epoch 33/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 2.3097 - binary_accuracy: 0.6890 - f1_score: 0.7249 - val_loss: 1.7486 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7287\n",
      "Epoch 34/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.2914 - binary_accuracy: 0.6709 - f1_score: 0.7061 - val_loss: 1.8172 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7199\n",
      "Epoch 35/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.3508 - binary_accuracy: 0.6825 - f1_score: 0.7073 - val_loss: 1.8153 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7206\n",
      "Epoch 36/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 2.1640 - binary_accuracy: 0.6830 - f1_score: 0.7242 - val_loss: 1.8114 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7106\n",
      "Epoch 37/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.2838 - binary_accuracy: 0.6724 - f1_score: 0.7171 - val_loss: 1.7451 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7174\n",
      "Epoch 38/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.0452 - binary_accuracy: 0.6804 - f1_score: 0.7295 - val_loss: 1.7487 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7027\n",
      "Epoch 39/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 2.1067 - binary_accuracy: 0.6830 - f1_score: 0.7127 - val_loss: 1.7404 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7098\n",
      "Epoch 40/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.4041 - binary_accuracy: 0.6905 - f1_score: 0.6950 - val_loss: 1.7391 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7270\n",
      "Epoch 41/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 2.3724 - binary_accuracy: 0.6744 - f1_score: 0.6928(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 19s 641ms/step - loss: 2.3724 - binary_accuracy: 0.6744 - f1_score: 0.6928 - val_loss: 1.7420 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7384\n",
      "Epoch 42/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.2392 - binary_accuracy: 0.6784 - f1_score: 0.7155 - val_loss: 1.7416 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7384\n",
      "Epoch 43/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.1687 - binary_accuracy: 0.7011 - f1_score: 0.7265 - val_loss: 1.7438 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 44/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.0881 - binary_accuracy: 0.6764 - f1_score: 0.7198 - val_loss: 1.7411 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7260\n",
      "Epoch 45/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.9428 - binary_accuracy: 0.7056 - f1_score: 0.7517 - val_loss: 1.7405 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7260\n",
      "Epoch 46/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.8437 - binary_accuracy: 0.6996 - f1_score: 0.7472 - val_loss: 1.7433 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7260\n",
      "Epoch 47/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 2.2590 - binary_accuracy: 0.6704 - f1_score: 0.7042 - val_loss: 1.7396 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7260\n",
      "Epoch 48/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8361 - binary_accuracy: 0.7006 - f1_score: 0.7352 - val_loss: 1.7373 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 49/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.1855 - binary_accuracy: 0.6956 - f1_score: 0.7190 - val_loss: 1.8794 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7204\n",
      "Epoch 50/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.0687 - binary_accuracy: 0.6996 - f1_score: 0.7125 - val_loss: 1.9466 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7204\n",
      "Epoch 51/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.2420 - binary_accuracy: 0.6890 - f1_score: 0.7196 - val_loss: 2.6155 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7204\n",
      "Epoch 52/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.3421 - binary_accuracy: 0.6951 - f1_score: 0.7097 - val_loss: 2.0619 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7115\n",
      "Epoch 53/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.1887 - binary_accuracy: 0.7021 - f1_score: 0.7351 - val_loss: 1.8047 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7115\n",
      "Epoch 54/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.1672 - binary_accuracy: 0.6804 - f1_score: 0.7187 - val_loss: 1.7628 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7026\n",
      "Epoch 55/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.1036 - binary_accuracy: 0.6900 - f1_score: 0.7132 - val_loss: 1.7298 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7287\n",
      "Epoch 56/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.9439 - binary_accuracy: 0.6956 - f1_score: 0.7403 - val_loss: 1.7304 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7287\n",
      "Epoch 57/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.0586 - binary_accuracy: 0.6809 - f1_score: 0.7139 - val_loss: 1.7303 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7279\n",
      "Epoch 58/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.9879 - binary_accuracy: 0.6794 - f1_score: 0.7130 - val_loss: 1.6628 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7279\n",
      "Epoch 59/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.1322 - binary_accuracy: 0.6835 - f1_score: 0.7278 - val_loss: 1.6712 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7115\n",
      "Epoch 60/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.9820 - binary_accuracy: 0.6910 - f1_score: 0.7341 - val_loss: 1.6665 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7374\n",
      "Epoch 61/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.1187 - binary_accuracy: 0.6865 - f1_score: 0.7303 - val_loss: 1.6644 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7374\n",
      "Epoch 62/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.1167 - binary_accuracy: 0.6845 - f1_score: 0.7127 - val_loss: 1.6587 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7270\n",
      "Epoch 63/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.0570 - binary_accuracy: 0.6890 - f1_score: 0.7278 - val_loss: 1.6597 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7270\n",
      "Epoch 64/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.9255 - binary_accuracy: 0.6794 - f1_score: 0.7513 - val_loss: 1.6638 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7270\n",
      "Epoch 65/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 2.0799 - binary_accuracy: 0.6739 - f1_score: 0.7170 - val_loss: 1.6638 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7270\n",
      "Epoch 66/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8904 - binary_accuracy: 0.6734 - f1_score: 0.7089 - val_loss: 1.6637 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7270\n",
      "Epoch 67/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7954 - binary_accuracy: 0.7031 - f1_score: 0.7572 - val_loss: 1.6677 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 68/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.8213 - binary_accuracy: 0.7016 - f1_score: 0.7501 - val_loss: 1.6654 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7366\n",
      "Epoch 69/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.8567 - binary_accuracy: 0.6734 - f1_score: 0.7274 - val_loss: 1.6683 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7270\n",
      "Epoch 70/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.9451 - binary_accuracy: 0.6860 - f1_score: 0.7185 - val_loss: 1.6661 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 71/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.9657 - binary_accuracy: 0.6754 - f1_score: 0.7292 - val_loss: 1.6663 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 72/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8768 - binary_accuracy: 0.6809 - f1_score: 0.7415 - val_loss: 1.6681 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 73/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.0004 - binary_accuracy: 0.6784 - f1_score: 0.7289 - val_loss: 1.6697 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 74/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8247 - binary_accuracy: 0.7036 - f1_score: 0.7385 - val_loss: 1.7394 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 75/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.9386 - binary_accuracy: 0.6734 - f1_score: 0.7196 - val_loss: 1.6946 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7270\n",
      "Epoch 76/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.8883 - binary_accuracy: 0.6603 - f1_score: 0.7282 - val_loss: 1.6957 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7270\n",
      "Epoch 77/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.9810 - binary_accuracy: 0.6593 - f1_score: 0.7151 - val_loss: 1.6931 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 78/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.0013 - binary_accuracy: 0.6739 - f1_score: 0.7417 - val_loss: 1.6878 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 79/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7953 - binary_accuracy: 0.6633 - f1_score: 0.7281 - val_loss: 1.6865 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 80/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.9809 - binary_accuracy: 0.6588 - f1_score: 0.7003 - val_loss: 1.6804 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7366\n",
      "Epoch 81/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.9068 - binary_accuracy: 0.6719 - f1_score: 0.7224 - val_loss: 1.7484 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7279\n",
      "Epoch 82/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7311 - binary_accuracy: 0.6699 - f1_score: 0.7248 - val_loss: 1.7477 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7279\n",
      "Epoch 83/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.9272 - binary_accuracy: 0.6714 - f1_score: 0.7321 - val_loss: 1.7471 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7279\n",
      "Epoch 84/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7763 - binary_accuracy: 0.6870 - f1_score: 0.7372 - val_loss: 1.7416 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7279\n",
      "Epoch 85/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.8443 - binary_accuracy: 0.6825 - f1_score: 0.7395 - val_loss: 1.7424 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7279\n",
      "Epoch 86/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8603 - binary_accuracy: 0.6830 - f1_score: 0.7160 - val_loss: 1.7416 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7279\n",
      "Epoch 87/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8996 - binary_accuracy: 0.6845 - f1_score: 0.7377 - val_loss: 1.7449 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7279\n",
      "Epoch 88/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8564 - binary_accuracy: 0.6870 - f1_score: 0.7166 - val_loss: 1.7389 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 89/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7127 - binary_accuracy: 0.7036 - f1_score: 0.7571 - val_loss: 1.6707 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7366\n",
      "Epoch 90/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7978 - binary_accuracy: 0.6668 - f1_score: 0.7279 - val_loss: 1.7111 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7270\n",
      "Epoch 91/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.9659 - binary_accuracy: 0.6467 - f1_score: 0.6938 - val_loss: 1.7263 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7270\n",
      "Epoch 92/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.8557 - binary_accuracy: 0.6477 - f1_score: 0.7295 - val_loss: 1.7305 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7270\n",
      "Epoch 93/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6975 - binary_accuracy: 0.6527 - f1_score: 0.7509 - val_loss: 1.7334 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7270\n",
      "Epoch 94/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7688 - binary_accuracy: 0.6638 - f1_score: 0.7486 - val_loss: 1.7319 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7270\n",
      "Epoch 95/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8893 - binary_accuracy: 0.6482 - f1_score: 0.7129 - val_loss: 1.7332 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7270\n",
      "Epoch 96/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6965 - binary_accuracy: 0.6714 - f1_score: 0.7499 - val_loss: 1.7288 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 97/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.8251 - binary_accuracy: 0.6431 - f1_score: 0.7160 - val_loss: 1.7246 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 98/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.8924 - binary_accuracy: 0.6431 - f1_score: 0.7226 - val_loss: 1.7922 - val_binary_accuracy: 0.6216 - val_f1_score: 0.7279\n",
      "Epoch 99/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.0192 - binary_accuracy: 0.6497 - f1_score: 0.7230 - val_loss: 1.7227 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 100/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.9068 - binary_accuracy: 0.6396 - f1_score: 0.7079 - val_loss: 1.7261 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 101/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6556 - binary_accuracy: 0.6668 - f1_score: 0.7591 - val_loss: 1.7258 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 102/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7050 - binary_accuracy: 0.6714 - f1_score: 0.7440 - val_loss: 1.7202 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 103/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8927 - binary_accuracy: 0.6472 - f1_score: 0.7279 - val_loss: 1.7245 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 104/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8668 - binary_accuracy: 0.6618 - f1_score: 0.7152 - val_loss: 1.7207 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 105/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7426 - binary_accuracy: 0.6573 - f1_score: 0.7260 - val_loss: 1.7231 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 106/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7924 - binary_accuracy: 0.6512 - f1_score: 0.7315 - val_loss: 1.7236 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 107/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.9179 - binary_accuracy: 0.6512 - f1_score: 0.7299 - val_loss: 1.7222 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 108/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8715 - binary_accuracy: 0.6583 - f1_score: 0.7000 - val_loss: 1.7240 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 109/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.8688 - binary_accuracy: 0.6568 - f1_score: 0.7258 - val_loss: 1.7206 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 110/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7429 - binary_accuracy: 0.6517 - f1_score: 0.7266 - val_loss: 1.7204 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 111/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.9023 - binary_accuracy: 0.6613 - f1_score: 0.7199 - val_loss: 1.7134 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 112/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8039 - binary_accuracy: 0.6603 - f1_score: 0.7223 - val_loss: 1.7137 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 113/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8931 - binary_accuracy: 0.6527 - f1_score: 0.7184 - val_loss: 1.7121 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 114/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.9966 - binary_accuracy: 0.6426 - f1_score: 0.7001 - val_loss: 1.7804 - val_binary_accuracy: 0.6216 - val_f1_score: 0.7279\n",
      "Epoch 115/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7641 - binary_accuracy: 0.6527 - f1_score: 0.7264 - val_loss: 1.7791 - val_binary_accuracy: 0.6216 - val_f1_score: 0.7279\n",
      "Epoch 116/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.0224 - binary_accuracy: 0.6467 - f1_score: 0.7014 - val_loss: 1.7801 - val_binary_accuracy: 0.6216 - val_f1_score: 0.7279\n",
      "Epoch 117/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8929 - binary_accuracy: 0.6467 - f1_score: 0.7262 - val_loss: 1.7106 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 118/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8484 - binary_accuracy: 0.6477 - f1_score: 0.7213 - val_loss: 1.7154 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 119/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8536 - binary_accuracy: 0.6482 - f1_score: 0.7043 - val_loss: 1.7134 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 120/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8029 - binary_accuracy: 0.6482 - f1_score: 0.7290 - val_loss: 1.7156 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 121/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8508 - binary_accuracy: 0.6598 - f1_score: 0.7127 - val_loss: 1.7115 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 122/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.9227 - binary_accuracy: 0.6462 - f1_score: 0.7318 - val_loss: 1.7154 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 123/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7751 - binary_accuracy: 0.6532 - f1_score: 0.7387 - val_loss: 1.7150 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 124/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.8469 - binary_accuracy: 0.6608 - f1_score: 0.7376 - val_loss: 1.7144 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 125/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7973 - binary_accuracy: 0.6562 - f1_score: 0.7266 - val_loss: 1.7146 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 126/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.8480 - binary_accuracy: 0.6557 - f1_score: 0.7041 - val_loss: 1.7093 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 127/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.9300 - binary_accuracy: 0.6517 - f1_score: 0.6976 - val_loss: 1.7093 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 128/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.8093 - binary_accuracy: 0.6552 - f1_score: 0.7160 - val_loss: 1.7114 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 129/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8827 - binary_accuracy: 0.6668 - f1_score: 0.7124 - val_loss: 1.7082 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 130/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8167 - binary_accuracy: 0.6421 - f1_score: 0.7017 - val_loss: 1.7095 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7270\n",
      "Epoch 131/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.9002 - binary_accuracy: 0.6578 - f1_score: 0.6948 - val_loss: 1.7083 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7270\n",
      "Epoch 132/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8720 - binary_accuracy: 0.6593 - f1_score: 0.7194 - val_loss: 1.7749 - val_binary_accuracy: 0.6216 - val_f1_score: 0.7279\n",
      "Epoch 133/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8919 - binary_accuracy: 0.6512 - f1_score: 0.7153 - val_loss: 1.7114 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 134/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.6666 - binary_accuracy: 0.6583 - f1_score: 0.7127 - val_loss: 1.7134 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7270\n",
      "Epoch 135/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7959 - binary_accuracy: 0.6552 - f1_score: 0.7200 - val_loss: 1.7136 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7270\n",
      "Epoch 136/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7227 - binary_accuracy: 0.6467 - f1_score: 0.7152 - val_loss: 1.7132 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7270\n",
      "Epoch 137/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8141 - binary_accuracy: 0.6502 - f1_score: 0.7191 - val_loss: 1.7140 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7270\n",
      "Epoch 138/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.8620 - binary_accuracy: 0.6699 - f1_score: 0.7157 - val_loss: 1.7067 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7270\n",
      "Epoch 139/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6074 - binary_accuracy: 0.6658 - f1_score: 0.7354 - val_loss: 1.7059 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7270\n",
      "Epoch 140/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7549 - binary_accuracy: 0.6734 - f1_score: 0.7274 - val_loss: 1.7705 - val_binary_accuracy: 0.6216 - val_f1_score: 0.7279\n",
      "Epoch 141/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.9163 - binary_accuracy: 0.6552 - f1_score: 0.7148 - val_loss: 1.7027 - val_binary_accuracy: 0.6261 - val_f1_score: 0.7366\n",
      "Epoch 142/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6998 - binary_accuracy: 0.6754 - f1_score: 0.7405 - val_loss: 1.7705 - val_binary_accuracy: 0.6216 - val_f1_score: 0.7184\n",
      "Epoch 143/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8157 - binary_accuracy: 0.6537 - f1_score: 0.7247 - val_loss: 1.7705 - val_binary_accuracy: 0.6216 - val_f1_score: 0.7279\n",
      "Epoch 144/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.8235 - binary_accuracy: 0.6487 - f1_score: 0.7184 - val_loss: 1.7715 - val_binary_accuracy: 0.6216 - val_f1_score: 0.7279\n",
      "Epoch 145/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8497 - binary_accuracy: 0.6633 - f1_score: 0.7273 - val_loss: 1.7663 - val_binary_accuracy: 0.6216 - val_f1_score: 0.7279\n",
      "Epoch 146/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7725 - binary_accuracy: 0.6477 - f1_score: 0.7021 - val_loss: 1.7661 - val_binary_accuracy: 0.6216 - val_f1_score: 0.7279\n",
      "Epoch 147/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8006 - binary_accuracy: 0.6653 - f1_score: 0.7268 - val_loss: 1.7772 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 148/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7898 - binary_accuracy: 0.6472 - f1_score: 0.7253 - val_loss: 1.7797 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 149/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8583 - binary_accuracy: 0.6759 - f1_score: 0.7130 - val_loss: 1.7757 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 150/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7537 - binary_accuracy: 0.6598 - f1_score: 0.7274 - val_loss: 1.7784 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 151/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8504 - binary_accuracy: 0.6623 - f1_score: 0.6992 - val_loss: 1.7785 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 152/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7553 - binary_accuracy: 0.6754 - f1_score: 0.7254 - val_loss: 1.7783 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 153/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8509 - binary_accuracy: 0.6809 - f1_score: 0.7316 - val_loss: 1.8330 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 154/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6778 - binary_accuracy: 0.6457 - f1_score: 0.7204 - val_loss: 1.7827 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7098\n",
      "Epoch 155/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6257 - binary_accuracy: 0.6673 - f1_score: 0.7404 - val_loss: 1.8381 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 156/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 1.8737 - binary_accuracy: 0.6552 - f1_score: 0.6997 - val_loss: 1.8315 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7193\n",
      "Epoch 157/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7802 - binary_accuracy: 0.6804 - f1_score: 0.7161 - val_loss: 1.7743 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7193\n",
      "Epoch 158/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 1.8130 - binary_accuracy: 0.6568 - f1_score: 0.7189 - val_loss: 1.7715 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7193\n",
      "Epoch 159/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 1.7440 - binary_accuracy: 0.6774 - f1_score: 0.7278 - val_loss: 1.7729 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7193\n",
      "Epoch 160/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 1.7209 - binary_accuracy: 0.6759 - f1_score: 0.7426 - val_loss: 1.7721 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7193\n",
      "Epoch 161/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6772 - binary_accuracy: 0.6694 - f1_score: 0.7301 - val_loss: 1.7735 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7193\n",
      "Epoch 162/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.0918 - binary_accuracy: 0.6457 - f1_score: 0.6781 - val_loss: 1.7569 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7279\n",
      "Epoch 163/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8655 - binary_accuracy: 0.6678 - f1_score: 0.7183 - val_loss: 1.7715 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7193\n",
      "Epoch 164/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8582 - binary_accuracy: 0.6613 - f1_score: 0.6975 - val_loss: 1.7800 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7193\n",
      "Epoch 165/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7073 - binary_accuracy: 0.6759 - f1_score: 0.7445 - val_loss: 1.7778 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7193\n",
      "Epoch 166/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7563 - binary_accuracy: 0.6739 - f1_score: 0.7082 - val_loss: 1.7772 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7193\n",
      "Epoch 167/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7098 - binary_accuracy: 0.6643 - f1_score: 0.7221 - val_loss: 1.7687 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7279\n",
      "Epoch 168/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8098 - binary_accuracy: 0.6709 - f1_score: 0.7269 - val_loss: 1.7632 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7279\n",
      "Epoch 169/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.8480 - binary_accuracy: 0.6815 - f1_score: 0.7085 - val_loss: 1.7765 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7193\n",
      "Epoch 170/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.9265 - binary_accuracy: 0.6562 - f1_score: 0.7111 - val_loss: 1.7788 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7193\n",
      "Epoch 171/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.9506 - binary_accuracy: 0.6618 - f1_score: 0.7165 - val_loss: 1.7732 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7193\n",
      "Epoch 172/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.9000 - binary_accuracy: 0.6547 - f1_score: 0.7187 - val_loss: 1.7762 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7193\n",
      "Epoch 173/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6774 - binary_accuracy: 0.6562 - f1_score: 0.7196 - val_loss: 1.7862 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7193\n",
      "Epoch 174/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7129 - binary_accuracy: 0.6724 - f1_score: 0.7251 - val_loss: 1.7618 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7279\n",
      "Epoch 175/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.8251 - binary_accuracy: 0.6527 - f1_score: 0.7168 - val_loss: 1.8290 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7193\n",
      "Epoch 176/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7526 - binary_accuracy: 0.6658 - f1_score: 0.7022 - val_loss: 1.7768 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7193\n",
      "Epoch 177/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7045 - binary_accuracy: 0.6542 - f1_score: 0.7301 - val_loss: 1.7769 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7193\n",
      "Epoch 178/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7160 - binary_accuracy: 0.6749 - f1_score: 0.7363 - val_loss: 1.8303 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7193\n",
      "Epoch 179/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6690 - binary_accuracy: 0.6694 - f1_score: 0.7342 - val_loss: 1.7823 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 180/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.8121 - binary_accuracy: 0.6673 - f1_score: 0.7277 - val_loss: 1.7772 - val_binary_accuracy: 0.6216 - val_f1_score: 0.7279\n",
      "Epoch 181/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8355 - binary_accuracy: 0.6462 - f1_score: 0.7155 - val_loss: 1.8457 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 182/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6700 - binary_accuracy: 0.6673 - f1_score: 0.7066 - val_loss: 1.7933 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 183/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.8459 - binary_accuracy: 0.6522 - f1_score: 0.7316 - val_loss: 1.7912 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 184/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.8388 - binary_accuracy: 0.6442 - f1_score: 0.7086 - val_loss: 1.7870 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 185/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6703 - binary_accuracy: 0.6678 - f1_score: 0.7536 - val_loss: 1.7859 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 186/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.8291 - binary_accuracy: 0.6492 - f1_score: 0.6757 - val_loss: 1.7845 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 187/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6714 - binary_accuracy: 0.6507 - f1_score: 0.7174 - val_loss: 1.7857 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 188/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8302 - binary_accuracy: 0.6527 - f1_score: 0.7098 - val_loss: 1.7851 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 189/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.8322 - binary_accuracy: 0.6447 - f1_score: 0.7152 - val_loss: 1.7861 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 190/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.8087 - binary_accuracy: 0.6623 - f1_score: 0.7175 - val_loss: 1.7856 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 191/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7653 - binary_accuracy: 0.6628 - f1_score: 0.7277 - val_loss: 1.7897 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 192/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7247 - binary_accuracy: 0.6673 - f1_score: 0.7235 - val_loss: 1.8378 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 193/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.8840 - binary_accuracy: 0.6467 - f1_score: 0.6989 - val_loss: 1.7925 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 194/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7272 - binary_accuracy: 0.6628 - f1_score: 0.7326 - val_loss: 1.7796 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 195/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6769 - binary_accuracy: 0.6517 - f1_score: 0.7234 - val_loss: 1.7785 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7193\n",
      "Epoch 196/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 2.0021 - binary_accuracy: 0.6457 - f1_score: 0.7008 - val_loss: 1.8518 - val_binary_accuracy: 0.6126 - val_f1_score: 0.7193\n",
      "Epoch 197/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7450 - binary_accuracy: 0.6623 - f1_score: 0.7413 - val_loss: 1.8508 - val_binary_accuracy: 0.6126 - val_f1_score: 0.7193\n",
      "Epoch 198/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6715 - binary_accuracy: 0.6573 - f1_score: 0.7092 - val_loss: 1.7859 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7279\n",
      "Epoch 199/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6442 - binary_accuracy: 0.6492 - f1_score: 0.7213 - val_loss: 1.7899 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7279\n",
      "Epoch 200/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.8957 - binary_accuracy: 0.6447 - f1_score: 0.7103 - val_loss: 1.7865 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7279\n",
      "Epoch 201/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 2.0052 - binary_accuracy: 0.6411 - f1_score: 0.7049 - val_loss: 1.7896 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7279\n",
      "Epoch 202/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7803 - binary_accuracy: 0.6593 - f1_score: 0.7251 - val_loss: 1.7816 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7279\n",
      "Epoch 203/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7887 - binary_accuracy: 0.6668 - f1_score: 0.7215 - val_loss: 1.8665 - val_binary_accuracy: 0.6982 - val_f1_score: 0.7193\n",
      "Epoch 204/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7496 - binary_accuracy: 0.6578 - f1_score: 0.7138 - val_loss: 1.8431 - val_binary_accuracy: 0.6982 - val_f1_score: 0.7106\n",
      "Epoch 205/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.8004 - binary_accuracy: 0.6749 - f1_score: 0.7136 - val_loss: 1.8418 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7106\n",
      "Epoch 206/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6762 - binary_accuracy: 0.6704 - f1_score: 0.7111 - val_loss: 1.7877 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7193\n",
      "Epoch 207/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5130 - binary_accuracy: 0.6744 - f1_score: 0.7599 - val_loss: 1.7685 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7279\n",
      "Epoch 208/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.9465 - binary_accuracy: 0.6452 - f1_score: 0.7043 - val_loss: 1.8465 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7106\n",
      "Epoch 209/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.8119 - binary_accuracy: 0.6547 - f1_score: 0.7197 - val_loss: 1.8649 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7193\n",
      "Epoch 210/500\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 1.9295 - binary_accuracy: 0.6603 - f1_score: 0.6991 - val_loss: 1.7834 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7193\n",
      "Epoch 211/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6967 - binary_accuracy: 0.6658 - f1_score: 0.7250 - val_loss: 1.7783 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7193\n",
      "Epoch 212/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5971 - binary_accuracy: 0.6628 - f1_score: 0.7178 - val_loss: 1.7830 - val_binary_accuracy: 0.6982 - val_f1_score: 0.7106\n",
      "Epoch 213/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6419 - binary_accuracy: 0.6724 - f1_score: 0.7344 - val_loss: 1.8461 - val_binary_accuracy: 0.6982 - val_f1_score: 0.7106\n",
      "Epoch 214/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7334 - binary_accuracy: 0.6774 - f1_score: 0.7279 - val_loss: 1.7951 - val_binary_accuracy: 0.6982 - val_f1_score: 0.7279\n",
      "Epoch 215/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7678 - binary_accuracy: 0.6709 - f1_score: 0.7152 - val_loss: 1.7738 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7279\n",
      "Epoch 216/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8231 - binary_accuracy: 0.6683 - f1_score: 0.7193 - val_loss: 1.8526 - val_binary_accuracy: 0.6982 - val_f1_score: 0.7106\n",
      "Epoch 217/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7483 - binary_accuracy: 0.6699 - f1_score: 0.7141 - val_loss: 1.7703 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7193\n",
      "Epoch 218/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7657 - binary_accuracy: 0.6603 - f1_score: 0.7028 - val_loss: 1.8304 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7193\n",
      "Epoch 219/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6895 - binary_accuracy: 0.6668 - f1_score: 0.7357 - val_loss: 1.8268 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7193\n",
      "Epoch 220/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7153 - binary_accuracy: 0.6689 - f1_score: 0.7457 - val_loss: 1.7665 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7279\n",
      "Epoch 221/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.8789 - binary_accuracy: 0.6704 - f1_score: 0.7155 - val_loss: 1.7706 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7279\n",
      "Epoch 222/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5760 - binary_accuracy: 0.6613 - f1_score: 0.7290 - val_loss: 1.7604 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7279\n",
      "Epoch 223/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7225 - binary_accuracy: 0.6704 - f1_score: 0.7131 - val_loss: 1.7570 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7279\n",
      "Epoch 224/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6395 - binary_accuracy: 0.6799 - f1_score: 0.7382 - val_loss: 1.7518 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 225/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7616 - binary_accuracy: 0.6709 - f1_score: 0.7267 - val_loss: 1.6798 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7287\n",
      "Epoch 226/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7164 - binary_accuracy: 0.6794 - f1_score: 0.7305 - val_loss: 1.7493 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 227/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.7293 - binary_accuracy: 0.6653 - f1_score: 0.7255 - val_loss: 1.7563 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7279\n",
      "Epoch 228/500\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 1.6567 - binary_accuracy: 0.6729 - f1_score: 0.7499 - val_loss: 1.7001 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7287\n",
      "Epoch 229/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.8021 - binary_accuracy: 0.6784 - f1_score: 0.7175 - val_loss: 1.6855 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7287\n",
      "Epoch 230/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.7684 - binary_accuracy: 0.6694 - f1_score: 0.7251 - val_loss: 1.7570 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7193\n",
      "Epoch 231/500\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 1.6649 - binary_accuracy: 0.6855 - f1_score: 0.7468 - val_loss: 1.6828 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7374\n",
      "Epoch 232/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.7726 - binary_accuracy: 0.6648 - f1_score: 0.7015 - val_loss: 1.7499 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7279\n",
      "Epoch 233/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.8547 - binary_accuracy: 0.6719 - f1_score: 0.7043 - val_loss: 1.6793 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7287\n",
      "Epoch 234/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7678 - binary_accuracy: 0.6502 - f1_score: 0.7382 - val_loss: 1.7625 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7193\n",
      "Epoch 235/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.7455 - binary_accuracy: 0.6663 - f1_score: 0.7217 - val_loss: 1.7665 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7193\n",
      "Epoch 236/500\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 1.6946 - binary_accuracy: 0.6699 - f1_score: 0.7208 - val_loss: 1.7520 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7287\n",
      "Epoch 237/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6445 - binary_accuracy: 0.6734 - f1_score: 0.7362 - val_loss: 1.7585 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7193\n",
      "Epoch 238/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6563 - binary_accuracy: 0.6658 - f1_score: 0.7349 - val_loss: 1.8239 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7193\n",
      "Epoch 239/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6930 - binary_accuracy: 0.6769 - f1_score: 0.7258 - val_loss: 1.8205 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7193\n",
      "Epoch 240/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7371 - binary_accuracy: 0.6593 - f1_score: 0.7244 - val_loss: 1.8212 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7098\n",
      "Epoch 241/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7330 - binary_accuracy: 0.6487 - f1_score: 0.6940 - val_loss: 1.7527 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7193\n",
      "Epoch 242/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7541 - binary_accuracy: 0.6804 - f1_score: 0.7271 - val_loss: 1.7498 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7287\n",
      "Epoch 243/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.5851 - binary_accuracy: 0.6774 - f1_score: 0.7259 - val_loss: 1.7507 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7287\n",
      "Epoch 244/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.6168 - binary_accuracy: 0.6900 - f1_score: 0.7467 - val_loss: 1.7504 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7287\n",
      "Epoch 245/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7176 - binary_accuracy: 0.6825 - f1_score: 0.7272 - val_loss: 1.6862 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7287\n",
      "Epoch 246/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7976 - binary_accuracy: 0.6618 - f1_score: 0.7184 - val_loss: 1.8189 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7193\n",
      "Epoch 247/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7035 - binary_accuracy: 0.6709 - f1_score: 0.7247 - val_loss: 1.6873 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7287\n",
      "Epoch 248/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.8151 - binary_accuracy: 0.6648 - f1_score: 0.7048 - val_loss: 1.6837 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7374\n",
      "Epoch 249/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6657 - binary_accuracy: 0.6714 - f1_score: 0.7251 - val_loss: 1.6798 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7374\n",
      "Epoch 250/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.8853 - binary_accuracy: 0.6704 - f1_score: 0.7075 - val_loss: 1.6771 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7374\n",
      "Epoch 251/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.8333 - binary_accuracy: 0.6653 - f1_score: 0.7074 - val_loss: 1.6748 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7374\n",
      "Epoch 252/500\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 1.8309 - binary_accuracy: 0.6734 - f1_score: 0.7082 - val_loss: 1.6912 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7374\n",
      "Epoch 253/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.6873 - binary_accuracy: 0.6749 - f1_score: 0.7187 - val_loss: 1.6757 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7287\n",
      "Epoch 254/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.8898 - binary_accuracy: 0.6689 - f1_score: 0.7172 - val_loss: 1.6739 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7374\n",
      "Epoch 255/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.8363 - binary_accuracy: 0.6724 - f1_score: 0.7106 - val_loss: 1.6889 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7287\n",
      "Epoch 256/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6767 - binary_accuracy: 0.6830 - f1_score: 0.7225 - val_loss: 1.6777 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7287\n",
      "Epoch 257/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7604 - binary_accuracy: 0.6704 - f1_score: 0.7092 - val_loss: 1.6861 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7287\n",
      "Epoch 258/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7326 - binary_accuracy: 0.6663 - f1_score: 0.7303 - val_loss: 1.6800 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7374\n",
      "Epoch 259/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6720 - binary_accuracy: 0.6910 - f1_score: 0.7270 - val_loss: 1.6796 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7374\n",
      "Epoch 260/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.6836 - binary_accuracy: 0.6900 - f1_score: 0.7296 - val_loss: 1.6974 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7374\n",
      "Epoch 261/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7968 - binary_accuracy: 0.6744 - f1_score: 0.7163 - val_loss: 1.6759 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7287\n",
      "Epoch 262/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7823 - binary_accuracy: 0.6900 - f1_score: 0.7177 - val_loss: 1.6662 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7374\n",
      "Epoch 263/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.8631 - binary_accuracy: 0.6704 - f1_score: 0.7130 - val_loss: 1.6693 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7374\n",
      "Epoch 264/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.8461 - binary_accuracy: 0.6724 - f1_score: 0.6962 - val_loss: 1.6670 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7287\n",
      "Epoch 265/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7423 - binary_accuracy: 0.6694 - f1_score: 0.7195 - val_loss: 1.6656 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7287\n",
      "Epoch 266/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6758 - binary_accuracy: 0.6860 - f1_score: 0.7342 - val_loss: 1.7007 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7287\n",
      "Epoch 267/500\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 1.6308 - binary_accuracy: 0.6794 - f1_score: 0.7308 - val_loss: 1.6956 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7287\n",
      "Epoch 268/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.8508 - binary_accuracy: 0.6573 - f1_score: 0.6885 - val_loss: 1.6956 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7287\n",
      "Epoch 269/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.4805 - binary_accuracy: 0.6865 - f1_score: 0.7501 - val_loss: 1.6998 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7287\n",
      "Epoch 270/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7397 - binary_accuracy: 0.6542 - f1_score: 0.7049 - val_loss: 1.7577 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7193\n",
      "Epoch 271/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6610 - binary_accuracy: 0.6603 - f1_score: 0.7054 - val_loss: 1.7571 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7193\n",
      "Epoch 272/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7682 - binary_accuracy: 0.6714 - f1_score: 0.7035 - val_loss: 1.6899 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 273/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7285 - binary_accuracy: 0.6557 - f1_score: 0.7089 - val_loss: 1.6480 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7279\n",
      "Epoch 274/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7322 - binary_accuracy: 0.6633 - f1_score: 0.7063 - val_loss: 1.6354 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7279\n",
      "Epoch 275/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7342 - binary_accuracy: 0.6653 - f1_score: 0.7048 - val_loss: 1.6457 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7279\n",
      "Epoch 276/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.7280 - binary_accuracy: 0.6734 - f1_score: 0.7224 - val_loss: 1.6956 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7193\n",
      "Epoch 277/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7126 - binary_accuracy: 0.6714 - f1_score: 0.7195 - val_loss: 1.6467 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7193\n",
      "Epoch 278/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6605 - binary_accuracy: 0.6699 - f1_score: 0.7157 - val_loss: 1.6955 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7193\n",
      "Epoch 279/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6880 - binary_accuracy: 0.6658 - f1_score: 0.7017 - val_loss: 1.7028 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7193\n",
      "Epoch 280/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5147 - binary_accuracy: 0.6815 - f1_score: 0.7473 - val_loss: 1.6388 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7193\n",
      "Epoch 281/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6051 - binary_accuracy: 0.6714 - f1_score: 0.7356 - val_loss: 1.6379 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7374\n",
      "Epoch 282/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5955 - binary_accuracy: 0.6653 - f1_score: 0.7231 - val_loss: 1.6515 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7374\n",
      "Epoch 283/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.9276 - binary_accuracy: 0.6573 - f1_score: 0.6968 - val_loss: 1.6400 - val_binary_accuracy: 0.7342 - val_f1_score: 0.7374\n",
      "Epoch 284/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.4729 - binary_accuracy: 0.6825 - f1_score: 0.7340 - val_loss: 1.6497 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7193\n",
      "Epoch 285/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6605 - binary_accuracy: 0.6774 - f1_score: 0.7103 - val_loss: 1.6515 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7287\n",
      "Epoch 286/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7154 - binary_accuracy: 0.6633 - f1_score: 0.7018 - val_loss: 1.6434 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7193\n",
      "Epoch 287/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.4751 - binary_accuracy: 0.6804 - f1_score: 0.7287 - val_loss: 1.6293 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7193\n",
      "Epoch 288/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6536 - binary_accuracy: 0.6678 - f1_score: 0.7147 - val_loss: 1.6904 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7193\n",
      "Epoch 289/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.6638 - binary_accuracy: 0.6573 - f1_score: 0.6991 - val_loss: 1.6367 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7193\n",
      "Epoch 290/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5935 - binary_accuracy: 0.6699 - f1_score: 0.7007 - val_loss: 1.6991 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7193\n",
      "Epoch 291/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6998 - binary_accuracy: 0.6885 - f1_score: 0.7186 - val_loss: 1.6181 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7287\n",
      "Epoch 292/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7659 - binary_accuracy: 0.6865 - f1_score: 0.7251 - val_loss: 1.6106 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7287\n",
      "Epoch 293/500\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 1.6223 - binary_accuracy: 0.6658 - f1_score: 0.7194 - val_loss: 1.6124 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7287\n",
      "Epoch 294/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6522 - binary_accuracy: 0.6850 - f1_score: 0.7318 - val_loss: 1.6073 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7374\n",
      "Epoch 295/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6389 - binary_accuracy: 0.6683 - f1_score: 0.7062 - val_loss: 1.6296 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7287\n",
      "Epoch 296/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.7257 - binary_accuracy: 0.6865 - f1_score: 0.7096 - val_loss: 1.6206 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7287\n",
      "Epoch 297/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5839 - binary_accuracy: 0.6779 - f1_score: 0.7294 - val_loss: 1.6465 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7374\n",
      "Epoch 298/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7496 - binary_accuracy: 0.6754 - f1_score: 0.7233 - val_loss: 1.6424 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7287\n",
      "Epoch 299/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5546 - binary_accuracy: 0.6855 - f1_score: 0.7234 - val_loss: 1.6138 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7199\n",
      "Epoch 300/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6190 - binary_accuracy: 0.6694 - f1_score: 0.6945 - val_loss: 1.6241 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7193\n",
      "Epoch 301/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6485 - binary_accuracy: 0.6845 - f1_score: 0.7192 - val_loss: 1.6306 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7199\n",
      "Epoch 302/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6996 - binary_accuracy: 0.6683 - f1_score: 0.7109 - val_loss: 1.6368 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7106\n",
      "Epoch 303/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7041 - binary_accuracy: 0.6678 - f1_score: 0.7105 - val_loss: 1.6379 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7106\n",
      "Epoch 304/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6519 - binary_accuracy: 0.6512 - f1_score: 0.7010 - val_loss: 1.7046 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7106\n",
      "Epoch 305/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7511 - binary_accuracy: 0.6573 - f1_score: 0.7105 - val_loss: 1.7002 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7193\n",
      "Epoch 306/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7530 - binary_accuracy: 0.6704 - f1_score: 0.7167 - val_loss: 1.6303 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7193\n",
      "Epoch 307/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6673 - binary_accuracy: 0.6689 - f1_score: 0.7041 - val_loss: 1.6957 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7193\n",
      "Epoch 308/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7687 - binary_accuracy: 0.6613 - f1_score: 0.6921 - val_loss: 1.6788 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 309/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6915 - binary_accuracy: 0.6583 - f1_score: 0.6961 - val_loss: 1.6095 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 310/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.5592 - binary_accuracy: 0.6759 - f1_score: 0.7301 - val_loss: 1.6129 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7366\n",
      "Epoch 311/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.6615 - binary_accuracy: 0.6744 - f1_score: 0.7056 - val_loss: 1.6100 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 312/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.6895 - binary_accuracy: 0.6593 - f1_score: 0.6913 - val_loss: 1.6122 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7366\n",
      "Epoch 313/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6442 - binary_accuracy: 0.6774 - f1_score: 0.7174 - val_loss: 1.6097 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 314/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4752 - binary_accuracy: 0.6850 - f1_score: 0.7393 - val_loss: 1.6071 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 315/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6576 - binary_accuracy: 0.6744 - f1_score: 0.7270 - val_loss: 1.6062 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7366\n",
      "Epoch 316/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6369 - binary_accuracy: 0.6815 - f1_score: 0.7139 - val_loss: 1.6051 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7366\n",
      "Epoch 317/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5351 - binary_accuracy: 0.6840 - f1_score: 0.7234 - val_loss: 1.6063 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7279\n",
      "Epoch 318/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.7629 - binary_accuracy: 0.6683 - f1_score: 0.6969 - val_loss: 1.6685 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7279\n",
      "Epoch 319/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7236 - binary_accuracy: 0.6845 - f1_score: 0.6924 - val_loss: 1.6681 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7279\n",
      "Epoch 320/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.4447 - binary_accuracy: 0.6880 - f1_score: 0.7223 - val_loss: 1.6132 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7279\n",
      "Epoch 321/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5604 - binary_accuracy: 0.6653 - f1_score: 0.7225 - val_loss: 1.6754 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7279\n",
      "Epoch 322/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5535 - binary_accuracy: 0.6719 - f1_score: 0.7101 - val_loss: 1.7074 - val_binary_accuracy: 0.6126 - val_f1_score: 0.7279\n",
      "Epoch 323/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6039 - binary_accuracy: 0.6527 - f1_score: 0.7032 - val_loss: 1.6411 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7366\n",
      "Epoch 324/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5372 - binary_accuracy: 0.6739 - f1_score: 0.7213 - val_loss: 1.6566 - val_binary_accuracy: 0.6216 - val_f1_score: 0.7366\n",
      "Epoch 325/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5902 - binary_accuracy: 0.6699 - f1_score: 0.6962 - val_loss: 1.6615 - val_binary_accuracy: 0.6216 - val_f1_score: 0.7279\n",
      "Epoch 326/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5968 - binary_accuracy: 0.6573 - f1_score: 0.7186 - val_loss: 1.7214 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7279\n",
      "Epoch 327/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5787 - binary_accuracy: 0.6653 - f1_score: 0.7309 - val_loss: 1.6417 - val_binary_accuracy: 0.6171 - val_f1_score: 0.7366\n",
      "Epoch 328/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6645 - binary_accuracy: 0.6562 - f1_score: 0.6964 - val_loss: 1.6371 - val_binary_accuracy: 0.6081 - val_f1_score: 0.7279\n",
      "Epoch 329/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5896 - binary_accuracy: 0.6759 - f1_score: 0.7104 - val_loss: 1.6303 - val_binary_accuracy: 0.6982 - val_f1_score: 0.7366\n",
      "Epoch 330/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6436 - binary_accuracy: 0.6769 - f1_score: 0.7067 - val_loss: 1.6328 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7366\n",
      "Epoch 331/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6086 - binary_accuracy: 0.6613 - f1_score: 0.7037 - val_loss: 1.6344 - val_binary_accuracy: 0.6982 - val_f1_score: 0.7279\n",
      "Epoch 332/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5352 - binary_accuracy: 0.6648 - f1_score: 0.7211 - val_loss: 1.6905 - val_binary_accuracy: 0.6937 - val_f1_score: 0.7279\n",
      "Epoch 333/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5386 - binary_accuracy: 0.6623 - f1_score: 0.7149 - val_loss: 1.6331 - val_binary_accuracy: 0.6937 - val_f1_score: 0.7279\n",
      "Epoch 334/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5838 - binary_accuracy: 0.6573 - f1_score: 0.7077 - val_loss: 1.6380 - val_binary_accuracy: 0.6126 - val_f1_score: 0.7279\n",
      "Epoch 335/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5030 - binary_accuracy: 0.6593 - f1_score: 0.7292 - val_loss: 1.6407 - val_binary_accuracy: 0.6126 - val_f1_score: 0.7279\n",
      "Epoch 336/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5525 - binary_accuracy: 0.6593 - f1_score: 0.7251 - val_loss: 1.6404 - val_binary_accuracy: 0.6982 - val_f1_score: 0.7279\n",
      "Epoch 337/500\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 1.6302 - binary_accuracy: 0.6517 - f1_score: 0.7099 - val_loss: 1.6382 - val_binary_accuracy: 0.6081 - val_f1_score: 0.7279\n",
      "Epoch 338/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.4253 - binary_accuracy: 0.6825 - f1_score: 0.7326 - val_loss: 1.6967 - val_binary_accuracy: 0.6937 - val_f1_score: 0.7279\n",
      "Epoch 339/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5413 - binary_accuracy: 0.6638 - f1_score: 0.7161 - val_loss: 1.6353 - val_binary_accuracy: 0.6982 - val_f1_score: 0.7279\n",
      "Epoch 340/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6400 - binary_accuracy: 0.6588 - f1_score: 0.7069 - val_loss: 1.6372 - val_binary_accuracy: 0.6937 - val_f1_score: 0.7279\n",
      "Epoch 341/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6126 - binary_accuracy: 0.6658 - f1_score: 0.6949 - val_loss: 1.6247 - val_binary_accuracy: 0.6937 - val_f1_score: 0.7279\n",
      "Epoch 342/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.7710 - binary_accuracy: 0.6653 - f1_score: 0.7115 - val_loss: 1.6160 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7366\n",
      "Epoch 343/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4556 - binary_accuracy: 0.6860 - f1_score: 0.7381 - val_loss: 1.6143 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7366\n",
      "Epoch 344/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5436 - binary_accuracy: 0.6673 - f1_score: 0.7055 - val_loss: 1.6180 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7366\n",
      "Epoch 345/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5401 - binary_accuracy: 0.6764 - f1_score: 0.7153 - val_loss: 1.6194 - val_binary_accuracy: 0.6982 - val_f1_score: 0.7366\n",
      "Epoch 346/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5004 - binary_accuracy: 0.6759 - f1_score: 0.7319 - val_loss: 1.6200 - val_binary_accuracy: 0.6982 - val_f1_score: 0.7366\n",
      "Epoch 347/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6230 - binary_accuracy: 0.6709 - f1_score: 0.7101 - val_loss: 1.6201 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7366\n",
      "Epoch 348/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.3969 - binary_accuracy: 0.6668 - f1_score: 0.7232 - val_loss: 1.6186 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7366\n",
      "Epoch 349/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.4374 - binary_accuracy: 0.6794 - f1_score: 0.7287 - val_loss: 1.6243 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7279\n",
      "Epoch 350/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6177 - binary_accuracy: 0.6573 - f1_score: 0.7075 - val_loss: 1.6169 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7366\n",
      "Epoch 351/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6841 - binary_accuracy: 0.6689 - f1_score: 0.7150 - val_loss: 1.6150 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7366\n",
      "Epoch 352/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6646 - binary_accuracy: 0.6532 - f1_score: 0.6935 - val_loss: 1.6092 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 353/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5308 - binary_accuracy: 0.6678 - f1_score: 0.7298 - val_loss: 1.6127 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 354/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5879 - binary_accuracy: 0.6855 - f1_score: 0.7252 - val_loss: 1.6066 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 355/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6218 - binary_accuracy: 0.6754 - f1_score: 0.7052 - val_loss: 1.6052 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 356/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.5649 - binary_accuracy: 0.6825 - f1_score: 0.7326 - val_loss: 1.6044 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 357/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6313 - binary_accuracy: 0.6578 - f1_score: 0.6943 - val_loss: 1.6080 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7366\n",
      "Epoch 358/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5743 - binary_accuracy: 0.6709 - f1_score: 0.7348 - val_loss: 1.6091 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7366\n",
      "Epoch 359/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6257 - binary_accuracy: 0.6658 - f1_score: 0.7115 - val_loss: 1.6066 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7366\n",
      "Epoch 360/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.4690 - binary_accuracy: 0.6648 - f1_score: 0.7140 - val_loss: 1.6120 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7279\n",
      "Epoch 361/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6241 - binary_accuracy: 0.6774 - f1_score: 0.7081 - val_loss: 1.6102 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7366\n",
      "Epoch 362/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5421 - binary_accuracy: 0.6840 - f1_score: 0.7242 - val_loss: 1.6119 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7366\n",
      "Epoch 363/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6851 - binary_accuracy: 0.6562 - f1_score: 0.7055 - val_loss: 1.6155 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 364/500\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 1.3671 - binary_accuracy: 0.6809 - f1_score: 0.7445 - val_loss: 1.6126 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7279\n",
      "Epoch 365/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5613 - binary_accuracy: 0.6573 - f1_score: 0.7065 - val_loss: 1.6166 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7279\n",
      "Epoch 366/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.6721 - binary_accuracy: 0.6678 - f1_score: 0.7039 - val_loss: 1.6092 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 367/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4610 - binary_accuracy: 0.6825 - f1_score: 0.7260 - val_loss: 1.6128 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 368/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4921 - binary_accuracy: 0.6638 - f1_score: 0.7022 - val_loss: 1.6719 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 369/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5255 - binary_accuracy: 0.6744 - f1_score: 0.7208 - val_loss: 1.6153 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7279\n",
      "Epoch 370/500\n",
      "31/31 [==============================] - ETA: 0s - loss: 1.7088 - binary_accuracy: 0.6729 - f1_score: 0.7266(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 117). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\MULTIMODAL_DEMPSTER_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 513ms/step - loss: 1.7088 - binary_accuracy: 0.6729 - f1_score: 0.7266 - val_loss: 1.6042 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7452\n",
      "Epoch 371/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5212 - binary_accuracy: 0.6935 - f1_score: 0.7305 - val_loss: 1.6020 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7452\n",
      "Epoch 372/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6272 - binary_accuracy: 0.6729 - f1_score: 0.7118 - val_loss: 1.6028 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 373/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5143 - binary_accuracy: 0.6804 - f1_score: 0.7139 - val_loss: 1.5983 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 374/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6149 - binary_accuracy: 0.6845 - f1_score: 0.7210 - val_loss: 1.5954 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7452\n",
      "Epoch 375/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.4003 - binary_accuracy: 0.6825 - f1_score: 0.7347 - val_loss: 1.5985 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7452\n",
      "Epoch 376/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7556 - binary_accuracy: 0.6825 - f1_score: 0.6932 - val_loss: 1.5937 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7452\n",
      "Epoch 377/500\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 1.5692 - binary_accuracy: 0.6825 - f1_score: 0.7279 - val_loss: 1.5910 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7452\n",
      "Epoch 378/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5365 - binary_accuracy: 0.6704 - f1_score: 0.7137 - val_loss: 1.5984 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7452\n",
      "Epoch 379/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.4856 - binary_accuracy: 0.6769 - f1_score: 0.7309 - val_loss: 1.6045 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 380/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6423 - binary_accuracy: 0.6855 - f1_score: 0.7181 - val_loss: 1.6032 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7279\n",
      "Epoch 381/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6357 - binary_accuracy: 0.6860 - f1_score: 0.7168 - val_loss: 1.5940 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7279\n",
      "Epoch 382/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7085 - binary_accuracy: 0.6709 - f1_score: 0.7134 - val_loss: 1.5917 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7279\n",
      "Epoch 383/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6068 - binary_accuracy: 0.6769 - f1_score: 0.7358 - val_loss: 1.5916 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7452\n",
      "Epoch 384/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5600 - binary_accuracy: 0.6729 - f1_score: 0.7113 - val_loss: 1.5971 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7452\n",
      "Epoch 385/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6572 - binary_accuracy: 0.6724 - f1_score: 0.7198 - val_loss: 1.6007 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7452\n",
      "Epoch 386/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.4878 - binary_accuracy: 0.6799 - f1_score: 0.7414 - val_loss: 1.6046 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7452\n",
      "Epoch 387/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6087 - binary_accuracy: 0.6784 - f1_score: 0.7237 - val_loss: 1.6008 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7452\n",
      "Epoch 388/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.5199 - binary_accuracy: 0.6714 - f1_score: 0.7127 - val_loss: 1.6042 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7452\n",
      "Epoch 389/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6148 - binary_accuracy: 0.6653 - f1_score: 0.7191 - val_loss: 1.6033 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7452\n",
      "Epoch 390/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5636 - binary_accuracy: 0.6628 - f1_score: 0.6979 - val_loss: 1.6036 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7452\n",
      "Epoch 391/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.4590 - binary_accuracy: 0.6825 - f1_score: 0.7360 - val_loss: 1.5368 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7452\n",
      "Epoch 392/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.4949 - binary_accuracy: 0.6749 - f1_score: 0.7307 - val_loss: 1.5358 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7452\n",
      "Epoch 393/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4566 - binary_accuracy: 0.6840 - f1_score: 0.7258 - val_loss: 1.5354 - val_binary_accuracy: 0.7297 - val_f1_score: 0.7452\n",
      "Epoch 394/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6736 - binary_accuracy: 0.6830 - f1_score: 0.7169 - val_loss: 1.5976 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7366\n",
      "Epoch 395/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4979 - binary_accuracy: 0.6784 - f1_score: 0.7396 - val_loss: 1.5990 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7366\n",
      "Epoch 396/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5990 - binary_accuracy: 0.6668 - f1_score: 0.7024 - val_loss: 1.6006 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7366\n",
      "Epoch 397/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5482 - binary_accuracy: 0.6830 - f1_score: 0.7264 - val_loss: 1.6074 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 398/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.3774 - binary_accuracy: 0.6749 - f1_score: 0.7111 - val_loss: 1.5365 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7452\n",
      "Epoch 399/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6502 - binary_accuracy: 0.6598 - f1_score: 0.7061 - val_loss: 1.5376 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7452\n",
      "Epoch 400/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.4914 - binary_accuracy: 0.6885 - f1_score: 0.7406 - val_loss: 1.5388 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7452\n",
      "Epoch 401/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6225 - binary_accuracy: 0.6870 - f1_score: 0.7160 - val_loss: 1.6143 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7366\n",
      "Epoch 402/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.3568 - binary_accuracy: 0.6875 - f1_score: 0.7396 - val_loss: 1.5905 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7452\n",
      "Epoch 403/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4837 - binary_accuracy: 0.6900 - f1_score: 0.7308 - val_loss: 1.5968 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7452\n",
      "Epoch 404/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5234 - binary_accuracy: 0.6890 - f1_score: 0.7268 - val_loss: 1.5970 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7452\n",
      "Epoch 405/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4241 - binary_accuracy: 0.6769 - f1_score: 0.7241 - val_loss: 1.5433 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7452\n",
      "Epoch 406/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4538 - binary_accuracy: 0.6880 - f1_score: 0.7262 - val_loss: 1.5342 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7452\n",
      "Epoch 407/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.4641 - binary_accuracy: 0.6915 - f1_score: 0.7408 - val_loss: 1.5335 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7452\n",
      "Epoch 408/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6597 - binary_accuracy: 0.6678 - f1_score: 0.6989 - val_loss: 1.5318 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7452\n",
      "Epoch 409/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6594 - binary_accuracy: 0.6779 - f1_score: 0.7142 - val_loss: 1.6111 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7452\n",
      "Epoch 410/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6448 - binary_accuracy: 0.6668 - f1_score: 0.7140 - val_loss: 1.6804 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 411/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4771 - binary_accuracy: 0.6673 - f1_score: 0.7255 - val_loss: 1.6140 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7452\n",
      "Epoch 412/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4291 - binary_accuracy: 0.6779 - f1_score: 0.7457 - val_loss: 1.6114 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7452\n",
      "Epoch 413/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4123 - binary_accuracy: 0.6699 - f1_score: 0.7085 - val_loss: 1.6111 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7452\n",
      "Epoch 414/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5595 - binary_accuracy: 0.6673 - f1_score: 0.7189 - val_loss: 1.6151 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7452\n",
      "Epoch 415/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5916 - binary_accuracy: 0.6754 - f1_score: 0.7134 - val_loss: 1.5577 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7452\n",
      "Epoch 416/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5754 - binary_accuracy: 0.6663 - f1_score: 0.7245 - val_loss: 1.5588 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7452\n",
      "Epoch 417/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.4430 - binary_accuracy: 0.6865 - f1_score: 0.7360 - val_loss: 1.6099 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7452\n",
      "Epoch 418/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.3870 - binary_accuracy: 0.6724 - f1_score: 0.7224 - val_loss: 1.6099 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7452\n",
      "Epoch 419/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5247 - binary_accuracy: 0.6658 - f1_score: 0.7206 - val_loss: 1.6122 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7452\n",
      "Epoch 420/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6092 - binary_accuracy: 0.6608 - f1_score: 0.7111 - val_loss: 1.6106 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7452\n",
      "Epoch 421/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4887 - binary_accuracy: 0.6663 - f1_score: 0.7115 - val_loss: 1.5486 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7452\n",
      "Epoch 422/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.5535 - binary_accuracy: 0.6628 - f1_score: 0.7087 - val_loss: 1.5551 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7452\n",
      "Epoch 423/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5612 - binary_accuracy: 0.6759 - f1_score: 0.7355 - val_loss: 1.5713 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7452\n",
      "Epoch 424/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5185 - binary_accuracy: 0.6719 - f1_score: 0.7395 - val_loss: 1.5474 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7452\n",
      "Epoch 425/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5149 - binary_accuracy: 0.6678 - f1_score: 0.7106 - val_loss: 1.5495 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7452\n",
      "Epoch 426/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.6023 - binary_accuracy: 0.6668 - f1_score: 0.7226 - val_loss: 1.6231 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7366\n",
      "Epoch 427/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5695 - binary_accuracy: 0.6683 - f1_score: 0.6947 - val_loss: 1.5568 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7452\n",
      "Epoch 428/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5621 - binary_accuracy: 0.6603 - f1_score: 0.7208 - val_loss: 1.6403 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7366\n",
      "Epoch 429/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5841 - binary_accuracy: 0.6683 - f1_score: 0.7111 - val_loss: 1.5567 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7452\n",
      "Epoch 430/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.5791 - binary_accuracy: 0.6638 - f1_score: 0.7010 - val_loss: 1.5522 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7452\n",
      "Epoch 431/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5507 - binary_accuracy: 0.6739 - f1_score: 0.7024 - val_loss: 1.5503 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7452\n",
      "Epoch 432/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6433 - binary_accuracy: 0.6593 - f1_score: 0.7258 - val_loss: 1.6157 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 433/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6886 - binary_accuracy: 0.6653 - f1_score: 0.6908 - val_loss: 1.6221 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 434/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5157 - binary_accuracy: 0.6648 - f1_score: 0.7223 - val_loss: 1.6243 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7366\n",
      "Epoch 435/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.4548 - binary_accuracy: 0.6729 - f1_score: 0.7236 - val_loss: 1.5493 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7452\n",
      "Epoch 436/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5879 - binary_accuracy: 0.6653 - f1_score: 0.7064 - val_loss: 1.6149 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 437/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.4132 - binary_accuracy: 0.6653 - f1_score: 0.7311 - val_loss: 1.5539 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7452\n",
      "Epoch 438/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5881 - binary_accuracy: 0.6704 - f1_score: 0.7154 - val_loss: 1.6192 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7366\n",
      "Epoch 439/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5253 - binary_accuracy: 0.6709 - f1_score: 0.7243 - val_loss: 1.6154 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 440/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6246 - binary_accuracy: 0.6618 - f1_score: 0.7193 - val_loss: 1.6162 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 441/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.4046 - binary_accuracy: 0.6714 - f1_score: 0.7312 - val_loss: 1.6167 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 442/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5242 - binary_accuracy: 0.6628 - f1_score: 0.7429 - val_loss: 1.6897 - val_binary_accuracy: 0.6982 - val_f1_score: 0.7279\n",
      "Epoch 443/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5302 - binary_accuracy: 0.6689 - f1_score: 0.7238 - val_loss: 1.5581 - val_binary_accuracy: 0.7027 - val_f1_score: 0.7366\n",
      "Epoch 444/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6494 - binary_accuracy: 0.6638 - f1_score: 0.6976 - val_loss: 1.6193 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 445/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.6255 - binary_accuracy: 0.6774 - f1_score: 0.7235 - val_loss: 1.5581 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 446/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.4700 - binary_accuracy: 0.6734 - f1_score: 0.7110 - val_loss: 1.6142 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 447/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.4985 - binary_accuracy: 0.6573 - f1_score: 0.7279 - val_loss: 1.5470 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7452\n",
      "Epoch 448/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6682 - binary_accuracy: 0.6683 - f1_score: 0.7052 - val_loss: 1.6767 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7279\n",
      "Epoch 449/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6144 - binary_accuracy: 0.6835 - f1_score: 0.7309 - val_loss: 1.6139 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7279\n",
      "Epoch 450/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5401 - binary_accuracy: 0.6709 - f1_score: 0.7100 - val_loss: 1.6748 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 451/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.4180 - binary_accuracy: 0.6744 - f1_score: 0.7378 - val_loss: 1.7407 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7193\n",
      "Epoch 452/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.4796 - binary_accuracy: 0.6830 - f1_score: 0.7168 - val_loss: 1.6717 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 453/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5236 - binary_accuracy: 0.6850 - f1_score: 0.7174 - val_loss: 1.6128 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 454/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5308 - binary_accuracy: 0.6789 - f1_score: 0.7295 - val_loss: 1.6757 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7193\n",
      "Epoch 455/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6575 - binary_accuracy: 0.6598 - f1_score: 0.7108 - val_loss: 1.6842 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7193\n",
      "Epoch 456/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.4490 - binary_accuracy: 0.6799 - f1_score: 0.7306 - val_loss: 1.6097 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 457/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.3969 - binary_accuracy: 0.6789 - f1_score: 0.7319 - val_loss: 1.6087 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 458/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6440 - binary_accuracy: 0.6643 - f1_score: 0.7057 - val_loss: 1.6785 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 459/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.6559 - binary_accuracy: 0.6653 - f1_score: 0.7107 - val_loss: 1.6138 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7279\n",
      "Epoch 460/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5308 - binary_accuracy: 0.6694 - f1_score: 0.7177 - val_loss: 1.6823 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7279\n",
      "Epoch 461/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4706 - binary_accuracy: 0.6850 - f1_score: 0.7450 - val_loss: 1.6106 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 462/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5031 - binary_accuracy: 0.6678 - f1_score: 0.7295 - val_loss: 1.6082 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7366\n",
      "Epoch 463/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5315 - binary_accuracy: 0.6653 - f1_score: 0.7112 - val_loss: 1.6064 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 464/500\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.4723 - binary_accuracy: 0.6759 - f1_score: 0.7138 - val_loss: 1.6127 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 465/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.4807 - binary_accuracy: 0.6638 - f1_score: 0.7258 - val_loss: 1.5527 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7452\n",
      "Epoch 466/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5664 - binary_accuracy: 0.6759 - f1_score: 0.7241 - val_loss: 1.6138 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 467/500\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 1.4120 - binary_accuracy: 0.6905 - f1_score: 0.7473 - val_loss: 1.6125 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 468/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.3993 - binary_accuracy: 0.6714 - f1_score: 0.7281 - val_loss: 1.6092 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7366\n",
      "Epoch 469/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5156 - binary_accuracy: 0.6714 - f1_score: 0.7127 - val_loss: 1.6149 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7366\n",
      "Epoch 470/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5211 - binary_accuracy: 0.6779 - f1_score: 0.7315 - val_loss: 1.6035 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 471/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.4827 - binary_accuracy: 0.6628 - f1_score: 0.7116 - val_loss: 1.6040 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 472/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5076 - binary_accuracy: 0.6759 - f1_score: 0.7060 - val_loss: 1.6043 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 473/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5953 - binary_accuracy: 0.6855 - f1_score: 0.7334 - val_loss: 1.6088 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 474/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6226 - binary_accuracy: 0.6643 - f1_score: 0.7032 - val_loss: 1.6021 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 475/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.4277 - binary_accuracy: 0.6663 - f1_score: 0.7302 - val_loss: 1.5987 - val_binary_accuracy: 0.7252 - val_f1_score: 0.7366\n",
      "Epoch 476/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5139 - binary_accuracy: 0.6673 - f1_score: 0.7106 - val_loss: 1.6675 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 477/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.4848 - binary_accuracy: 0.6809 - f1_score: 0.7094 - val_loss: 1.6666 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 478/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.4781 - binary_accuracy: 0.6895 - f1_score: 0.7300 - val_loss: 1.6599 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7279\n",
      "Epoch 479/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.4383 - binary_accuracy: 0.6759 - f1_score: 0.7362 - val_loss: 1.6619 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7279\n",
      "Epoch 480/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5047 - binary_accuracy: 0.6714 - f1_score: 0.7282 - val_loss: 1.6666 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 481/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5018 - binary_accuracy: 0.6739 - f1_score: 0.7181 - val_loss: 1.6683 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 482/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4833 - binary_accuracy: 0.6709 - f1_score: 0.7085 - val_loss: 1.6677 - val_binary_accuracy: 0.7207 - val_f1_score: 0.7279\n",
      "Epoch 483/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5368 - binary_accuracy: 0.6769 - f1_score: 0.7181 - val_loss: 1.6643 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7279\n",
      "Epoch 484/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.4588 - binary_accuracy: 0.6774 - f1_score: 0.7278 - val_loss: 1.6619 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7279\n",
      "Epoch 485/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4825 - binary_accuracy: 0.6875 - f1_score: 0.7283 - val_loss: 1.6604 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7279\n",
      "Epoch 486/500\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6293 - binary_accuracy: 0.6825 - f1_score: 0.7007 - val_loss: 1.5936 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7366\n",
      "Epoch 487/500\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 1.4978 - binary_accuracy: 0.6739 - f1_score: 0.7095 - val_loss: 1.6564 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7374\n",
      "Epoch 488/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4242 - binary_accuracy: 0.6855 - f1_score: 0.7373 - val_loss: 1.6534 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7374\n",
      "Epoch 489/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4899 - binary_accuracy: 0.6820 - f1_score: 0.7353 - val_loss: 1.6571 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7374\n",
      "Epoch 490/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.4438 - binary_accuracy: 0.6744 - f1_score: 0.7061 - val_loss: 1.6609 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7279\n",
      "Epoch 491/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5596 - binary_accuracy: 0.7001 - f1_score: 0.7247 - val_loss: 1.6577 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7279\n",
      "Epoch 492/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5238 - binary_accuracy: 0.6815 - f1_score: 0.7286 - val_loss: 1.6588 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7279\n",
      "Epoch 493/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5722 - binary_accuracy: 0.6885 - f1_score: 0.7230 - val_loss: 1.6581 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7374\n",
      "Epoch 494/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4881 - binary_accuracy: 0.6825 - f1_score: 0.7082 - val_loss: 1.6550 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7374\n",
      "Epoch 495/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5029 - binary_accuracy: 0.6956 - f1_score: 0.7441 - val_loss: 1.7259 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7287\n",
      "Epoch 496/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5499 - binary_accuracy: 0.6704 - f1_score: 0.7134 - val_loss: 1.7268 - val_binary_accuracy: 0.7072 - val_f1_score: 0.7287\n",
      "Epoch 497/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4294 - binary_accuracy: 0.6678 - f1_score: 0.7135 - val_loss: 1.6592 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7279\n",
      "Epoch 498/500\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.5284 - binary_accuracy: 0.6820 - f1_score: 0.7132 - val_loss: 1.6581 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7374\n",
      "Epoch 499/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.3847 - binary_accuracy: 0.7016 - f1_score: 0.7336 - val_loss: 1.6546 - val_binary_accuracy: 0.7117 - val_f1_score: 0.7374\n",
      "Epoch 500/500\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.5652 - binary_accuracy: 0.6885 - f1_score: 0.7381 - val_loss: 1.6510 - val_binary_accuracy: 0.7162 - val_f1_score: 0.7374\n"
     ]
    }
   ],
   "source": [
    "bestPt = checkpointPath / Path('efficientnet_Classification_acc')\n",
    "CNNmod = tf.keras.models.load_model(bestPt)\n",
    "CNNmod = changeModelLayers(CNNmod,'_CNN_ds')\n",
    "CNNmod.trainable = False\n",
    "\n",
    "bestPt = checkpointPath / Path('MLP_Classification_acc')\n",
    "MLPmod = tf.keras.models.load_model(bestPt)\n",
    "MLPmod = changeModelLayers(MLPmod,'_MLP_ds')\n",
    "MLPmod.trainable = False\n",
    "    \n",
    "\n",
    "# concatMultimodal = create_decision_Multimodal(MLPmod,CNNmod,-1,-1)\n",
    "# concatMultimodal = create_decision_Multimodal_DEMPSTER(MLPmod,CNNmod, uncertainty=True)\n",
    "concatMultimodal = create_decision_Multimodal_DEMPSTER(MLPmod,CNNmod)\n",
    "\n",
    "plot_model(concatMultimodal, to_file=plotpath / Path('MULTIMODAL DEMPSTER_only.png'), show_shapes=True, show_layer_names=True)\n",
    "\n",
    "\n",
    "initial_learning_rate = 1e-3  # migliore come accuracy massima  0.69\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=100, decay_rate=0.99, staircase=False)\n",
    "\n",
    "concatMultimodal.compile(\n",
    "        # optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-2, decay=1e-4),   #FUSION 2022\n",
    "        # optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-4),   #FUSION 2022\n",
    "        optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "        # optimizer=tf.keras.optimizers.Adam(0.0011),\n",
    "        # optimizer = tf.keras.optimizers.Adam(1e-2),\n",
    "        # optimizer=tf.keras.optimizers.Adam(1.1e-3),\n",
    "        # loss=\"categorical_crossentropy\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        # loss=\"mean_squared_logarithmic_error\",\n",
    "        # metrics=[\"accuracy\",dice_coef,\"categorical_accuracy\"],\n",
    "        # metrics=[\"categorical_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=4)],\n",
    "        # metrics=[\"categorical_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=2)],\n",
    "        metrics=[\"binary_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=2)],\n",
    "        # metrics=[\"categorical_accuracy\"],\n",
    ")\n",
    "# CREATE CALLBACKS\n",
    "checkpointClassification = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        checkpointPath / Path('MULTIMODAL_DEMPSTER_Classification_acc'),\n",
    "                        monitor='val_binary_accuracy', verbose=0, \n",
    "                        # monitor='val_dm_pignistic_16_categorical_accuracy', verbose=0, \n",
    "                        save_best_only=True, mode='max')\n",
    "                                                # CREATE CALLBACKS\n",
    "\n",
    "\n",
    "checkpointClassificationF1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        checkpointPath / Path('MULTIMODAL_DEMPSTER_Classification_F1'), \n",
    "                        monitor='val_f1_score', verbose=0, \n",
    "                        save_best_only=True, mode='max')\n",
    "                                                # CREATE CALLBACKS\n",
    "\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "\n",
    "    \n",
    "# lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "\n",
    "cb = TimingCallback()\n",
    "\n",
    "print(checkpointPath / Path('MULTIMODAL_DEMPSTER'))\n",
    "\n",
    "# callbacks_list = [checkpointClassification]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb, schedule_lr,lrate]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb, schedule_lr]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, cb, schedule_lr]\n",
    "callbacks_list = [checkpointClassification, checkpointClassificationF1, cb]\n",
    "# callbacks_list = [checkpointClassification, cb]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb]\n",
    "# callbacks_list = [checkpointClassificationF1, earlyStopping]\n",
    "\n",
    "history = concatMultimodal.fit(\n",
    "        # x = X_train_IMGS,\n",
    "        # trainset,\n",
    "        x = [X_train_IMGS,X_train],\n",
    "        # y = [Y_train,Y_tr_unc],\n",
    "        y = Y_train,\n",
    "        # epochs = 1,\n",
    "        epochs = 500,\n",
    "        validation_data = [[X_val_IMGS,X_val],Y_val],\n",
    "        # validation_data = [X_val_IMGS,[Y_val,Y_val_unc]],\n",
    "        # validation_data = validset,\n",
    "        callbacks = callbacks_list,    #some problems with DS layers and callbacks\n",
    "        batch_size=32,\n",
    "        # batch_size=1,\n",
    "        # class_weight=c_w,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train loss",
         "type": "scatter",
         "y": [
          4.151388168334961,
          4.231709003448486,
          3.692570686340332,
          3.8564138412475586,
          3.5314204692840576,
          3.8018736839294434,
          3.3602967262268066,
          3.218773365020752,
          2.7974722385406494,
          2.695164680480957,
          2.763653516769409,
          2.714772939682007,
          2.778103828430176,
          2.3681704998016357,
          2.605031728744507,
          2.679175615310669,
          2.4238786697387695,
          2.7330217361450195,
          2.3260321617126465,
          2.737473487854004,
          2.568382501602173,
          2.5410654544830322,
          2.3909740447998047,
          2.346877098083496,
          2.2743403911590576,
          2.5478882789611816,
          2.4802098274230957,
          2.515301465988159,
          2.417815923690796,
          2.3843436241149902,
          2.275484323501587,
          2.5301153659820557,
          2.3096771240234375,
          2.2913737297058105,
          2.3508479595184326,
          2.1640334129333496,
          2.283754587173462,
          2.0452072620391846,
          2.106743097305298,
          2.4041314125061035,
          2.3724398612976074,
          2.239227294921875,
          2.1686666011810303,
          2.08808970451355,
          1.9427746534347534,
          1.8437386751174927,
          2.258967399597168,
          1.8360772132873535,
          2.185464859008789,
          2.0686821937561035,
          2.242003917694092,
          2.3420603275299072,
          2.1886773109436035,
          2.167173385620117,
          2.103630304336548,
          1.943922996520996,
          2.058595895767212,
          1.9878756999969482,
          2.1322405338287354,
          1.9819544553756714,
          2.1187262535095215,
          2.116705894470215,
          2.05698299407959,
          1.9254813194274902,
          2.07987904548645,
          1.8904317617416382,
          1.795393943786621,
          1.8212852478027344,
          1.8566999435424805,
          1.9451388120651245,
          1.9657131433486938,
          1.8767505884170532,
          2.000441074371338,
          1.8246628046035767,
          1.9385528564453125,
          1.888296365737915,
          1.9810243844985962,
          2.001343011856079,
          1.7953448295593262,
          1.980862021446228,
          1.9068329334259033,
          1.7310842275619507,
          1.9271841049194336,
          1.7763075828552246,
          1.8442723751068115,
          1.8602865934371948,
          1.8995686769485474,
          1.8564468622207642,
          1.7126785516738892,
          1.7977945804595947,
          1.965885877609253,
          1.855664849281311,
          1.6974676847457886,
          1.7688175439834595,
          1.8893259763717651,
          1.6964956521987915,
          1.8250638246536255,
          1.8924201726913452,
          2.019249200820923,
          1.906840205192566,
          1.6555765867233276,
          1.7050362825393677,
          1.8926842212677002,
          1.8668261766433716,
          1.7426172494888306,
          1.7923965454101562,
          1.9178545475006104,
          1.8715027570724487,
          1.8688249588012695,
          1.7429267168045044,
          1.9023058414459229,
          1.8038839101791382,
          1.8930832147598267,
          1.9965835809707642,
          1.7640856504440308,
          2.022414207458496,
          1.8929252624511719,
          1.8484429121017456,
          1.853579044342041,
          1.8028886318206787,
          1.8508092164993286,
          1.9227027893066406,
          1.7751117944717407,
          1.8469032049179077,
          1.7972700595855713,
          1.848006248474121,
          1.9299709796905518,
          1.8092989921569824,
          1.8826813697814941,
          1.8167015314102173,
          1.900199294090271,
          1.8719688653945923,
          1.8918901681900024,
          1.6666353940963745,
          1.7958821058273315,
          1.7227002382278442,
          1.8141390085220337,
          1.8620103597640991,
          1.6074364185333252,
          1.7548810243606567,
          1.9163252115249634,
          1.6997915506362915,
          1.8157060146331787,
          1.8235410451889038,
          1.8496832847595215,
          1.7724653482437134,
          1.800613284111023,
          1.7897601127624512,
          1.8583141565322876,
          1.7537213563919067,
          1.8503766059875488,
          1.7553328275680542,
          1.850890874862671,
          1.67776358127594,
          1.6256996393203735,
          1.8736915588378906,
          1.780229926109314,
          1.8129808902740479,
          1.7440143823623657,
          1.7209292650222778,
          1.6772100925445557,
          2.0918128490448,
          1.8655486106872559,
          1.8582327365875244,
          1.7073103189468384,
          1.7563058137893677,
          1.7098103761672974,
          1.8098065853118896,
          1.8480364084243774,
          1.9264945983886719,
          1.9505822658538818,
          1.8999704122543335,
          1.677428960800171,
          1.7129371166229248,
          1.8251372575759888,
          1.7525629997253418,
          1.7044992446899414,
          1.7160028219223022,
          1.6689958572387695,
          1.8120731115341187,
          1.8355478048324585,
          1.6700444221496582,
          1.8458870649337769,
          1.8387925624847412,
          1.6703224182128906,
          1.8291451930999756,
          1.6714459657669067,
          1.8301666975021362,
          1.8322222232818604,
          1.8086843490600586,
          1.765260934829712,
          1.7246538400650024,
          1.884009838104248,
          1.7271788120269775,
          1.6768954992294312,
          2.0020956993103027,
          1.7450377941131592,
          1.6715084314346313,
          1.644242763519287,
          1.8957486152648926,
          2.00520920753479,
          1.7802952527999878,
          1.7887479066848755,
          1.7495646476745605,
          1.8004450798034668,
          1.676171898841858,
          1.5129506587982178,
          1.946459412574768,
          1.8119124174118042,
          1.929504156112671,
          1.696747064590454,
          1.5971421003341675,
          1.6419270038604736,
          1.733412742614746,
          1.7677671909332275,
          1.823056936264038,
          1.7482571601867676,
          1.765741229057312,
          1.6895183324813843,
          1.7153041362762451,
          1.8789042234420776,
          1.5759817361831665,
          1.7225245237350464,
          1.639504313468933,
          1.7616302967071533,
          1.7163814306259155,
          1.7293342351913452,
          1.6567001342773438,
          1.8021018505096436,
          1.7683771848678589,
          1.6649057865142822,
          1.7725814580917358,
          1.854651689529419,
          1.7678451538085938,
          1.7455495595932007,
          1.694577693939209,
          1.6445133686065674,
          1.6562683582305908,
          1.6930140256881714,
          1.7371008396148682,
          1.7329905033111572,
          1.7540546655654907,
          1.5850869417190552,
          1.6167709827423096,
          1.7175606489181519,
          1.7976224422454834,
          1.7034796476364136,
          1.8150663375854492,
          1.665720820426941,
          1.8852952718734741,
          1.8332982063293457,
          1.8308923244476318,
          1.6872650384902954,
          1.8897590637207031,
          1.8363336324691772,
          1.676716685295105,
          1.760385274887085,
          1.732627034187317,
          1.6719529628753662,
          1.6835951805114746,
          1.7967727184295654,
          1.7822892665863037,
          1.863148808479309,
          1.8461127281188965,
          1.7423235177993774,
          1.6758086681365967,
          1.630813479423523,
          1.8508371114730835,
          1.4804809093475342,
          1.7396663427352905,
          1.6609605550765991,
          1.7681978940963745,
          1.7285151481628418,
          1.7321889400482178,
          1.734218955039978,
          1.7279527187347412,
          1.712559700012207,
          1.6604787111282349,
          1.6879640817642212,
          1.5147305727005005,
          1.6050795316696167,
          1.5955283641815186,
          1.9276151657104492,
          1.472884178161621,
          1.6604783535003662,
          1.7153959274291992,
          1.4751019477844238,
          1.6536067724227905,
          1.663754940032959,
          1.5935144424438477,
          1.6998131275177002,
          1.7658910751342773,
          1.6223347187042236,
          1.6521975994110107,
          1.6388826370239258,
          1.725715160369873,
          1.5839383602142334,
          1.7495646476745605,
          1.554556965827942,
          1.6190091371536255,
          1.6485487222671509,
          1.6996071338653564,
          1.704095721244812,
          1.6519231796264648,
          1.751092553138733,
          1.7529969215393066,
          1.667304515838623,
          1.768739938735962,
          1.6915241479873657,
          1.559221625328064,
          1.661529541015625,
          1.6895490884780884,
          1.6441551446914673,
          1.475151777267456,
          1.6576008796691895,
          1.6369069814682007,
          1.5351396799087524,
          1.762876033782959,
          1.7235599756240845,
          1.4446932077407837,
          1.5603864192962646,
          1.5534932613372803,
          1.6039174795150757,
          1.5372493267059326,
          1.590185284614563,
          1.5967800617218018,
          1.5786620378494263,
          1.6645474433898926,
          1.5895919799804688,
          1.6436095237731934,
          1.6085965633392334,
          1.5352160930633545,
          1.5386093854904175,
          1.583784580230713,
          1.5030103921890259,
          1.5524636507034302,
          1.6302311420440674,
          1.4253407716751099,
          1.5412713289260864,
          1.6399983167648315,
          1.612621545791626,
          1.7709931135177612,
          1.4555858373641968,
          1.5436253547668457,
          1.5401171445846558,
          1.500413179397583,
          1.622972846031189,
          1.396902322769165,
          1.4374423027038574,
          1.6176996231079102,
          1.6840566396713257,
          1.664626121520996,
          1.5307945013046265,
          1.5878671407699585,
          1.6218082904815674,
          1.5649338960647583,
          1.6312952041625977,
          1.574282169342041,
          1.62567937374115,
          1.4689795970916748,
          1.6241294145584106,
          1.5421029329299927,
          1.685101866722107,
          1.3670562505722046,
          1.5613019466400146,
          1.6720823049545288,
          1.4610496759414673,
          1.4921176433563232,
          1.5255450010299683,
          1.7088207006454468,
          1.5211695432662964,
          1.6272090673446655,
          1.5143492221832275,
          1.6149046421051025,
          1.4002983570098877,
          1.755601406097412,
          1.5692241191864014,
          1.536496639251709,
          1.485612392425537,
          1.6422847509384155,
          1.6356550455093384,
          1.7085282802581787,
          1.6067990064620972,
          1.5600022077560425,
          1.6571580171585083,
          1.4877525568008423,
          1.6086796522140503,
          1.5199229717254639,
          1.6147738695144653,
          1.5636388063430786,
          1.4590156078338623,
          1.494905710220337,
          1.4566423892974854,
          1.673636794090271,
          1.4979268312454224,
          1.5990149974822998,
          1.5481889247894287,
          1.3774372339248657,
          1.6502153873443604,
          1.4913818836212158,
          1.6225358247756958,
          1.3567935228347778,
          1.4837380647659302,
          1.5233994722366333,
          1.424090027809143,
          1.4538079500198364,
          1.4641385078430176,
          1.6596713066101074,
          1.6594167947769165,
          1.644849181175232,
          1.4771400690078735,
          1.4291164875030518,
          1.4122670888900757,
          1.559543490409851,
          1.5915542840957642,
          1.575416922569275,
          1.443037748336792,
          1.3870353698730469,
          1.5246835947036743,
          1.6092053651809692,
          1.488659143447876,
          1.5534582138061523,
          1.5612356662750244,
          1.5185497999191284,
          1.5149296522140503,
          1.6022731065750122,
          1.5695422887802124,
          1.5621243715286255,
          1.5840585231781006,
          1.5791369676589966,
          1.5507014989852905,
          1.6432909965515137,
          1.6886237859725952,
          1.515726923942566,
          1.4548389911651611,
          1.5879392623901367,
          1.413205623626709,
          1.5881410837173462,
          1.525294303894043,
          1.6246258020401,
          1.4045554399490356,
          1.5241838693618774,
          1.5302484035491943,
          1.6494486331939697,
          1.625516653060913,
          1.4699889421463013,
          1.4984651803970337,
          1.6682343482971191,
          1.6143907308578491,
          1.540092945098877,
          1.4180030822753906,
          1.4796212911605835,
          1.5236310958862305,
          1.5307753086090088,
          1.6574550867080688,
          1.4490015506744385,
          1.3969218730926514,
          1.643971562385559,
          1.6558985710144043,
          1.5307695865631104,
          1.470568060874939,
          1.5031113624572754,
          1.5315253734588623,
          1.4723131656646729,
          1.4806580543518066,
          1.566360354423523,
          1.4119775295257568,
          1.3993496894836426,
          1.5155717134475708,
          1.5211138725280762,
          1.4827197790145874,
          1.5076106786727905,
          1.5952507257461548,
          1.6225917339324951,
          1.4276682138442993,
          1.5139113664627075,
          1.4847924709320068,
          1.478084921836853,
          1.4383138418197632,
          1.504719853401184,
          1.5017855167388916,
          1.483292818069458,
          1.5367887020111084,
          1.4587891101837158,
          1.4825133085250854,
          1.629258632659912,
          1.49776291847229,
          1.4241770505905151,
          1.4898990392684937,
          1.4437512159347534,
          1.5596067905426025,
          1.5237740278244019,
          1.5722495317459106,
          1.4880975484848022,
          1.5028959512710571,
          1.549919843673706,
          1.429416298866272,
          1.5283516645431519,
          1.3847086429595947,
          1.5652345418930054
         ]
        },
        {
         "name": "Validation loss",
         "type": "scatter",
         "y": [
          4.2985687255859375,
          4.427812576293945,
          4.32131814956665,
          3.879709005355835,
          3.3994531631469727,
          3.311373233795166,
          3.060332775115967,
          2.46897029876709,
          2.439700126647949,
          2.4903528690338135,
          2.4093518257141113,
          2.3148393630981445,
          2.1902287006378174,
          2.294347047805786,
          2.2316198348999023,
          2.0997941493988037,
          2.153188943862915,
          2.061373710632324,
          2.0358903408050537,
          2.0264484882354736,
          2.3175301551818848,
          1.778886318206787,
          1.7705283164978027,
          1.8086879253387451,
          2.681436538696289,
          2.613630533218384,
          2.6141648292541504,
          2.6812925338745117,
          2.6166274547576904,
          2.6815109252929688,
          2.6815600395202637,
          1.8435276746749878,
          1.7485713958740234,
          1.8171603679656982,
          1.8153282403945923,
          1.811380386352539,
          1.745100975036621,
          1.7487428188323975,
          1.7404382228851318,
          1.739127278327942,
          1.7419999837875366,
          1.7415968179702759,
          1.7437540292739868,
          1.7411425113677979,
          1.7405250072479248,
          1.743330717086792,
          1.739556074142456,
          1.7373002767562866,
          1.8794492483139038,
          1.946645736694336,
          2.615530014038086,
          2.061920404434204,
          1.8046778440475464,
          1.762756109237671,
          1.7297732830047607,
          1.7304127216339111,
          1.7303298711776733,
          1.6628258228302002,
          1.6712325811386108,
          1.6665102243423462,
          1.6643657684326172,
          1.6586834192276,
          1.6597435474395752,
          1.6637898683547974,
          1.663763403892517,
          1.6636844873428345,
          1.667736530303955,
          1.6653797626495361,
          1.6682876348495483,
          1.6661033630371094,
          1.666317343711853,
          1.6680893898010254,
          1.6696583032608032,
          1.7394158840179443,
          1.6946486234664917,
          1.695717453956604,
          1.6930745840072632,
          1.6877986192703247,
          1.6865202188491821,
          1.6804245710372925,
          1.74842369556427,
          1.7476614713668823,
          1.7470918893814087,
          1.7415690422058105,
          1.7424236536026,
          1.7416274547576904,
          1.7449216842651367,
          1.7388657331466675,
          1.6706560850143433,
          1.7111164331436157,
          1.7262674570083618,
          1.7305362224578857,
          1.7333831787109375,
          1.7318646907806396,
          1.733222484588623,
          1.728805422782898,
          1.7245917320251465,
          1.7921546697616577,
          1.722749948501587,
          1.7261468172073364,
          1.7257615327835083,
          1.7201975584030151,
          1.7245455980300903,
          1.7207032442092896,
          1.7230520248413086,
          1.7235980033874512,
          1.7221696376800537,
          1.7239584922790527,
          1.7205804586410522,
          1.7204339504241943,
          1.713412880897522,
          1.713707685470581,
          1.7121115922927856,
          1.7803934812545776,
          1.779084324836731,
          1.7800546884536743,
          1.710627555847168,
          1.7154275178909302,
          1.7134132385253906,
          1.7155593633651733,
          1.7114571332931519,
          1.7154459953308105,
          1.715003490447998,
          1.7143930196762085,
          1.7146342992782593,
          1.7093265056610107,
          1.709335446357727,
          1.7113951444625854,
          1.7082117795944214,
          1.7094818353652954,
          1.708290934562683,
          1.7749342918395996,
          1.7113817930221558,
          1.7133924961090088,
          1.7136026620864868,
          1.7132056951522827,
          1.7140398025512695,
          1.7066570520401,
          1.7059382200241089,
          1.7704596519470215,
          1.7026851177215576,
          1.7704925537109375,
          1.7704614400863647,
          1.7715262174606323,
          1.7662814855575562,
          1.7660596370697021,
          1.777191400527954,
          1.7797449827194214,
          1.7756779193878174,
          1.7783901691436768,
          1.7784881591796875,
          1.7783278226852417,
          1.833017110824585,
          1.7827396392822266,
          1.8381084203720093,
          1.8315016031265259,
          1.7742602825164795,
          1.7715132236480713,
          1.7729145288467407,
          1.772060751914978,
          1.7735257148742676,
          1.7568650245666504,
          1.7714972496032715,
          1.7800251245498657,
          1.7777763605117798,
          1.777161717414856,
          1.7687087059020996,
          1.763180136680603,
          1.7764674425125122,
          1.7788349390029907,
          1.7731871604919434,
          1.7762017250061035,
          1.786198616027832,
          1.7617652416229248,
          1.8290256261825562,
          1.776800513267517,
          1.7768785953521729,
          1.8302626609802246,
          1.782259225845337,
          1.7772126197814941,
          1.8457330465316772,
          1.793315052986145,
          1.7911750078201294,
          1.7870123386383057,
          1.7858777046203613,
          1.7844693660736084,
          1.7856823205947876,
          1.7850602865219116,
          1.786075234413147,
          1.7856329679489136,
          1.7897260189056396,
          1.8377666473388672,
          1.792514681816101,
          1.7795592546463013,
          1.7785484790802002,
          1.8517742156982422,
          1.850750207901001,
          1.7858645915985107,
          1.7899179458618164,
          1.786482810974121,
          1.7895946502685547,
          1.7815991640090942,
          1.8664673566818237,
          1.8430923223495483,
          1.841767430305481,
          1.7877476215362549,
          1.768465518951416,
          1.846468210220337,
          1.864898920059204,
          1.783378005027771,
          1.7783066034317017,
          1.7830454111099243,
          1.846113920211792,
          1.7951456308364868,
          1.7738432884216309,
          1.8526270389556885,
          1.7703362703323364,
          1.8304327726364136,
          1.826758623123169,
          1.7665135860443115,
          1.770642638206482,
          1.7603793144226074,
          1.7569838762283325,
          1.7517876625061035,
          1.6797854900360107,
          1.7492810487747192,
          1.7562847137451172,
          1.7000985145568848,
          1.6854910850524902,
          1.7569595575332642,
          1.682799220085144,
          1.7499299049377441,
          1.679349422454834,
          1.7625278234481812,
          1.766463279724121,
          1.752034068107605,
          1.7585126161575317,
          1.8239333629608154,
          1.8204811811447144,
          1.8212412595748901,
          1.7527190446853638,
          1.7497756481170654,
          1.7507357597351074,
          1.750395655632019,
          1.6861943006515503,
          1.8188872337341309,
          1.6872984170913696,
          1.683661937713623,
          1.679805040359497,
          1.677123785018921,
          1.6747705936431885,
          1.6911860704421997,
          1.6757065057754517,
          1.673861026763916,
          1.688938021659851,
          1.677689552307129,
          1.68607759475708,
          1.6800261735916138,
          1.6796079874038696,
          1.6973767280578613,
          1.675940752029419,
          1.6662147045135498,
          1.669296145439148,
          1.66695237159729,
          1.6656051874160767,
          1.7007256746292114,
          1.6955995559692383,
          1.6956418752670288,
          1.699833869934082,
          1.7577379941940308,
          1.7571308612823486,
          1.689911127090454,
          1.6479583978652954,
          1.635428786277771,
          1.6457064151763916,
          1.69561767578125,
          1.6466883420944214,
          1.695526361465454,
          1.7027860879898071,
          1.6387981176376343,
          1.6379259824752808,
          1.6515138149261475,
          1.6399714946746826,
          1.649688959121704,
          1.651485800743103,
          1.6433852910995483,
          1.6292777061462402,
          1.6904252767562866,
          1.636745810508728,
          1.6990565061569214,
          1.6181029081344604,
          1.6105927228927612,
          1.612405776977539,
          1.6073404550552368,
          1.629616379737854,
          1.6206440925598145,
          1.646485447883606,
          1.6424446105957031,
          1.6138147115707397,
          1.6240853071212769,
          1.630621314048767,
          1.6367894411087036,
          1.637854814529419,
          1.7046411037445068,
          1.7001593112945557,
          1.6302616596221924,
          1.695712685585022,
          1.678809642791748,
          1.6094938516616821,
          1.6128665208816528,
          1.6100045442581177,
          1.612160563468933,
          1.6096936464309692,
          1.6071491241455078,
          1.6061583757400513,
          1.605116605758667,
          1.606345534324646,
          1.6684644222259521,
          1.6681371927261353,
          1.6131763458251953,
          1.6753568649291992,
          1.7073938846588135,
          1.6411484479904175,
          1.6566030979156494,
          1.6615008115768433,
          1.7213690280914307,
          1.6416789293289185,
          1.6371116638183594,
          1.630272626876831,
          1.6328223943710327,
          1.6344199180603027,
          1.690453290939331,
          1.6331361532211304,
          1.6379529237747192,
          1.6407135725021362,
          1.6404303312301636,
          1.6381940841674805,
          1.6966758966445923,
          1.6352566480636597,
          1.637247085571289,
          1.6247414350509644,
          1.6160331964492798,
          1.6142510175704956,
          1.6179689168930054,
          1.6194380521774292,
          1.6200355291366577,
          1.6201239824295044,
          1.618556261062622,
          1.6242856979370117,
          1.6168904304504395,
          1.6149606704711914,
          1.6091817617416382,
          1.6127015352249146,
          1.6065996885299683,
          1.6052402257919312,
          1.6043821573257446,
          1.608018159866333,
          1.609139323234558,
          1.6065653562545776,
          1.6120461225509644,
          1.610161304473877,
          1.611910343170166,
          1.6154838800430298,
          1.612550973892212,
          1.6166026592254639,
          1.609218955039978,
          1.6127911806106567,
          1.6719080209732056,
          1.6153082847595215,
          1.6041674613952637,
          1.6020078659057617,
          1.602759599685669,
          1.5983213186264038,
          1.595415711402893,
          1.598487138748169,
          1.5936734676361084,
          1.5909736156463623,
          1.5984320640563965,
          1.6044507026672363,
          1.6031979322433472,
          1.5939948558807373,
          1.5917463302612305,
          1.591566562652588,
          1.5970852375030518,
          1.6006972789764404,
          1.6045939922332764,
          1.6008046865463257,
          1.6042238473892212,
          1.6032627820968628,
          1.6036376953125,
          1.5367740392684937,
          1.5357718467712402,
          1.5353857278823853,
          1.5975955724716187,
          1.5989952087402344,
          1.6006110906600952,
          1.6074130535125732,
          1.5365251302719116,
          1.537615418434143,
          1.5387760400772095,
          1.614250659942627,
          1.590503454208374,
          1.5968047380447388,
          1.5970282554626465,
          1.5432889461517334,
          1.5341752767562866,
          1.533507227897644,
          1.531760811805725,
          1.6110903024673462,
          1.6804165840148926,
          1.6139742136001587,
          1.6113553047180176,
          1.6110889911651611,
          1.6150627136230469,
          1.5576953887939453,
          1.5587525367736816,
          1.6099193096160889,
          1.6099164485931396,
          1.6121517419815063,
          1.6105695962905884,
          1.5486276149749756,
          1.5550892353057861,
          1.571340799331665,
          1.5473710298538208,
          1.5494818687438965,
          1.623078465461731,
          1.5568393468856812,
          1.6402866840362549,
          1.5567444562911987,
          1.5521758794784546,
          1.5503334999084473,
          1.6157220602035522,
          1.62212073802948,
          1.6243233680725098,
          1.5493488311767578,
          1.6149336099624634,
          1.553878664970398,
          1.6191669702529907,
          1.615415096282959,
          1.616197109222412,
          1.6167335510253906,
          1.6897468566894531,
          1.5580763816833496,
          1.6192797422409058,
          1.5581483840942383,
          1.6141839027404785,
          1.5469834804534912,
          1.6766542196273804,
          1.6138936281204224,
          1.674784779548645,
          1.7406729459762573,
          1.671722173690796,
          1.6127632856369019,
          1.6757045984268188,
          1.6842228174209595,
          1.6096673011779785,
          1.6086970567703247,
          1.6784731149673462,
          1.6138089895248413,
          1.6823192834854126,
          1.6105661392211914,
          1.6081818342208862,
          1.606411099433899,
          1.612664818763733,
          1.552730679512024,
          1.6138296127319336,
          1.6124770641326904,
          1.6091874837875366,
          1.6149163246154785,
          1.6034749746322632,
          1.6039704084396362,
          1.6043318510055542,
          1.6087573766708374,
          1.6020876169204712,
          1.5987411737442017,
          1.667485237121582,
          1.6665899753570557,
          1.6599047183990479,
          1.6619127988815308,
          1.6666215658187866,
          1.6682804822921753,
          1.6676942110061646,
          1.664336919784546,
          1.6619094610214233,
          1.6604254245758057,
          1.593612551689148,
          1.6563891172409058,
          1.6534353494644165,
          1.6570990085601807,
          1.6609165668487549,
          1.6576656103134155,
          1.6587656736373901,
          1.6580640077590942,
          1.6549674272537231,
          1.7259331941604614,
          1.7267920970916748,
          1.6591557264328003,
          1.658116102218628,
          1.654640793800354,
          1.650993824005127
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Loss MULTIMODAL_DEMPSTER"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train accuracy",
         "type": "scatter",
         "y": [
          0.5771169066429138,
          0.5735887289047241,
          0.6209677457809448,
          0.6174395084381104,
          0.647681474685669,
          0.6340726017951965,
          0.6688507795333862,
          0.6844757795333862,
          0.7096773982048035,
          0.6849798560142517,
          0.6910282373428345,
          0.6995967626571655,
          0.6885080933570862,
          0.6985887289047241,
          0.6920362710952759,
          0.6754032373428345,
          0.6819556355476379,
          0.6698588728904724,
          0.6955645084381104,
          0.6779233813285828,
          0.6920362710952759,
          0.6844757795333862,
          0.6799395084381104,
          0.7081653475761414,
          0.6970766186714172,
          0.6743951439857483,
          0.6839717626571655,
          0.7106854915618896,
          0.6900201439857483,
          0.6955645084381104,
          0.6950604915618896,
          0.6738911271095276,
          0.6890121102333069,
          0.6708669066429138,
          0.6824596524238586,
          0.6829637289047241,
          0.6723790168762207,
          0.680443525314331,
          0.6829637289047241,
          0.6905242204666138,
          0.6743951439857483,
          0.6784273982048035,
          0.7011088728904724,
          0.6764112710952759,
          0.7056451439857483,
          0.6995967626571655,
          0.6703628897666931,
          0.7006048560142517,
          0.6955645084381104,
          0.6995967626571655,
          0.6890121102333069,
          0.6950604915618896,
          0.7021169066429138,
          0.680443525314331,
          0.6900201439857483,
          0.6955645084381104,
          0.6809476017951965,
          0.6794354915618896,
          0.6834677457809448,
          0.6910282373428345,
          0.6864919066429138,
          0.6844757795333862,
          0.6890121102333069,
          0.6794354915618896,
          0.6738911271095276,
          0.6733871102333069,
          0.703125,
          0.7016128897666931,
          0.6733871102333069,
          0.6859878897666931,
          0.6754032373428345,
          0.6809476017951965,
          0.6784273982048035,
          0.7036290168762207,
          0.6733871102333069,
          0.6602822542190552,
          0.6592742204666138,
          0.6738911271095276,
          0.663306474685669,
          0.6587701439857483,
          0.671875,
          0.6698588728904724,
          0.6713709831237793,
          0.6869959831237793,
          0.6824596524238586,
          0.6829637289047241,
          0.6844757795333862,
          0.6869959831237793,
          0.7036290168762207,
          0.6668346524238586,
          0.6466733813285828,
          0.647681474685669,
          0.6527217626571655,
          0.6638104915618896,
          0.6481854915618896,
          0.6713709831237793,
          0.6431451439857483,
          0.6431451439857483,
          0.6496976017951965,
          0.6396169066429138,
          0.6668346524238586,
          0.6713709831237793,
          0.6471773982048035,
          0.6617943644523621,
          0.6572580933570862,
          0.6512096524238586,
          0.6512096524238586,
          0.6582661271095276,
          0.6567540168762207,
          0.6517137289047241,
          0.6612903475761414,
          0.6602822542190552,
          0.6527217626571655,
          0.6426411271095276,
          0.6527217626571655,
          0.6466733813285828,
          0.6466733813285828,
          0.647681474685669,
          0.6481854915618896,
          0.6481854915618896,
          0.6597782373428345,
          0.6461693644523621,
          0.6532257795333862,
          0.6607862710952759,
          0.65625,
          0.6557459831237793,
          0.6517137289047241,
          0.6552419066429138,
          0.6668346524238586,
          0.6421371102333069,
          0.6577621102333069,
          0.6592742204666138,
          0.6512096524238586,
          0.6582661271095276,
          0.6552419066429138,
          0.6466733813285828,
          0.6502016186714172,
          0.6698588728904724,
          0.6658266186714172,
          0.6733871102333069,
          0.6552419066429138,
          0.6754032373428345,
          0.6537298560142517,
          0.6486895084381104,
          0.663306474685669,
          0.647681474685669,
          0.6653226017951965,
          0.6471773982048035,
          0.6759072542190552,
          0.6597782373428345,
          0.6622983813285828,
          0.6754032373428345,
          0.6809476017951965,
          0.6456653475761414,
          0.6673387289047241,
          0.6552419066429138,
          0.680443525314331,
          0.6567540168762207,
          0.6774193644523621,
          0.6759072542190552,
          0.6693548560142517,
          0.6456653475761414,
          0.6678427457809448,
          0.6612903475761414,
          0.6759072542190552,
          0.6738911271095276,
          0.6643145084381104,
          0.6708669066429138,
          0.6814516186714172,
          0.65625,
          0.6617943644523621,
          0.6547378897666931,
          0.65625,
          0.6723790168762207,
          0.6527217626571655,
          0.6658266186714172,
          0.6542338728904724,
          0.6748992204666138,
          0.6693548560142517,
          0.6673387289047241,
          0.6461693644523621,
          0.6673387289047241,
          0.6522177457809448,
          0.6441532373428345,
          0.6678427457809448,
          0.649193525314331,
          0.6507056355476379,
          0.6527217626571655,
          0.6446572542190552,
          0.6622983813285828,
          0.6628023982048035,
          0.6673387289047241,
          0.6466733813285828,
          0.6628023982048035,
          0.6517137289047241,
          0.6456653475761414,
          0.6622983813285828,
          0.6572580933570862,
          0.649193525314331,
          0.6446572542190552,
          0.6411290168762207,
          0.6592742204666138,
          0.6668346524238586,
          0.6577621102333069,
          0.6748992204666138,
          0.6703628897666931,
          0.6743951439857483,
          0.6451612710952759,
          0.6547378897666931,
          0.6602822542190552,
          0.6658266186714172,
          0.6628023982048035,
          0.6723790168762207,
          0.6774193644523621,
          0.6708669066429138,
          0.6683467626571655,
          0.6698588728904724,
          0.6602822542190552,
          0.6668346524238586,
          0.6688507795333862,
          0.6703628897666931,
          0.6612903475761414,
          0.6703628897666931,
          0.6799395084381104,
          0.6708669066429138,
          0.6794354915618896,
          0.6653226017951965,
          0.6728830933570862,
          0.6784273982048035,
          0.6693548560142517,
          0.6854838728904724,
          0.664818525314331,
          0.671875,
          0.6502016186714172,
          0.6663306355476379,
          0.6698588728904724,
          0.6733871102333069,
          0.6658266186714172,
          0.6769153475761414,
          0.6592742204666138,
          0.6486895084381104,
          0.680443525314331,
          0.6774193644523621,
          0.6900201439857483,
          0.6824596524238586,
          0.6617943644523621,
          0.6708669066429138,
          0.664818525314331,
          0.6713709831237793,
          0.6703628897666931,
          0.6653226017951965,
          0.6733871102333069,
          0.6748992204666138,
          0.6688507795333862,
          0.6723790168762207,
          0.6829637289047241,
          0.6703628897666931,
          0.6663306355476379,
          0.6910282373428345,
          0.6900201439857483,
          0.6743951439857483,
          0.6900201439857483,
          0.6703628897666931,
          0.6723790168762207,
          0.6693548560142517,
          0.6859878897666931,
          0.6794354915618896,
          0.6572580933570862,
          0.6864919066429138,
          0.6542338728904724,
          0.6602822542190552,
          0.6713709831237793,
          0.6557459831237793,
          0.663306474685669,
          0.6653226017951965,
          0.6733871102333069,
          0.6713709831237793,
          0.6698588728904724,
          0.6658266186714172,
          0.6814516186714172,
          0.6713709831237793,
          0.6653226017951965,
          0.6572580933570862,
          0.6824596524238586,
          0.6774193644523621,
          0.663306474685669,
          0.680443525314331,
          0.6678427457809448,
          0.6572580933570862,
          0.6698588728904724,
          0.6885080933570862,
          0.6864919066429138,
          0.6658266186714172,
          0.6849798560142517,
          0.6683467626571655,
          0.6864919066429138,
          0.6779233813285828,
          0.6754032373428345,
          0.6854838728904724,
          0.6693548560142517,
          0.6844757795333862,
          0.6683467626571655,
          0.6678427457809448,
          0.6512096524238586,
          0.6572580933570862,
          0.6703628897666931,
          0.6688507795333862,
          0.6612903475761414,
          0.6582661271095276,
          0.6759072542190552,
          0.6743951439857483,
          0.6592742204666138,
          0.6774193644523621,
          0.6849798560142517,
          0.6743951439857483,
          0.6814516186714172,
          0.6839717626571655,
          0.6683467626571655,
          0.6844757795333862,
          0.6880040168762207,
          0.6653226017951965,
          0.671875,
          0.6527217626571655,
          0.6738911271095276,
          0.6698588728904724,
          0.6572580933570862,
          0.6653226017951965,
          0.65625,
          0.6759072542190552,
          0.6769153475761414,
          0.6612903475761414,
          0.664818525314331,
          0.6622983813285828,
          0.6572580933570862,
          0.6592742204666138,
          0.6592742204666138,
          0.6517137289047241,
          0.6824596524238586,
          0.6638104915618896,
          0.6587701439857483,
          0.6658266186714172,
          0.6653226017951965,
          0.6859878897666931,
          0.6673387289047241,
          0.6764112710952759,
          0.6759072542190552,
          0.6708669066429138,
          0.6668346524238586,
          0.6794354915618896,
          0.6572580933570862,
          0.6688507795333862,
          0.6532257795333862,
          0.6678427457809448,
          0.6854838728904724,
          0.6754032373428345,
          0.6824596524238586,
          0.6577621102333069,
          0.6708669066429138,
          0.6658266186714172,
          0.664818525314331,
          0.6774193644523621,
          0.6839717626571655,
          0.65625,
          0.6809476017951965,
          0.6572580933570862,
          0.6678427457809448,
          0.6824596524238586,
          0.6638104915618896,
          0.6743951439857483,
          0.6728830933570862,
          0.6935483813285828,
          0.6728830933570862,
          0.680443525314331,
          0.6844757795333862,
          0.6824596524238586,
          0.6824596524238586,
          0.6824596524238586,
          0.6703628897666931,
          0.6769153475761414,
          0.6854838728904724,
          0.6859878897666931,
          0.6708669066429138,
          0.6769153475761414,
          0.6728830933570862,
          0.6723790168762207,
          0.6799395084381104,
          0.6784273982048035,
          0.6713709831237793,
          0.6653226017951965,
          0.6628023982048035,
          0.6824596524238586,
          0.6748992204666138,
          0.6839717626571655,
          0.6829637289047241,
          0.6784273982048035,
          0.6668346524238586,
          0.6829637289047241,
          0.6748992204666138,
          0.6597782373428345,
          0.6885080933570862,
          0.6869959831237793,
          0.6875,
          0.6900201439857483,
          0.6890121102333069,
          0.6769153475761414,
          0.6880040168762207,
          0.6915322542190552,
          0.6678427457809448,
          0.6779233813285828,
          0.6668346524238586,
          0.6673387289047241,
          0.6779233813285828,
          0.6698588728904724,
          0.6673387289047241,
          0.6754032373428345,
          0.6663306355476379,
          0.6864919066429138,
          0.6723790168762207,
          0.6658266186714172,
          0.6607862710952759,
          0.6663306355476379,
          0.6628023982048035,
          0.6759072542190552,
          0.671875,
          0.6678427457809448,
          0.6668346524238586,
          0.6683467626571655,
          0.6602822542190552,
          0.6683467626571655,
          0.6638104915618896,
          0.6738911271095276,
          0.6592742204666138,
          0.6653226017951965,
          0.664818525314331,
          0.6728830933570862,
          0.6653226017951965,
          0.6653226017951965,
          0.6703628897666931,
          0.6708669066429138,
          0.6617943644523621,
          0.6713709831237793,
          0.6628023982048035,
          0.6688507795333862,
          0.6638104915618896,
          0.6774193644523621,
          0.6733871102333069,
          0.6572580933570862,
          0.6683467626571655,
          0.6834677457809448,
          0.6708669066429138,
          0.6743951439857483,
          0.6829637289047241,
          0.6849798560142517,
          0.678931474685669,
          0.6597782373428345,
          0.6799395084381104,
          0.678931474685669,
          0.6643145084381104,
          0.6653226017951965,
          0.6693548560142517,
          0.6849798560142517,
          0.6678427457809448,
          0.6653226017951965,
          0.6759072542190552,
          0.6638104915618896,
          0.6759072542190552,
          0.6905242204666138,
          0.6713709831237793,
          0.6713709831237793,
          0.6779233813285828,
          0.6628023982048035,
          0.6759072542190552,
          0.6854838728904724,
          0.6643145084381104,
          0.6663306355476379,
          0.6673387289047241,
          0.6809476017951965,
          0.6895161271095276,
          0.6759072542190552,
          0.6713709831237793,
          0.6738911271095276,
          0.6708669066429138,
          0.6769153475761414,
          0.6774193644523621,
          0.6875,
          0.6824596524238586,
          0.6738911271095276,
          0.6854838728904724,
          0.6819556355476379,
          0.6743951439857483,
          0.7001007795333862,
          0.6814516186714172,
          0.6885080933570862,
          0.6824596524238586,
          0.6955645084381104,
          0.6703628897666931,
          0.6678427457809448,
          0.6819556355476379,
          0.7016128897666931,
          0.6885080933570862
         ]
        },
        {
         "name": "Validation accuracy",
         "type": "scatter",
         "y": [
          0.4864864945411682,
          0.4864864945411682,
          0.4954954981803894,
          0.522522509098053,
          0.5585585832595825,
          0.5585585832595825,
          0.5585585832595825,
          0.5945945978164673,
          0.6171171069145203,
          0.6126126050949097,
          0.6126126050949097,
          0.6081081032752991,
          0.6261261105537415,
          0.6171171069145203,
          0.6216216087341309,
          0.630630612373352,
          0.6261261105537415,
          0.7117117047309875,
          0.7342342138290405,
          0.7252252101898193,
          0.7027027010917664,
          0.7027027010917664,
          0.7027027010917664,
          0.7027027010917664,
          0.7162162065505981,
          0.7117117047309875,
          0.7117117047309875,
          0.7162162065505981,
          0.7162162065505981,
          0.707207202911377,
          0.707207202911377,
          0.7117117047309875,
          0.7207207083702087,
          0.7162162065505981,
          0.7162162065505981,
          0.7162162065505981,
          0.7207207083702087,
          0.7162162065505981,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7162162065505981,
          0.7207207083702087,
          0.7207207083702087,
          0.7252252101898193,
          0.7207207083702087,
          0.7252252101898193,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7297297120094299,
          0.7297297120094299,
          0.7252252101898193,
          0.7297297120094299,
          0.7297297120094299,
          0.7297297120094299,
          0.7297297120094299,
          0.7252252101898193,
          0.7297297120094299,
          0.7252252101898193,
          0.7297297120094299,
          0.7297297120094299,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7207207083702087,
          0.7117117047309875,
          0.6261261105537415,
          0.7117117047309875,
          0.7117117047309875,
          0.7117117047309875,
          0.7207207083702087,
          0.7252252101898193,
          0.7297297120094299,
          0.7252252101898193,
          0.7297297120094299,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7207207083702087,
          0.7297297120094299,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6216216087341309,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6216216087341309,
          0.6216216087341309,
          0.6216216087341309,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6216216087341309,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6261261105537415,
          0.6216216087341309,
          0.6261261105537415,
          0.6216216087341309,
          0.6216216087341309,
          0.6216216087341309,
          0.6216216087341309,
          0.6216216087341309,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.7027027010917664,
          0.707207202911377,
          0.7117117047309875,
          0.7117117047309875,
          0.7027027010917664,
          0.7027027010917664,
          0.7162162065505981,
          0.7117117047309875,
          0.7117117047309875,
          0.7207207083702087,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7207207083702087,
          0.7027027010917664,
          0.7027027010917664,
          0.7027027010917664,
          0.707207202911377,
          0.7027027010917664,
          0.7027027010917664,
          0.7027027010917664,
          0.7027027010917664,
          0.6171171069145203,
          0.6216216087341309,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.6126126050949097,
          0.6126126050949097,
          0.6171171069145203,
          0.6171171069145203,
          0.6171171069145203,
          0.7027027010917664,
          0.7027027010917664,
          0.6981981992721558,
          0.6981981992721558,
          0.707207202911377,
          0.7117117047309875,
          0.7117117047309875,
          0.707207202911377,
          0.7027027010917664,
          0.707207202911377,
          0.7027027010917664,
          0.6981981992721558,
          0.6981981992721558,
          0.6981981992721558,
          0.7027027010917664,
          0.6981981992721558,
          0.707207202911377,
          0.707207202911377,
          0.707207202911377,
          0.7117117047309875,
          0.7117117047309875,
          0.7117117047309875,
          0.7117117047309875,
          0.7207207083702087,
          0.7252252101898193,
          0.7207207083702087,
          0.7117117047309875,
          0.7117117047309875,
          0.7207207083702087,
          0.707207202911377,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.707207202911377,
          0.7027027010917664,
          0.7117117047309875,
          0.707207202911377,
          0.707207202911377,
          0.7117117047309875,
          0.7117117047309875,
          0.7117117047309875,
          0.7162162065505981,
          0.7162162065505981,
          0.7117117047309875,
          0.7162162065505981,
          0.7117117047309875,
          0.7297297120094299,
          0.7252252101898193,
          0.7342342138290405,
          0.7342342138290405,
          0.7252252101898193,
          0.7252252101898193,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7252252101898193,
          0.7297297120094299,
          0.7252252101898193,
          0.7252252101898193,
          0.7207207083702087,
          0.7297297120094299,
          0.7207207083702087,
          0.7252252101898193,
          0.7207207083702087,
          0.7162162065505981,
          0.7162162065505981,
          0.7162162065505981,
          0.7207207083702087,
          0.7162162065505981,
          0.7207207083702087,
          0.7252252101898193,
          0.7342342138290405,
          0.7297297120094299,
          0.7297297120094299,
          0.7297297120094299,
          0.7297297120094299,
          0.7252252101898193,
          0.7297297120094299,
          0.7207207083702087,
          0.7297297120094299,
          0.7342342138290405,
          0.7297297120094299,
          0.7297297120094299,
          0.7297297120094299,
          0.7297297120094299,
          0.7297297120094299,
          0.7297297120094299,
          0.7252252101898193,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7297297120094299,
          0.7252252101898193,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7162162065505981,
          0.7117117047309875,
          0.7117117047309875,
          0.7252252101898193,
          0.7252252101898193,
          0.7207207083702087,
          0.7252252101898193,
          0.7297297120094299,
          0.7252252101898193,
          0.7297297120094299,
          0.7252252101898193,
          0.7252252101898193,
          0.7207207083702087,
          0.7207207083702087,
          0.7117117047309875,
          0.7117117047309875,
          0.7117117047309875,
          0.7117117047309875,
          0.7162162065505981,
          0.6126126050949097,
          0.6171171069145203,
          0.6216216087341309,
          0.6216216087341309,
          0.6171171069145203,
          0.6171171069145203,
          0.6081081032752991,
          0.6981981992721558,
          0.7027027010917664,
          0.6981981992721558,
          0.6936936974525452,
          0.6936936974525452,
          0.6126126050949097,
          0.6126126050949097,
          0.6981981992721558,
          0.6081081032752991,
          0.6936936974525452,
          0.6981981992721558,
          0.6936936974525452,
          0.6936936974525452,
          0.707207202911377,
          0.707207202911377,
          0.707207202911377,
          0.6981981992721558,
          0.6981981992721558,
          0.7027027010917664,
          0.7027027010917664,
          0.7027027010917664,
          0.707207202911377,
          0.707207202911377,
          0.7117117047309875,
          0.7117117047309875,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7117117047309875,
          0.7162162065505981,
          0.7162162065505981,
          0.7117117047309875,
          0.7162162065505981,
          0.707207202911377,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7162162065505981,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7297297120094299,
          0.7252252101898193,
          0.7207207083702087,
          0.7207207083702087,
          0.7252252101898193,
          0.7162162065505981,
          0.7162162065505981,
          0.7162162065505981,
          0.7207207083702087,
          0.7252252101898193,
          0.7297297120094299,
          0.7252252101898193,
          0.7297297120094299,
          0.7297297120094299,
          0.7297297120094299,
          0.7252252101898193,
          0.7297297120094299,
          0.7252252101898193,
          0.7297297120094299,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7162162065505981,
          0.7207207083702087,
          0.7252252101898193,
          0.7252252101898193,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7162162065505981,
          0.7117117047309875,
          0.707207202911377,
          0.7162162065505981,
          0.7162162065505981,
          0.7117117047309875,
          0.7117117047309875,
          0.7162162065505981,
          0.7162162065505981,
          0.7162162065505981,
          0.7162162065505981,
          0.7162162065505981,
          0.7162162065505981,
          0.7162162065505981,
          0.7162162065505981,
          0.7162162065505981,
          0.7162162065505981,
          0.7027027010917664,
          0.7117117047309875,
          0.7027027010917664,
          0.707207202911377,
          0.707207202911377,
          0.7162162065505981,
          0.7117117047309875,
          0.7117117047309875,
          0.7027027010917664,
          0.7162162065505981,
          0.7117117047309875,
          0.707207202911377,
          0.7027027010917664,
          0.7117117047309875,
          0.7117117047309875,
          0.7117117047309875,
          0.6981981992721558,
          0.7027027010917664,
          0.7117117047309875,
          0.7117117047309875,
          0.7117117047309875,
          0.7162162065505981,
          0.7117117047309875,
          0.7162162065505981,
          0.7207207083702087,
          0.7162162065505981,
          0.7207207083702087,
          0.7207207083702087,
          0.7162162065505981,
          0.7162162065505981,
          0.7207207083702087,
          0.7252252101898193,
          0.7207207083702087,
          0.7162162065505981,
          0.707207202911377,
          0.7117117047309875,
          0.7162162065505981,
          0.7252252101898193,
          0.7117117047309875,
          0.7162162065505981,
          0.7117117047309875,
          0.7117117047309875,
          0.7162162065505981,
          0.7117117047309875,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7252252101898193,
          0.7207207083702087,
          0.7207207083702087,
          0.7162162065505981,
          0.7162162065505981,
          0.7207207083702087,
          0.7207207083702087,
          0.7207207083702087,
          0.7162162065505981,
          0.7162162065505981,
          0.7117117047309875,
          0.7162162065505981,
          0.7162162065505981,
          0.7162162065505981,
          0.7117117047309875,
          0.7117117047309875,
          0.7117117047309875,
          0.7117117047309875,
          0.7117117047309875,
          0.7117117047309875,
          0.707207202911377,
          0.707207202911377,
          0.7162162065505981,
          0.7117117047309875,
          0.7117117047309875,
          0.7162162065505981
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Accuracy MULTIMODAL_DEMPSTER"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Accuracy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train F1 score",
         "type": "scatter",
         "y": [
          0.5763787627220154,
          0.558623194694519,
          0.6175335645675659,
          0.5890302658081055,
          0.6098328828811646,
          0.5856501460075378,
          0.6207579374313354,
          0.6456030607223511,
          0.7009127140045166,
          0.6834650635719299,
          0.67682284116745,
          0.6906766891479492,
          0.6859804391860962,
          0.7342825531959534,
          0.7011752128601074,
          0.6650758981704712,
          0.7041637301445007,
          0.6698019504547119,
          0.720137894153595,
          0.6803283095359802,
          0.7003159523010254,
          0.6941103935241699,
          0.7082302570343018,
          0.6903727054595947,
          0.7097100019454956,
          0.6719869375228882,
          0.6968750357627869,
          0.7150586843490601,
          0.6953437924385071,
          0.7098573446273804,
          0.7090222835540771,
          0.6827552318572998,
          0.7248659133911133,
          0.7060861587524414,
          0.7072679400444031,
          0.7241875529289246,
          0.7171337604522705,
          0.7295382618904114,
          0.7127065658569336,
          0.6950178146362305,
          0.6928061842918396,
          0.7155030965805054,
          0.7264605164527893,
          0.7197610139846802,
          0.751689076423645,
          0.7471781969070435,
          0.7041637301445007,
          0.7352159023284912,
          0.718985915184021,
          0.7125130891799927,
          0.7196440696716309,
          0.7097271084785461,
          0.7351021766662598,
          0.7186590433120728,
          0.7132487893104553,
          0.7403445243835449,
          0.7138649225234985,
          0.7130378484725952,
          0.7278014421463013,
          0.7341201305389404,
          0.7303401231765747,
          0.7126584649085999,
          0.7278014421463013,
          0.7512791156768799,
          0.7169961333274841,
          0.7089318037033081,
          0.7572346329689026,
          0.7501382827758789,
          0.7273541688919067,
          0.7185120582580566,
          0.7292153239250183,
          0.7415438890457153,
          0.7289119958877563,
          0.7385420799255371,
          0.7196412086486816,
          0.7282484769821167,
          0.715105414390564,
          0.7416938543319702,
          0.7281391024589539,
          0.7003353834152222,
          0.7224080562591553,
          0.724797785282135,
          0.7320982217788696,
          0.7372448444366455,
          0.7395168542861938,
          0.7160348892211914,
          0.7377148270606995,
          0.7166436910629272,
          0.7570532560348511,
          0.7278875708580017,
          0.6938158273696899,
          0.7295070886611938,
          0.7509282231330872,
          0.748604416847229,
          0.7128751277923584,
          0.7498646974563599,
          0.7160075902938843,
          0.7226395606994629,
          0.7229914665222168,
          0.7078925371170044,
          0.7590612173080444,
          0.743952751159668,
          0.727913498878479,
          0.7151839733123779,
          0.7259612083435059,
          0.731517493724823,
          0.7299188375473022,
          0.7000252604484558,
          0.7257581949234009,
          0.7265981435775757,
          0.7198784351348877,
          0.7222511172294617,
          0.7184368371963501,
          0.7000740766525269,
          0.7263504266738892,
          0.7014204263687134,
          0.7262231111526489,
          0.7213090062141418,
          0.7042694091796875,
          0.7289896011352539,
          0.7126882076263428,
          0.731758177280426,
          0.7387228012084961,
          0.7376425862312317,
          0.7265981435775757,
          0.7041161060333252,
          0.6976194977760315,
          0.7159629464149475,
          0.7124425172805786,
          0.701705813407898,
          0.694819986820221,
          0.7193603515625,
          0.715309739112854,
          0.7126541137695312,
          0.7199611067771912,
          0.7151627540588379,
          0.7191438674926758,
          0.7156848907470703,
          0.7354254722595215,
          0.7274341583251953,
          0.714832603931427,
          0.7404620051383972,
          0.7246789932250977,
          0.7184368371963501,
          0.7272846102714539,
          0.7021143436431885,
          0.7268249988555908,
          0.7252657413482666,
          0.7130383253097534,
          0.7273908853530884,
          0.6992306709289551,
          0.7253924608230591,
          0.7315621376037598,
          0.7203581929206848,
          0.7404018640518188,
          0.6996595859527588,
          0.7161468267440796,
          0.7189178466796875,
          0.7277971506118774,
          0.7425990104675293,
          0.7300513386726379,
          0.6781340837478638,
          0.7182901501655579,
          0.6974762082099915,
          0.7445331811904907,
          0.7082302570343018,
          0.7221417427062988,
          0.7269188761711121,
          0.7085455656051636,
          0.7111157774925232,
          0.7165212631225586,
          0.7187008261680603,
          0.7196412086486816,
          0.7251366972923279,
          0.7167634963989258,
          0.7022453546524048,
          0.7300513386726379,
          0.7363137006759644,
          0.7341581583023071,
          0.7276785373687744,
          0.7154542803764343,
          0.7066047787666321,
          0.7316389679908752,
          0.7086487412452698,
          0.7536441087722778,
          0.6757211089134216,
          0.717357873916626,
          0.7098104953765869,
          0.7151839733123779,
          0.7175014019012451,
          0.7276785373687744,
          0.7235022783279419,
          0.6988503932952881,
          0.732599139213562,
          0.7234276533126831,
          0.7007541656494141,
          0.741322934627533,
          0.7091905474662781,
          0.7213090062141418,
          0.7102756500244141,
          0.7048976421356201,
          0.7251366972923279,
          0.7215027809143066,
          0.7137668132781982,
          0.713550329208374,
          0.7110643982887268,
          0.7599272131919861,
          0.7042728662490845,
          0.7196673154830933,
          0.6991204023361206,
          0.7249689698219299,
          0.7177809476852417,
          0.7343552708625793,
          0.7278727293014526,
          0.7151839733123779,
          0.7192533612251282,
          0.7140974998474121,
          0.7027967572212219,
          0.7357279062271118,
          0.7456920146942139,
          0.7155030965805054,
          0.7290402054786682,
          0.7131422758102417,
          0.7382315993309021,
          0.7267184257507324,
          0.7304942607879639,
          0.7254612445831299,
          0.7498646974563599,
          0.7174791097640991,
          0.7251366972923279,
          0.7467941045761108,
          0.7015432119369507,
          0.7042728662490845,
          0.7382137179374695,
          0.7216987013816833,
          0.7208279371261597,
          0.7361764907836914,
          0.7348760366439819,
          0.7257581949234009,
          0.7244342565536499,
          0.6939530968666077,
          0.7270998954772949,
          0.7258753776550293,
          0.7466685771942139,
          0.7271802425384521,
          0.7184368371963501,
          0.7246969938278198,
          0.704780101776123,
          0.7251400947570801,
          0.7075324654579163,
          0.7073745727539062,
          0.7082302570343018,
          0.7187008261680603,
          0.7172186374664307,
          0.7105767130851746,
          0.7225174903869629,
          0.7091923356056213,
          0.7302839756011963,
          0.7269518971443176,
          0.7296133637428284,
          0.7163294553756714,
          0.7176824808120728,
          0.7130383253097534,
          0.6961528658866882,
          0.7194650769233704,
          0.7342441082000732,
          0.7307972311973572,
          0.6884983777999878,
          0.7501382827758789,
          0.7049168348312378,
          0.7053660154342651,
          0.703465461730957,
          0.7088520526885986,
          0.7063242197036743,
          0.704780101776123,
          0.7223581671714783,
          0.7195190191268921,
          0.7156848907470703,
          0.701705813407898,
          0.7473286390304565,
          0.7356009483337402,
          0.7230931520462036,
          0.6968086957931519,
          0.7340277433395386,
          0.7102634906768799,
          0.7018445730209351,
          0.7286627292633057,
          0.714725911617279,
          0.6990775465965271,
          0.7007080912590027,
          0.7185866832733154,
          0.7250509262084961,
          0.7193603515625,
          0.7317600846290588,
          0.7061969637870789,
          0.7095705270767212,
          0.729381263256073,
          0.7233221530914307,
          0.7234276533126831,
          0.6944758892059326,
          0.719160795211792,
          0.7108678817749023,
          0.7105280756950378,
          0.700972855091095,
          0.7105280756950378,
          0.7166697978973389,
          0.7040520906448364,
          0.6920778751373291,
          0.69612717628479,
          0.7300646305084229,
          0.7055500149726868,
          0.6913439631462097,
          0.7173826694488525,
          0.7392860651016235,
          0.7269518971443176,
          0.7138649225234985,
          0.7234276533126831,
          0.6968750357627869,
          0.6924030780792236,
          0.7222511172294617,
          0.7225174903869629,
          0.7101455926895142,
          0.7031720876693726,
          0.7213090062141418,
          0.6962343454360962,
          0.7185606956481934,
          0.7309134006500244,
          0.6963812708854675,
          0.7104091644287109,
          0.706655740737915,
          0.7036580443382263,
          0.7210681438446045,
          0.7149187326431274,
          0.7076966166496277,
          0.729223906993866,
          0.7250708341598511,
          0.7099281549453735,
          0.732599139213562,
          0.7161132097244263,
          0.7068820595741272,
          0.694905161857605,
          0.7114853858947754,
          0.738103985786438,
          0.7055105566978455,
          0.7153128385543823,
          0.7318750619888306,
          0.7101455926895142,
          0.723214328289032,
          0.7286627292633057,
          0.7074788808822632,
          0.7149600982666016,
          0.6935200691223145,
          0.729836106300354,
          0.7251585721969604,
          0.7052350044250488,
          0.7326145172119141,
          0.6943367123603821,
          0.7347601652145386,
          0.7114853858947754,
          0.7139993906021118,
          0.7081044316291809,
          0.7241772413253784,
          0.7054944038391113,
          0.7445331811904907,
          0.7064613699913025,
          0.7039072513580322,
          0.7259901762008667,
          0.7022453546524048,
          0.7208389043807983,
          0.7266124486923218,
          0.7304942607879639,
          0.7117847204208374,
          0.7138649225234985,
          0.7209547162055969,
          0.7346623539924622,
          0.6932393312454224,
          0.7278875708580017,
          0.7136727571487427,
          0.7308870553970337,
          0.7180705070495605,
          0.7168395519256592,
          0.7133664488792419,
          0.7357606887817383,
          0.7113049030303955,
          0.7197648286819458,
          0.7413853406906128,
          0.7237300872802734,
          0.7126882076263428,
          0.7191438674926758,
          0.697898268699646,
          0.7359611392021179,
          0.73073410987854,
          0.7257615327835083,
          0.716903030872345,
          0.7395973205566406,
          0.7023738622665405,
          0.7264262437820435,
          0.7111005783081055,
          0.7060861587524414,
          0.7405688762664795,
          0.7160348892211914,
          0.7395973205566406,
          0.7307922840118408,
          0.7267552018165588,
          0.7240663170814514,
          0.7262156009674072,
          0.7407620549201965,
          0.6988503932952881,
          0.7142221331596375,
          0.7139993906021118,
          0.7254612445831299,
          0.745700478553772,
          0.7085069417953491,
          0.7189178466796875,
          0.7133994698524475,
          0.7244961261749268,
          0.7360103130340576,
          0.7223930358886719,
          0.7206334471702576,
          0.7111005783081055,
          0.7114853858947754,
          0.7087336778640747,
          0.7354789972305298,
          0.7394957542419434,
          0.7106016874313354,
          0.7226395606994629,
          0.6946709156036377,
          0.7208389043807983,
          0.7111005783081055,
          0.701027512550354,
          0.7023738622665405,
          0.7257615327835083,
          0.6908236742019653,
          0.7222661375999451,
          0.7235987186431885,
          0.7064111232757568,
          0.7310698628425598,
          0.7153993844985962,
          0.7242859601974487,
          0.7193284034729004,
          0.7311578989028931,
          0.7428901195526123,
          0.7238372564315796,
          0.6976302862167358,
          0.7235307693481445,
          0.7110031843185425,
          0.7278875708580017,
          0.7052350044250488,
          0.7308870553970337,
          0.7100423574447632,
          0.7377607822418213,
          0.7167671918869019,
          0.7173826694488525,
          0.7295382618904114,
          0.7107704877853394,
          0.7305958271026611,
          0.7318750619888306,
          0.7057386040687561,
          0.7106761932373047,
          0.71772301197052,
          0.7450145483016968,
          0.729528546333313,
          0.7112258672714233,
          0.7138288021087646,
          0.7257615327835083,
          0.7240663170814514,
          0.7473071217536926,
          0.7281373739242554,
          0.7127065658569336,
          0.7314874529838562,
          0.7116086483001709,
          0.7059727907180786,
          0.7333548665046692,
          0.7032011151313782,
          0.7301867008209229,
          0.7105767130851746,
          0.7094464302062988,
          0.7300100326538086,
          0.7361764907836914,
          0.7282435894012451,
          0.7180705070495605,
          0.7085455656051636,
          0.7181034684181213,
          0.7277971506118774,
          0.7283270359039307,
          0.7007229328155518,
          0.709510326385498,
          0.7373286485671997,
          0.7353180050849915,
          0.7060632109642029,
          0.7246969938278198,
          0.7286374568939209,
          0.7230418920516968,
          0.7082302570343018,
          0.7440543174743652,
          0.7133664488792419,
          0.7134710550308228,
          0.7131644487380981,
          0.7335901260375977,
          0.7381062507629395
         ]
        },
        {
         "name": "Validation F1 score",
         "type": "scatter",
         "y": [
          0.3272727429866791,
          0.3272727429866791,
          0.3783729076385498,
          0.4912146329879761,
          0.6247724294662476,
          0.651289701461792,
          0.675438642501831,
          0.7286505699157715,
          0.7279411554336548,
          0.7192624807357788,
          0.7270492315292358,
          0.7184354066848755,
          0.7279411554336548,
          0.7184354066848755,
          0.7192624807357788,
          0.7366009950637817,
          0.7366009950637817,
          0.7270492315292358,
          0.7270492315292358,
          0.7270492315292358,
          0.693668782711029,
          0.7206300497055054,
          0.7206300497055054,
          0.7206300497055054,
          0.7115010023117065,
          0.7297078371047974,
          0.7115010023117065,
          0.7018314599990845,
          0.6930709481239319,
          0.702316164970398,
          0.7206300497055054,
          0.7297078371047974,
          0.7286505699157715,
          0.7199022769927979,
          0.7206300497055054,
          0.7105606198310852,
          0.7174180746078491,
          0.7027027010917664,
          0.7098039388656616,
          0.7270492315292358,
          0.7383990287780762,
          0.7383990287780762,
          0.7279411554336548,
          0.725970983505249,
          0.725970983505249,
          0.725970983505249,
          0.725970983505249,
          0.7279411554336548,
          0.7203575968742371,
          0.7203575968742371,
          0.7203575968742371,
          0.7115010023117065,
          0.7115010023117065,
          0.7026061415672302,
          0.7286505699157715,
          0.7286505699157715,
          0.7279411554336548,
          0.7279411554336548,
          0.7115010023117065,
          0.7373745441436768,
          0.7373745441436768,
          0.7270492315292358,
          0.7270492315292358,
          0.7270492315292358,
          0.7270492315292358,
          0.7270492315292358,
          0.7366009950637817,
          0.7366009950637817,
          0.7270492315292358,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7279411554336548,
          0.7270492315292358,
          0.7270492315292358,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7366009950637817,
          0.7270492315292358,
          0.7270492315292358,
          0.7270492315292358,
          0.7270492315292358,
          0.7270492315292358,
          0.7270492315292358,
          0.7366009950637817,
          0.7366009950637817,
          0.7279411554336548,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7270492315292358,
          0.7270492315292358,
          0.7279411554336548,
          0.7366009950637817,
          0.7270492315292358,
          0.7270492315292358,
          0.7270492315292358,
          0.7270492315292358,
          0.7270492315292358,
          0.7270492315292358,
          0.7279411554336548,
          0.7366009950637817,
          0.7184354066848755,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7098039388656616,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7279411554336548,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7279411554336548,
          0.7279411554336548,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7279411554336548,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7279411554336548,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7192624807357788,
          0.7105606198310852,
          0.7105606198310852,
          0.7192624807357788,
          0.7279411554336548,
          0.7105606198310852,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7105606198310852,
          0.7105606198310852,
          0.7279411554336548,
          0.7279411554336548,
          0.7105606198310852,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7286505699157715,
          0.7279411554336548,
          0.7279411554336548,
          0.7286505699157715,
          0.7286505699157715,
          0.7192624807357788,
          0.7373745441436768,
          0.7279411554336548,
          0.7286505699157715,
          0.7192624807357788,
          0.7192624807357788,
          0.7286505699157715,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7098039388656616,
          0.7192624807357788,
          0.7286505699157715,
          0.7286505699157715,
          0.7286505699157715,
          0.7286505699157715,
          0.7192624807357788,
          0.7286505699157715,
          0.7373745441436768,
          0.7373745441436768,
          0.7373745441436768,
          0.7373745441436768,
          0.7373745441436768,
          0.7286505699157715,
          0.7373745441436768,
          0.7286505699157715,
          0.7286505699157715,
          0.7286505699157715,
          0.7373745441436768,
          0.7373745441436768,
          0.7373745441436768,
          0.7286505699157715,
          0.7373745441436768,
          0.7373745441436768,
          0.7286505699157715,
          0.7286505699157715,
          0.7286505699157715,
          0.7286505699157715,
          0.7286505699157715,
          0.7286505699157715,
          0.7192624807357788,
          0.7192624807357788,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7373745441436768,
          0.7373745441436768,
          0.7373745441436768,
          0.7192624807357788,
          0.7286505699157715,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7286505699157715,
          0.7286505699157715,
          0.7286505699157715,
          0.7373745441436768,
          0.7286505699157715,
          0.7286505699157715,
          0.7373745441436768,
          0.7286505699157715,
          0.7199022769927979,
          0.7192624807357788,
          0.7199022769927979,
          0.7105606198310852,
          0.7105606198310852,
          0.7105606198310852,
          0.7192624807357788,
          0.7192624807357788,
          0.7192624807357788,
          0.7279411554336548,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7366009950637817,
          0.7366009950637817,
          0.7279411554336548,
          0.7279411554336548,
          0.7366009950637817,
          0.7279411554336548,
          0.7366009950637817,
          0.7366009950637817,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7279411554336548,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7279411554336548,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7452459335327148,
          0.7452459335327148,
          0.7366009950637817,
          0.7366009950637817,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7366009950637817,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7366009950637817,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7366009950637817,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7366009950637817,
          0.7452459335327148,
          0.7366009950637817,
          0.7452459335327148,
          0.7452459335327148,
          0.7452459335327148,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7452459335327148,
          0.7366009950637817,
          0.7452459335327148,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7279411554336548,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7452459335327148,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7192624807357788,
          0.7279411554336548,
          0.7279411554336548,
          0.7192624807357788,
          0.7192624807357788,
          0.7279411554336548,
          0.7366009950637817,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7452459335327148,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7366009950637817,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7366009950637817,
          0.7373745441436768,
          0.7373745441436768,
          0.7373745441436768,
          0.7279411554336548,
          0.7279411554336548,
          0.7279411554336548,
          0.7373745441436768,
          0.7373745441436768,
          0.7286505699157715,
          0.7286505699157715,
          0.7279411554336548,
          0.7373745441436768,
          0.7373745441436768,
          0.7373745441436768
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Accuracy MULTIMODAL_DEMPSTER"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "F1 score"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'..\\\\..\\\\Outputs\\\\COVID\\\\LR\\\\NuovoTest_Not_Kfold\\\\GRAPHS\\\\training MULTIMODAL_DEMPSTER F1 score.html'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLOT HISTORY\n",
    "plt.clf()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['loss'],\n",
    "                    name='Train loss'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_loss'],\n",
    "                    name='Validation loss'))\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Loss MULTIMODAL_DEMPSTER',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='Loss')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training MULTIMODAL_DEMPSTER loss.html\"))), auto_open=False)\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    # y=history.history['categorical_accuracy'],\n",
    "                    y=history.history['binary_accuracy'],\n",
    "                    name='Train accuracy'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    # y=history.history['val_categorical_accuracy'],\n",
    "                    y=history.history['val_binary_accuracy'],\n",
    "                    name='Validation accuracy'))\n",
    "\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Accuracy MULTIMODAL_DEMPSTER',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='Accuracy')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training MULTIMODAL_DEMPSTER accuracy.html\"))), auto_open=False)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['f1_score'],\n",
    "                    name='Train F1 score'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_f1_score'],\n",
    "                    name='Validation F1 score'))\n",
    "\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Accuracy MULTIMODAL_DEMPSTER',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='F1 score')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training MULTIMODAL_DEMPSTER F1 score.html\"))), auto_open=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "Ground truth of the validation\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "4/4 [==============================] - 1s 29ms/step - loss: 1.6042 - binary_accuracy: 0.7252 - f1_score: 0.7452\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "4/4 [==============================] - 1s 30ms/step\n",
      "[0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 0 0]\n",
      "[0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1]\n",
      "accuracy:  0.7477477477477478\n",
      "log_loss:  2.7784524678844154\n",
      "[[47  7]\n",
      " [21 36]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.69118   0.87037   0.77049        54\n",
      "           1    0.83721   0.63158   0.72000        57\n",
      "\n",
      "    accuracy                        0.74775       111\n",
      "   macro avg    0.76419   0.75097   0.74525       111\n",
      "weighted avg    0.76617   0.74775   0.74456       111\n",
      "\n",
      "{'loss': 1.6041674613952637, 'binary_accuracy': 0.7252252101898193, 'f1_score': 0.7452459335327148}\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "Ground truth of the validation\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "4/4 [==============================] - 1s 29ms/step - loss: 2.0359 - binary_accuracy: 0.7342 - f1_score: 0.7270\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "4/4 [==============================] - 1s 26ms/step\n",
      "[0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0]\n",
      "[0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1]\n",
      "accuracy:  0.7297297297297297\n",
      "log_loss:  2.808898484572424\n",
      "[[46  8]\n",
      " [22 35]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.67647   0.85185   0.75410        54\n",
      "           1    0.81395   0.61404   0.70000        57\n",
      "\n",
      "    accuracy                        0.72973       111\n",
      "   macro avg    0.74521   0.73294   0.72705       111\n",
      "weighted avg    0.74707   0.72973   0.72632       111\n",
      "\n",
      "{'loss': 2.0358903408050537, 'binary_accuracy': 0.7342342138290405, 'f1_score': 0.7270492315292358}\n"
     ]
    }
   ],
   "source": [
    "## EVALUATION\n",
    "gc.collect()\n",
    "\n",
    "bestPt = checkpointPath / Path('MULTIMODAL_DEMPSTER_Classification_F1')\n",
    "model = tf.keras.models.load_model(bestPt)\n",
    "\n",
    "print(\"Ground truth of the validation\")\n",
    "\n",
    "evaluation = model.evaluate(x = [X_val_IMGS,X_val], y = Y_val)\n",
    "prediction = model.predict([X_val_IMGS,X_val])\n",
    "\n",
    "ypreds = np.argmax(prediction, axis=1)\n",
    "yground = np.argmax(Y_val.to_numpy(), axis=1)\n",
    "# yground = np.argmax(targets, axis=1)\n",
    "\n",
    "print(ypreds)\n",
    "print(yground)\n",
    "\n",
    "\n",
    "\n",
    "score = accuracy_score(yground, ypreds)\n",
    "print('accuracy: ', score)\n",
    "\n",
    "loss = log_loss(yground, prediction)\n",
    "print('log_loss: ', loss)\n",
    "\n",
    "# Print the confusion matrix\n",
    "cmatrix = confusion_matrix(yground, ypreds)\n",
    "print(cmatrix)\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "creport = classification_report(yground, ypreds, digits=5)\n",
    "print(creport)\n",
    "\n",
    "\n",
    "evaluation = dict(zip(model.metrics_names,evaluation))\n",
    "\n",
    "# FF_f1_values.append(evaluation['f1_score'])\n",
    "\n",
    "\n",
    "\n",
    "print(evaluation)\n",
    "with open(str(evalspath) + 'MULTIMODAL_DEMPSTER_F1_EVALUATION.txt', 'w') as f:\n",
    "    f.write('MULTIMODAL_DEMPSTER FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    for k, v in evaluation.items():\n",
    "        f.write(str(k) + ' >>> '+ str(v) + '\\n\\n')\n",
    "    f.write('MULTIMODAL_DEMPSTER FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    f.write('SKLEARN METRICS:')\n",
    "    f.write(str('Loss') + ' >>> '+ str(loss) + '\\n\\n')\n",
    "    f.write(str('Accuracy') + ' >>> '+ str(score) + '\\n\\n')\n",
    "    f.write(str('Confusion Matrix') + ' >>> \\n'+ str(cmatrix) + '\\n\\n')\n",
    "    f.write(str('CLASSIFICATION REPORT WITH CLASSES') + ' >>> \\n'+ str(creport) + '\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bestPt = checkpointPath / Path('MULTIMODAL_DEMPSTER_Classification_acc')\n",
    "model = tf.keras.models.load_model(bestPt)\n",
    "\n",
    "print(\"Ground truth of the validation\")\n",
    "\n",
    "evaluation = model.evaluate(x = [X_val_IMGS,X_val], y = Y_val)\n",
    "prediction = model.predict([X_val_IMGS,X_val])\n",
    "\n",
    "ypreds = np.argmax(prediction, axis=1)\n",
    "yground = np.argmax(Y_val.to_numpy(), axis=1)\n",
    "# yground = np.argmax(targets, axis=1)\n",
    "\n",
    "print(ypreds)\n",
    "print(yground)\n",
    "\n",
    "\n",
    "\n",
    "score = accuracy_score(yground, ypreds)\n",
    "print('accuracy: ', score)\n",
    "\n",
    "loss = log_loss(yground, prediction)\n",
    "print('log_loss: ', loss)\n",
    "\n",
    "\n",
    "# Print the confusion matrix\n",
    "cmatrix = confusion_matrix(yground, ypreds)\n",
    "print(cmatrix)\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "creport = classification_report(yground, ypreds, digits=5)\n",
    "print(creport)\n",
    "\n",
    "\n",
    "# FF_accuracy_values.append(score)\n",
    "\n",
    "evaluation = dict(zip(model.metrics_names,evaluation))\n",
    "print(evaluation)\n",
    "with open(str(evalspath) + 'MULTIMODAL_DEMPSTER_acc_EVALUATION.txt', 'w') as f:\n",
    "    f.write('MULTIMODAL_DEMPSTER FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    for k, v in evaluation.items():\n",
    "        f.write(str(k) + ' >>> '+ str(v) + '\\n\\n')\n",
    "    f.write('MULTIMODAL_DEMPSTER FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    f.write('SKLEARN METRICS:')\n",
    "    f.write(str('Loss') + ' >>> '+ str(loss) + '\\n\\n')\n",
    "    f.write(str('Accuracy') + ' >>> '+ str(score) + '\\n\\n')\n",
    "    f.write(str('Confusion Matrix') + ' >>> \\n'+ str(cmatrix) + '\\n\\n')\n",
    "    f.write(str('CLASSIFICATION REPORT WITH CLASSES') + ' >>> \\n'+ str(creport) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRINT DATI SU CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 1, 3)\n",
      "(None, 3, 1)\n",
      "(None, 1, 3)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 112, 112, 16  1216        ['input_2[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 112, 112, 16  64         ['conv2d_16[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)                (None, 112, 112, 16  0           ['batch_normalization_16[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 112, 112, 16  0           ['re_lu_16[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 56, 56, 32)   4640        ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 56, 56, 32)  128         ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_17 (ReLU)                (None, 56, 56, 32)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 56, 56, 32)   0           ['re_lu_17[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 56, 56, 32)   9248        ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 56, 56, 32)   544         ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 56, 56, 32)  128         ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 56, 56, 32)  128         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_18 (ReLU)                (None, 56, 56, 32)   0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_19 (ReLU)                (None, 56, 56, 32)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 56, 56, 32)   0           ['re_lu_18[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 56, 56, 32)   0           ['re_lu_19[0][0]']               \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 56, 56, 32)   0           ['dropout_18[0][0]',             \n",
      "                                                                  'dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " average_pooling2d_5 (AveragePo  (None, 28, 28, 32)  0           ['add_5[0][0]']                  \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 28, 28, 48)   13872       ['average_pooling2d_5[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 28, 28, 48)  192         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_20 (ReLU)                (None, 28, 28, 48)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 28, 28, 48)   0           ['re_lu_20[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 28, 28, 48)   20784       ['dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 28, 28, 48)   1584        ['average_pooling2d_5[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 28, 28, 48)  192         ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 28, 28, 48)  192         ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_21 (ReLU)                (None, 28, 28, 48)   0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_22 (ReLU)                (None, 28, 28, 48)   0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 28, 28, 48)   0           ['re_lu_21[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 28, 28, 48)   0           ['re_lu_22[0][0]']               \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 28, 28, 48)   0           ['dropout_21[0][0]',             \n",
      "                                                                  'dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " average_pooling2d_6 (AveragePo  (None, 14, 14, 48)  0           ['add_6[0][0]']                  \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 14, 14, 64)   27712       ['average_pooling2d_6[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 14, 14, 64)  256         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_23 (ReLU)                (None, 14, 14, 64)   0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 14, 14, 64)   0           ['re_lu_23[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 14, 14, 64)   36928       ['dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 14, 14, 64)   3136        ['average_pooling2d_6[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 14, 14, 64)  256         ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 14, 14, 64)  256         ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_24 (ReLU)                (None, 14, 14, 64)   0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_25 (ReLU)                (None, 14, 14, 64)   0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 14, 14, 64)   0           ['re_lu_24[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 14, 14, 64)   0           ['re_lu_25[0][0]']               \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 14, 14, 64)   0           ['dropout_24[0][0]',             \n",
      "                                                                  'dropout_25[0][0]']             \n",
      "                                                                                                  \n",
      " average_pooling2d_7 (AveragePo  (None, 7, 7, 64)    0           ['add_7[0][0]']                  \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 7, 7, 80)     46160       ['average_pooling2d_7[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 7, 7, 80)    320         ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_26 (ReLU)                (None, 7, 7, 80)     0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 7, 7, 80)     0           ['re_lu_26[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 7, 7, 80)     57680       ['dropout_26[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 7, 7, 80)     5200        ['average_pooling2d_7[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 7, 7, 80)    320         ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 7, 7, 80)    320         ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_27 (ReLU)                (None, 7, 7, 80)     0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_28 (ReLU)                (None, 7, 7, 80)     0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)           (None, 7, 7, 80)     0           ['re_lu_27[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)           (None, 7, 7, 80)     0           ['re_lu_28[0][0]']               \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 7, 7, 80)     0           ['dropout_27[0][0]',             \n",
      "                                                                  'dropout_28[0][0]']             \n",
      "                                                                                                  \n",
      " average_pooling2d_8 (AveragePo  (None, 4, 4, 80)    0           ['add_8[0][0]']                  \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 4, 4, 96)     69216       ['average_pooling2d_8[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 4, 4, 96)    384         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_29 (ReLU)                (None, 4, 4, 96)     0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)           (None, 4, 4, 96)     0           ['re_lu_29[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 4, 4, 96)     83040       ['dropout_29[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 4, 4, 96)     7776        ['average_pooling2d_8[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 4, 4, 96)    384         ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 4, 4, 96)    384         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_30 (ReLU)                (None, 4, 4, 96)     0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_31 (ReLU)                (None, 4, 4, 96)     0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)           (None, 4, 4, 96)     0           ['re_lu_30[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)           (None, 4, 4, 96)     0           ['re_lu_31[0][0]']               \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4, 4, 96)     0           ['dropout_30[0][0]',             \n",
      "                                                                  'dropout_31[0][0]']             \n",
      "                                                                                                  \n",
      " average_pooling2d_9 (AveragePo  (None, 2, 2, 96)    0           ['add_9[0][0]']                  \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " flatten_8 (Flatten)            (None, 1280)         0           ['average_pooling2d_8[0][0]']    \n",
      "                                                                                                  \n",
      " flatten_9 (Flatten)            (None, 384)          0           ['average_pooling2d_9[0][0]']    \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 32)           40992       ['flatten_8[0][0]']              \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 32)           12320       ['flatten_9[0][0]']              \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 8)            264         ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 8)            264         ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 2)            18          ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 2)            18          ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " mass_prob4 (massesFromProbabil  (None, 2)           2           ['dense_26[0][0]']               \n",
      " ities)                                                                                           \n",
      "                                                                                                  \n",
      " mass_prob5 (massesFromProbabil  (None, 2)           2           ['dense_29[0][0]']               \n",
      " ities)                                                                                           \n",
      "                                                                                                  \n",
      " thetaOther4 (Lambda)           (None, 3)            0           ['mass_prob4[0][0]']             \n",
      "                                                                                                  \n",
      " thetaOther5 (Lambda)           (None, 3)            0           ['mass_prob5[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_9 (Reshape)            (None, 1, 3)         0           ['thetaOther4[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_10 (Reshape)           (None, 1, 3)         0           ['thetaOther5[0][0]']            \n",
      "                                                                                                  \n",
      " plausibility4 (Lambda)         (None, 1, 3)         0           ['reshape_9[0][0]']              \n",
      "                                                                                                  \n",
      " beleaf4 (Lambda)               (None, 1, 3)         0           ['reshape_9[0][0]']              \n",
      "                                                                                                  \n",
      " plausibility5 (Lambda)         (None, 1, 3)         0           ['reshape_10[0][0]']             \n",
      "                                                                                                  \n",
      " beleaf5 (Lambda)               (None, 1, 3)         0           ['reshape_10[0][0]']             \n",
      "                                                                                                  \n",
      " uncertainty4 (Subtract)        (None, 1, 3)         0           ['plausibility4[0][0]',          \n",
      "                                                                  'beleaf4[0][0]']                \n",
      "                                                                                                  \n",
      " uncertainty5 (Subtract)        (None, 1, 3)         0           ['plausibility5[0][0]',          \n",
      "                                                                  'beleaf5[0][0]']                \n",
      "                                                                                                  \n",
      " S_X4 (Lambda)                  (None, 1, 3)         0           ['uncertainty4[0][0]',           \n",
      "                                                                  'reshape_9[0][0]']              \n",
      "                                                                                                  \n",
      " S_X5 (Lambda)                  (None, 1, 3)         0           ['uncertainty5[0][0]',           \n",
      "                                                                  'reshape_10[0][0]']             \n",
      "                                                                                                  \n",
      " unc_general4 (Lambda)          (None, 1)            0           ['S_X4[0][0]']                   \n",
      "                                                                                                  \n",
      " unc_general5 (Lambda)          (None, 1)            0           ['S_X5[0][0]']                   \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)          (None, 1, 3)         0           ['reshape_9[0][0]',              \n",
      "                                                                  'unc_general4[0][0]']           \n",
      "                                                                                                  \n",
      " multiply_3 (Multiply)          (None, 1, 3)         0           ['reshape_10[0][0]',             \n",
      "                                                                  'unc_general5[0][0]']           \n",
      "                                                                                                  \n",
      " massCombination (Lambda)       (None, 1, 3)         0           ['multiply_2[0][0]',             \n",
      "                                                                  'multiply_3[0][0]']             \n",
      "                                                                                                  \n",
      " pigProb (Lambda)               (None, 1, 2)         0           ['massCombination[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_11 (Reshape)           (None, 2)            0           ['pigProb[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 446,520\n",
      "Trainable params: 444,568\n",
      "Non-trainable params: 1,952\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## EVALUATION\n",
    "gc.collect()\n",
    "\n",
    "bestPt = checkpointPath / Path('ML_DF_U_Classification_F1')\n",
    "model = tf.keras.models.load_model(bestPt)\n",
    "\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "#                                        outputs=model.get_layer(\"mass_prob\").output)\n",
    "# intermediate_output = intermediate_layer_model(X_val_IMGS)\n",
    "# uncerainties_Lr1 = intermediate_output.numpy()\n",
    "# uncerainties_Lr1.shape\n",
    "# # uncerainties_Lr1_rs = uncerainties_Lr1.reshape(uncerainties_Lr1.shape[0],uncerainties_Lr1.shape[1]*uncerainties_Lr1.shape[2])\n",
    "# uncerainties_Lr1_rs = uncerainties_Lr1.reshape(uncerainties_Lr1.shape[0],uncerainties_Lr1.shape[1])\n",
    "# # uncerainties_Lr1_rs_pd = pd.DataFrame(uncerainties_Lr1_rs, columns = ['LR_1_Un_theta_1_1','LR_1_Un_theta_2_1','LR_1_Un_theta_1_2','LR_1_Un_theta_2_2'])\n",
    "# uncerainties_Lr1_rs_pd = pd.DataFrame(uncerainties_Lr1_rs, columns = ['Theta_1_mass','Theta_2_mass'])\n",
    "# uncerainties_Lr1_rs_pd.head\n",
    "# uncerainties_Lr1_rs_pd.to_csv('../massesFromProb_bel.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "#                                        outputs=model.get_layer(\"thetaOther1\").output)\n",
    "# intermediate_output = intermediate_layer_model(X_val_IMGS)\n",
    "# uncerainties_Lr2 = intermediate_output.numpy()\n",
    "# uncerainties_Lr2.shape\n",
    "# uncerainties_Lr2_rs = uncerainties_Lr2.reshape(uncerainties_Lr2.shape[0],uncerainties_Lr2.shape[1])\n",
    "# tetha_Other = pd.DataFrame(uncerainties_Lr2_rs, columns = ['Theta_1_mass','Theta_2_mass','ThetaOther'])\n",
    "# tetha_Other.head\n",
    "# # tetha_Other.to_csv('../massesFromProb_theta_bel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "#                                        outputs=model.get_layer(\"beleaf1\").output)\n",
    "# intermediate_output = intermediate_layer_model(X_val_IMGS)\n",
    "# uncerainties_Lr2 = intermediate_output.numpy()\n",
    "# uncerainties_Lr2.shape\n",
    "# uncerainties_Lr2_rs = uncerainties_Lr2.reshape(uncerainties_Lr2.shape[0],uncerainties_Lr2.shape[-1])\n",
    "# beleaf = pd.DataFrame(uncerainties_Lr2_rs, columns = ['Theta_1_BEL','Theta_2_BEL','Theta_Other_BEL'])\n",
    "# beleaf.head\n",
    "# # beleaf.to_csv('../massesFromProb_BEL_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "#                                        outputs=model.get_layer(\"plausibility1\").output)\n",
    "# intermediate_output = intermediate_layer_model(X_val_IMGS)\n",
    "# uncerainties_Lr2 = intermediate_output.numpy()\n",
    "# uncerainties_Lr2.shape\n",
    "# uncerainties_Lr2_rs = uncerainties_Lr2.reshape(uncerainties_Lr2.shape[0],uncerainties_Lr2.shape[-1])\n",
    "# plausibility = pd.DataFrame(uncerainties_Lr2_rs, columns = ['Theta_1_PL','Theta_2_PL','Theta_Other_PL'])\n",
    "# plausibility.head\n",
    "# # plausibility.to_csv('../massesFromProb_PL_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "#                                        outputs=model.get_layer(\"uncertainty1\").output)\n",
    "# intermediate_output = intermediate_layer_model(X_val_IMGS)\n",
    "# uncerainties_Lr2 = intermediate_output.numpy()\n",
    "# uncerainties_Lr2.shape\n",
    "# uncerainties_Lr2_rs = uncerainties_Lr2.reshape(uncerainties_Lr2.shape[0],uncerainties_Lr2.shape[-1])\n",
    "# uncertainty_single = pd.DataFrame(uncerainties_Lr2_rs, columns = ['Theta_1_UNC','Theta_2_UNC','Theta_Other_UNC'])\n",
    "# uncertainty_single.head\n",
    "# # uncertainty_single.to_csv('../massesFromProb_UNC_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "#                                        outputs=model.get_layer(\"S_X1\").output)\n",
    "# intermediate_output = intermediate_layer_model(X_val_IMGS)\n",
    "# uncerainties_Lr2 = intermediate_output.numpy()\n",
    "# uncerainties_Lr2.shape\n",
    "# uncerainties_Lr2_rs = uncerainties_Lr2.reshape(uncerainties_Lr2.shape[0],uncerainties_Lr2.shape[-1])\n",
    "# S_X = pd.DataFrame(uncerainties_Lr2_rs, columns = ['S(Theta_1)','S(Theta_2)','S(Theta_Other)'])\n",
    "# S_X.head\n",
    "# # S_X.to_csv('../massesFromProb_S(X)_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "#                                        outputs=model.get_layer(\"unc_general1\").output)\n",
    "# intermediate_output = intermediate_layer_model(X_val_IMGS)\n",
    "# uncerainties_Lr2 = intermediate_output.numpy()\n",
    "# uncerainties_Lr2.shape\n",
    "# uncerainties_Lr2_rs = uncerainties_Lr2.reshape(uncerainties_Lr2.shape[0],uncerainties_Lr2.shape[-1])\n",
    "# Uncertainty_glob = pd.DataFrame(uncerainties_Lr2_rs, columns = ['UNCERTAINTY'])\n",
    "# Uncertainty_glob.head\n",
    "# # Uncertainty_glob.to_csv('../massesFromProb_UNCERTAINTY_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OutputToPrint = pd.concat([tetha_Other, beleaf, plausibility, uncertainty_single, S_X, Uncertainty_glob], axis=1)\n",
    "# OutputToPrint.to_csv('../Print_Outcome_massesAndUnc.csv')\n",
    "# OutputToPrint.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      Theta_1_mass_lr1  Theta_2_mass_lr1  ThetaOther_lr1\n",
       "0            0.000047          0.000396        0.999557\n",
       "1            0.000067          0.000573        0.999360\n",
       "2            0.000065          0.000649        0.999286\n",
       "3            0.000055          0.000548        0.999397\n",
       "4            0.000062          0.000513        0.999425\n",
       "..                ...               ...             ...\n",
       "106          0.000063          0.000528        0.999409\n",
       "107          0.000049          0.000472        0.999479\n",
       "108          0.000083          0.000823        0.999095\n",
       "109          0.000088          0.000917        0.998995\n",
       "110          0.000044          0.000326        0.999630\n",
       "\n",
       "[111 rows x 3 columns]>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "                                       outputs=model.get_layer(\"thetaOther4\").output)\n",
    "intermediate_output = intermediate_layer_model(X_val_IMGS)\n",
    "uncerainties_Lr2 = intermediate_output.numpy()\n",
    "uncerainties_Lr2.shape\n",
    "uncerainties_Lr2_rs = uncerainties_Lr2.reshape(uncerainties_Lr2.shape[0],uncerainties_Lr2.shape[1])\n",
    "tetha_Other_lr1 = pd.DataFrame(uncerainties_Lr2_rs, columns = ['Theta_1_mass_lr1','Theta_2_mass_lr1','ThetaOther_lr1'])\n",
    "tetha_Other_lr1.head\n",
    "# tetha_Other.to_csv('../massesFromProb_theta_bel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      Theta_1_mass_lr2  Theta_2_mass_lr2  ThetaOther_lr2\n",
       "0            0.708461          0.294793       -0.003254\n",
       "1            0.704181          0.289642        0.006176\n",
       "2            0.700775          0.293204        0.006021\n",
       "3            0.702501          0.293230        0.004269\n",
       "4            0.708374          0.292311       -0.000684\n",
       "..                ...               ...             ...\n",
       "106          0.703818          0.293226        0.002956\n",
       "107          0.704219          0.293255        0.002526\n",
       "108          0.702028          0.285260        0.012712\n",
       "109          0.689751          0.291258        0.018990\n",
       "110          0.712756          0.294414       -0.007170\n",
       "\n",
       "[111 rows x 3 columns]>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "                                       outputs=model.get_layer(\"thetaOther5\").output)\n",
    "intermediate_output = intermediate_layer_model(X_val_IMGS)\n",
    "uncerainties_Lr2 = intermediate_output.numpy()\n",
    "uncerainties_Lr2.shape\n",
    "uncerainties_Lr2_rs = uncerainties_Lr2.reshape(uncerainties_Lr2.shape[0],uncerainties_Lr2.shape[1])\n",
    "tetha_Other_lr2 = pd.DataFrame(uncerainties_Lr2_rs, columns = ['Theta_1_mass_lr2','Theta_2_mass_lr2','ThetaOther_lr2'])\n",
    "tetha_Other_lr2.head\n",
    "# tetha_Other.to_csv('../massesFromProb_theta_bel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111, 1, 3)\n",
      "(111, 1, 3)\n",
      "(111, 1, 3)\n",
      "(111, 3, 1)\n",
      "(111, 1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      Theta_1_mass_COMB  Theta_2_mass_COMB  ThetaOther_COMB\n",
       "0            -0.000021           0.001894        -0.000021\n",
       "1             0.000054           0.002542         0.000054\n",
       "2             0.000058           0.002829         0.000058\n",
       "3             0.000036           0.002443         0.000036\n",
       "4            -0.000006           0.002355        -0.000006\n",
       "..                 ...                ...              ...\n",
       "106           0.000024           0.002411         0.000024\n",
       "107           0.000019           0.002155         0.000019\n",
       "108           0.000149           0.003352         0.000149\n",
       "109           0.000244           0.003738         0.000244\n",
       "110          -0.000039           0.001621        -0.000039\n",
       "\n",
       "[111 rows x 3 columns]>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "                                       outputs=model.get_layer(\"massCombination\").output)\n",
    "intermediate_output = intermediate_layer_model(X_val_IMGS)\n",
    "uncerainties_Lr2 = intermediate_output.numpy()\n",
    "uncerainties_Lr2.shape\n",
    "uncerainties_Lr2_rs = uncerainties_Lr2.reshape(uncerainties_Lr2.shape[0],uncerainties_Lr2.shape[-1])\n",
    "tetha_Comb = pd.DataFrame(uncerainties_Lr2_rs, columns = ['Theta_1_mass_COMB','Theta_2_mass_COMB','ThetaOther_COMB'])\n",
    "tetha_Comb.head\n",
    "# tetha_Other.to_csv('../massesFromProb_theta_bel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111, 1, 3)\n",
      "(111, 1, 3)\n",
      "(111, 1, 3)\n",
      "(111, 3, 1)\n",
      "(111, 1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      Theta_1_PigProb  Theta_2_PigProb\n",
       "0           1.000047       -90.585915\n",
       "1           1.000067        46.895359\n",
       "2           1.000065        48.699677\n",
       "3           1.000055        68.685219\n",
       "4           1.000062      -427.043274\n",
       "..               ...              ...\n",
       "106         1.000063        99.181709\n",
       "107         1.000049       116.098061\n",
       "108         1.000083        22.440638\n",
       "109         1.000088        15.337331\n",
       "110         1.000044       -41.061501\n",
       "\n",
       "[111 rows x 2 columns]>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "                                       outputs=model.get_layer(\"pigProb\").output)\n",
    "intermediate_output = intermediate_layer_model(X_val_IMGS)\n",
    "uncerainties_Lr2 = intermediate_output.numpy()\n",
    "uncerainties_Lr2.shape\n",
    "uncerainties_Lr2_rs = uncerainties_Lr2.reshape(uncerainties_Lr2.shape[0],uncerainties_Lr2.shape[-1])\n",
    "pigProb = pd.DataFrame(uncerainties_Lr2_rs, columns = ['Theta_1_PigProb','Theta_2_PigProb'])\n",
    "pigProb.head\n",
    "# tetha_Other.to_csv('../massesFromProb_theta_bel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      Theta_1_mass_lr1  Theta_2_mass_lr1  ThetaOther_lr1  Theta_1_mass_lr2  \\\n",
       "0            0.000047          0.000396        0.999557          0.708461   \n",
       "1            0.000067          0.000573        0.999360          0.704181   \n",
       "2            0.000065          0.000649        0.999286          0.700775   \n",
       "3            0.000055          0.000548        0.999397          0.702501   \n",
       "4            0.000062          0.000513        0.999425          0.708374   \n",
       "..                ...               ...             ...               ...   \n",
       "106          0.000063          0.000528        0.999409          0.703818   \n",
       "107          0.000049          0.000472        0.999479          0.704219   \n",
       "108          0.000083          0.000823        0.999095          0.702028   \n",
       "109          0.000088          0.000917        0.998995          0.689751   \n",
       "110          0.000044          0.000326        0.999630          0.712756   \n",
       "\n",
       "     Theta_2_mass_lr2  ThetaOther_lr2  Theta_1_mass_COMB  Theta_2_mass_COMB  \\\n",
       "0            0.294793       -0.003254          -0.000021           0.001894   \n",
       "1            0.289642        0.006176           0.000054           0.002542   \n",
       "2            0.293204        0.006021           0.000058           0.002829   \n",
       "3            0.293230        0.004269           0.000036           0.002443   \n",
       "4            0.292311       -0.000684          -0.000006           0.002355   \n",
       "..                ...             ...                ...                ...   \n",
       "106          0.293226        0.002956           0.000024           0.002411   \n",
       "107          0.293255        0.002526           0.000019           0.002155   \n",
       "108          0.285260        0.012712           0.000149           0.003352   \n",
       "109          0.291258        0.018990           0.000244           0.003738   \n",
       "110          0.294414       -0.007170          -0.000039           0.001621   \n",
       "\n",
       "     ThetaOther_COMB  Theta_1_PigProb  Theta_2_PigProb  \n",
       "0          -0.000021         1.000047       -90.585915  \n",
       "1           0.000054         1.000067        46.895359  \n",
       "2           0.000058         1.000065        48.699677  \n",
       "3           0.000036         1.000055        68.685219  \n",
       "4          -0.000006         1.000062      -427.043274  \n",
       "..               ...              ...              ...  \n",
       "106         0.000024         1.000063        99.181709  \n",
       "107         0.000019         1.000049       116.098061  \n",
       "108         0.000149         1.000083        22.440638  \n",
       "109         0.000244         1.000088        15.337331  \n",
       "110        -0.000039         1.000044       -41.061501  \n",
       "\n",
       "[111 rows x 11 columns]>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OutputToPrint = pd.concat([tetha_Other_lr1, tetha_Other_lr2, tetha_Comb, pigProb], axis=1)\n",
    "OutputToPrint.to_csv('../massesCombination.csv')\n",
    "OutputToPrint.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST ON CIFAR-10\n",
    "\n",
    "An evidential classifier based on Dempster-Shafer theory and deep learning\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S0925231221004525?casa_token=DZYcVFo3zHgAAAAA:9MY2VhVvBY21Vp1DkyNmm_-TsLS1PstmhtB9IllTaA17UPEd1OzZ7k7P4isGRsoKmryvu810lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to the range [0, 1]\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "# train_labels = tf.keras.utils.to_categorical(train_labels, 10)\n",
    "# test_labels = tf.keras.utils.to_categorical(test_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already loaded the CIFAR-10 dataset as train_images, train_labels, test_images, and test_labels\n",
    "import numpy as np\n",
    "\n",
    "# Define the two classes you want for binary classification\n",
    "class_1 = 0  # For example, \"airplane\"\n",
    "class_2 = 1  # For example, \"automobile\"\n",
    "\n",
    "# Filter the data for the selected classes\n",
    "train_mask = (train_labels == class_1) | (train_labels == class_2)\n",
    "test_mask = (test_labels == class_1) | (test_labels == class_2)\n",
    "\n",
    "# Filter the images and labels for the selected classes\n",
    "\n",
    "# Use numpy.delete() to delete elements based on the mask\n",
    "train_images_binary = np.delete(train_images, np.where(train_mask == False), axis=0)\n",
    "train_labels_binary = np.delete(train_labels, np.where(train_mask == False), axis=0)\n",
    "\n",
    "# train_images_binary = train_images[train_mask]\n",
    "# train_labels_binary = train_labels[train_mask]\n",
    "\n",
    "test_images_binary = np.delete(test_images, np.where(test_mask == False), axis=0)\n",
    "test_labels_binary = np.delete(test_labels, np.where(test_mask == False), axis=0)\n",
    "# test_images_binary = test_images[test_mask]\n",
    "# test_labels_binary = test_labels[test_mask]\n",
    "\n",
    "train_labels_binary = tf.keras.utils.to_categorical(train_labels_binary, 2)\n",
    "test_labels_binary = tf.keras.utils.to_categorical(test_labels_binary, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(train_images_binary.shape)\n",
    "print(train_labels_binary.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False]\n"
     ]
    }
   ],
   "source": [
    "print(train_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR\n",
      "Epoch 1/100\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4289 - binary_accuracy: 0.7941 - f1_score: 0.7941"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 0.4286 - binary_accuracy: 0.7942 - f1_score: 0.7942 - val_loss: 0.2895 - val_binary_accuracy: 0.8765 - val_f1_score: 0.8762\n",
      "Epoch 2/100\n",
      "307/313 [============================>.] - ETA: 0s - loss: 0.2706 - binary_accuracy: 0.8858 - f1_score: 0.8858"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2700 - binary_accuracy: 0.8864 - f1_score: 0.8864 - val_loss: 0.2121 - val_binary_accuracy: 0.9145 - val_f1_score: 0.9144\n",
      "Epoch 3/100\n",
      "306/313 [============================>.] - ETA: 0s - loss: 0.1992 - binary_accuracy: 0.9196 - f1_score: 0.9196"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.1981 - binary_accuracy: 0.9201 - f1_score: 0.9201 - val_loss: 0.1690 - val_binary_accuracy: 0.9310 - val_f1_score: 0.9310\n",
      "Epoch 4/100\n",
      "306/313 [============================>.] - ETA: 0s - loss: 0.1591 - binary_accuracy: 0.9386 - f1_score: 0.9386"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.1588 - binary_accuracy: 0.9384 - f1_score: 0.9384 - val_loss: 0.1453 - val_binary_accuracy: 0.9380 - val_f1_score: 0.9380\n",
      "Epoch 5/100\n",
      "296/313 [===========================>..] - ETA: 0s - loss: 0.1321 - binary_accuracy: 0.9484 - f1_score: 0.9484"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 0.1311 - binary_accuracy: 0.9492 - f1_score: 0.9492 - val_loss: 0.1427 - val_binary_accuracy: 0.9465 - val_f1_score: 0.9465\n",
      "Epoch 6/100\n",
      "310/313 [============================>.] - ETA: 0s - loss: 0.1079 - binary_accuracy: 0.9573 - f1_score: 0.9573"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.1087 - binary_accuracy: 0.9568 - f1_score: 0.9568 - val_loss: 0.1428 - val_binary_accuracy: 0.9475 - val_f1_score: 0.9475\n",
      "Epoch 7/100\n",
      "302/313 [===========================>..] - ETA: 0s - loss: 0.0890 - binary_accuracy: 0.9661 - f1_score: 0.9661"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0902 - binary_accuracy: 0.9656 - f1_score: 0.9656 - val_loss: 0.1366 - val_binary_accuracy: 0.9485 - val_f1_score: 0.9485\n",
      "Epoch 8/100\n",
      "306/313 [============================>.] - ETA: 0s - loss: 0.0742 - binary_accuracy: 0.9729 - f1_score: 0.9729"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0748 - binary_accuracy: 0.9727 - f1_score: 0.9727 - val_loss: 0.1350 - val_binary_accuracy: 0.9505 - val_f1_score: 0.9505\n",
      "Epoch 9/100\n",
      "305/313 [============================>.] - ETA: 0s - loss: 0.0588 - binary_accuracy: 0.9773 - f1_score: 0.9773"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0602 - binary_accuracy: 0.9768 - f1_score: 0.9768 - val_loss: 0.1307 - val_binary_accuracy: 0.9545 - val_f1_score: 0.9545\n",
      "Epoch 10/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0441 - binary_accuracy: 0.9841 - f1_score: 0.9841 - val_loss: 0.1661 - val_binary_accuracy: 0.9460 - val_f1_score: 0.9460\n",
      "Epoch 11/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0376 - binary_accuracy: 0.9848 - f1_score: 0.9848 - val_loss: 0.1648 - val_binary_accuracy: 0.9535 - val_f1_score: 0.9535\n",
      "Epoch 12/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0244 - binary_accuracy: 0.9919 - f1_score: 0.9919 - val_loss: 0.1668 - val_binary_accuracy: 0.9535 - val_f1_score: 0.9535\n",
      "Epoch 13/100\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.0182 - binary_accuracy: 0.9938 - f1_score: 0.9938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0182 - binary_accuracy: 0.9937 - f1_score: 0.9937 - val_loss: 0.1731 - val_binary_accuracy: 0.9580 - val_f1_score: 0.9580\n",
      "Epoch 14/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0146 - binary_accuracy: 0.9959 - f1_score: 0.9959 - val_loss: 0.2017 - val_binary_accuracy: 0.9530 - val_f1_score: 0.9530\n",
      "Epoch 15/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0063 - binary_accuracy: 0.9983 - f1_score: 0.9983 - val_loss: 0.1893 - val_binary_accuracy: 0.9555 - val_f1_score: 0.9555\n",
      "Epoch 16/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0043 - binary_accuracy: 0.9995 - f1_score: 0.9995 - val_loss: 0.2206 - val_binary_accuracy: 0.9540 - val_f1_score: 0.9540\n",
      "Epoch 17/100\n",
      "301/313 [===========================>..] - ETA: 0s - loss: 0.0019 - binary_accuracy: 0.9999 - f1_score: 0.9999"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0019 - binary_accuracy: 0.9999 - f1_score: 0.9999 - val_loss: 0.2164 - val_binary_accuracy: 0.9595 - val_f1_score: 0.9595\n",
      "Epoch 18/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 8.2985e-04 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.2286 - val_binary_accuracy: 0.9565 - val_f1_score: 0.9565\n",
      "Epoch 19/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 4.4512e-04 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.2337 - val_binary_accuracy: 0.9585 - val_f1_score: 0.9585\n",
      "Epoch 20/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 3.4753e-04 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.2401 - val_binary_accuracy: 0.9590 - val_f1_score: 0.9590\n",
      "Epoch 21/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.5702e-04 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.2452 - val_binary_accuracy: 0.9590 - val_f1_score: 0.9590\n",
      "Epoch 22/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.0761e-04 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.2549 - val_binary_accuracy: 0.9585 - val_f1_score: 0.9585\n",
      "Epoch 23/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.7399e-04 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.2575 - val_binary_accuracy: 0.9585 - val_f1_score: 0.9585\n",
      "Epoch 24/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.4696e-04 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.2627 - val_binary_accuracy: 0.9585 - val_f1_score: 0.9585\n",
      "Epoch 25/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2337e-04 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.2676 - val_binary_accuracy: 0.9595 - val_f1_score: 0.9595\n",
      "Epoch 26/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.0804e-04 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.2725 - val_binary_accuracy: 0.9590 - val_f1_score: 0.9590\n",
      "Epoch 27/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 9.5196e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.2764 - val_binary_accuracy: 0.9595 - val_f1_score: 0.9595\n",
      "Epoch 28/100\n",
      "304/313 [============================>.] - ETA: 0s - loss: 8.0904e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 8.0692e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.2803 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 29/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 7.1279e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.2856 - val_binary_accuracy: 0.9585 - val_f1_score: 0.9585\n",
      "Epoch 30/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 6.1576e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.2894 - val_binary_accuracy: 0.9595 - val_f1_score: 0.9595\n",
      "Epoch 31/100\n",
      "308/313 [============================>.] - ETA: 0s - loss: 5.3753e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 5ms/step - loss: 5.3702e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.2927 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 32/100\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 4.7088e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.2965 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 33/100\n",
      "310/313 [============================>.] - ETA: 0s - loss: 4.1866e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_acc\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 4.1554e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3011 - val_binary_accuracy: 0.9610 - val_f1_score: 0.9610\n",
      "Epoch 34/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 3.6738e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3046 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 35/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 3.2779e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3072 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 36/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.8876e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3097 - val_binary_accuracy: 0.9610 - val_f1_score: 0.9610\n",
      "Epoch 37/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.5039e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3148 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 38/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.3020e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3188 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 39/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.9865e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3224 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 40/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.7502e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3256 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 41/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.5653e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3292 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 42/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3869e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3336 - val_binary_accuracy: 0.9610 - val_f1_score: 0.9610\n",
      "Epoch 43/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2530e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3349 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 44/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.1190e-05 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3389 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 45/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 9.8778e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3420 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 46/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 8.8345e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3468 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 47/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 7.8756e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3486 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 48/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 7.1026e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3542 - val_binary_accuracy: 0.9610 - val_f1_score: 0.9610\n",
      "Epoch 49/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 6.1869e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3556 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 50/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 5.6094e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3586 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 51/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 5.0221e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3629 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 52/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 4.5884e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3679 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 53/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 4.0731e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3683 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 54/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 3.6597e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3731 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 55/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 3.2635e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3759 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 56/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.9406e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3780 - val_binary_accuracy: 0.9595 - val_f1_score: 0.9595\n",
      "Epoch 57/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.6186e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3818 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 58/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.3616e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3851 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 59/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.1304e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3893 - val_binary_accuracy: 0.9595 - val_f1_score: 0.9595\n",
      "Epoch 60/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.9215e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3915 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 61/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.7429e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3954 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 62/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.5759e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.3968 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 63/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.4286e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4008 - val_binary_accuracy: 0.9590 - val_f1_score: 0.9590\n",
      "Epoch 64/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2940e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4023 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 65/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.1813e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4061 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 66/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.0803e-06 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4089 - val_binary_accuracy: 0.9595 - val_f1_score: 0.9595\n",
      "Epoch 67/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 9.7733e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4115 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 68/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 8.9885e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4134 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 69/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 8.1892e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4176 - val_binary_accuracy: 0.9595 - val_f1_score: 0.9595\n",
      "Epoch 70/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 7.4912e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4198 - val_binary_accuracy: 0.9590 - val_f1_score: 0.9590\n",
      "Epoch 71/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 6.8561e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4234 - val_binary_accuracy: 0.9595 - val_f1_score: 0.9595\n",
      "Epoch 72/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 6.3044e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4247 - val_binary_accuracy: 0.9595 - val_f1_score: 0.9595\n",
      "Epoch 73/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 5.7969e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4284 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 74/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 5.3253e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4300 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 75/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 4.9966e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4312 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 76/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 4.6031e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4353 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 77/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 4.2712e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4363 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 78/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 4.0124e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4386 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 79/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 3.7193e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4414 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 80/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 3.5027e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4426 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 81/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 3.2921e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4439 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 82/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 3.1235e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4460 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 83/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.9518e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4479 - val_binary_accuracy: 0.9605 - val_f1_score: 0.9605\n",
      "Epoch 84/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.7827e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4493 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 85/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.6486e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4508 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 86/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.5192e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4526 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 87/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.4003e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4543 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 88/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.3282e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4557 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 89/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.2093e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4562 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 90/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.1314e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4577 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 91/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.0519e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4583 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 92/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.9874e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4597 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 93/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.9179e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4610 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 94/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.8553e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4617 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 95/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.8080e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4623 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 96/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.7586e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4633 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 97/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.7160e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4639 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 98/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.6704e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4648 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 99/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.6389e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4653 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n",
      "Epoch 100/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.6042e-07 - binary_accuracy: 1.0000 - f1_score: 1.0000 - val_loss: 0.4660 - val_binary_accuracy: 0.9600 - val_f1_score: 0.9600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plot_model(model, to_file=plotpath / Path('cifar_only.png'), show_shapes=True, show_layer_names=True)\n",
    "\n",
    "\n",
    "initial_learning_rate = 1e-3  # migliore come accuracy massima  0.69\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=100, decay_rate=0.98, staircase=False)\n",
    "\n",
    "model.compile(\n",
    "        # optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-2, decay=1e-4),   #FUSION 2022\n",
    "        # optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-4),   #FUSION 2022\n",
    "        optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "        # optimizer=tf.keras.optimizers.Adam(),\n",
    "        # optimizer=tf.keras.optimizers.Adam(0.0011),\n",
    "        # optimizer = tf.keras.optimizers.Adam(1e-2),\n",
    "        # optimizer=tf.keras.optimizers.Adam(1.1e-3),\n",
    "        # loss=\"categorical_crossentropy\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        # loss=\"mean_squared_logarithmic_error\",\n",
    "        # metrics=[\"accuracy\",dice_coef,\"categorical_accuracy\"],\n",
    "        # metrics=[\"categorical_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=4)],\n",
    "        # metrics=[\"categorical_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=2)],\n",
    "        metrics=[\"binary_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=2)],\n",
    "        # metrics=[\"categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "# CREATE CALLBACKS\n",
    "checkpointClassification = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        checkpointPath / Path('CIFAR_Classification_acc'),\n",
    "                        monitor='val_binary_accuracy', verbose=0, \n",
    "                        # monitor='val_dm_pignistic_16_categorical_accuracy', verbose=0, \n",
    "                        save_best_only=True, mode='max')\n",
    "                                                # CREATE CALLBACKS\n",
    "\n",
    "\n",
    "checkpointClassificationF1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        checkpointPath / Path('CIFAR_Classification_F1'), \n",
    "                        monitor='val_f1_score', verbose=0, \n",
    "                        save_best_only=True, mode='max')\n",
    "                                                # CREATE CALLBACKS\n",
    "\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "\n",
    "    \n",
    "# lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "\n",
    "cb = TimingCallback()\n",
    "\n",
    "print(checkpointPath / Path('CIFAR'))\n",
    "\n",
    "# callbacks_list = [checkpointClassification]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb, schedule_lr,lrate]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb, schedule_lr]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, cb, schedule_lr]\n",
    "callbacks_list = [checkpointClassification, checkpointClassificationF1, cb]\n",
    "# callbacks_list = [checkpointClassification, cb]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb]\n",
    "# callbacks_list = [checkpointClassificationF1, earlyStopping]\n",
    "\n",
    "history = model.fit(\n",
    "        # x = X_train_IMGS,\n",
    "        # trainset,\n",
    "        x = train_images_binary,\n",
    "        # y = [Y_train,Y_tr_unc],\n",
    "        y = train_labels_binary,\n",
    "        # epochs = 1,\n",
    "        epochs = 100,\n",
    "        validation_data = (test_images_binary,test_labels_binary),\n",
    "        # validation_data = [X_val_IMGS,[Y_val,Y_val_unc]],\n",
    "        # validation_data = validset,\n",
    "        callbacks = callbacks_list,    #some problems with DS layers and callbacks\n",
    "        # batch_size=32,\n",
    "        # batch_size=1,\n",
    "        # class_weight=c_w,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train loss",
         "type": "scatter",
         "y": [
          0.4286153316497803,
          0.26998206973075867,
          0.19811560213565826,
          0.1588098704814911,
          0.13107696175575256,
          0.10873738676309586,
          0.09016639739274979,
          0.0747961550951004,
          0.060223858803510666,
          0.044114116579294205,
          0.03760432079434395,
          0.024436132982373238,
          0.01824362576007843,
          0.014621485956013203,
          0.006349888630211353,
          0.004262401722371578,
          0.0018888908671215177,
          0.000829846307169646,
          0.0004451244021765888,
          0.0003475267731118947,
          0.0002570199139881879,
          0.00020760558254551142,
          0.00017398640920873731,
          0.0001469579292461276,
          0.00012336565123405308,
          0.00010803538316395134,
          0.00009519611921859905,
          0.00008069189061643556,
          0.00007127920252969489,
          0.00006157586176414043,
          0.000053701667638961226,
          0.00004708753476734273,
          0.000041554307244950905,
          0.00003673848550533876,
          0.000032778851164039224,
          0.000028876154829049483,
          0.000025038641979335807,
          0.00002301967288076412,
          0.000019864803107338957,
          0.00001750151568558067,
          0.00001565324782859534,
          0.000013868992027710192,
          0.000012529603736766148,
          0.000011190337318112142,
          0.000009877786396828014,
          0.000008834476830088533,
          0.0000078755783761153,
          0.000007102579729689751,
          0.0000061869454839325044,
          0.000005609449090115959,
          0.000005022131972509669,
          0.000004588406227412634,
          0.000004073129730386427,
          0.0000036597441521735163,
          0.000003263458665969665,
          0.0000029405607619992224,
          0.0000026186239665548783,
          0.000002361597353228717,
          0.000002130425400537206,
          0.0000019214642179576913,
          0.0000017429427998649771,
          0.0000015758532754261978,
          0.000001428588802809827,
          0.0000012939580074089463,
          0.0000011813012861239258,
          0.00000108034691947978,
          9.773349347597104e-7,
          8.98851169495174e-7,
          8.189168738681474e-7,
          7.491216820199043e-7,
          6.856135428279231e-7,
          6.304425141934189e-7,
          5.796881623609806e-7,
          5.325283609636244e-7,
          4.996647362531803e-7,
          4.6031010469960165e-7,
          4.2711820924523636e-7,
          4.012420902199665e-7,
          3.7192717172729317e-7,
          3.5026752698286145e-7,
          3.2921448678280285e-7,
          3.123522844816762e-7,
          2.951796034267318e-7,
          2.782705621484638e-7,
          2.64858414311675e-7,
          2.519189763461327e-7,
          2.400309995209682e-7,
          2.328224724124084e-7,
          2.2093045970450476e-7,
          2.131405096861272e-7,
          2.0518810117664543e-7,
          1.9874094903116202e-7,
          1.9179415744474682e-7,
          1.855342333101362e-7,
          1.8080197605740977e-7,
          1.7585794864771742e-7,
          1.7160124343718053e-7,
          1.670393032782158e-7,
          1.638889131072574e-7,
          1.6042037032093504e-7
         ]
        },
        {
         "name": "Validation loss",
         "type": "scatter",
         "y": [
          0.28950998187065125,
          0.2120712399482727,
          0.1690119355916977,
          0.14526835083961487,
          0.14274704456329346,
          0.14276669919490814,
          0.13659244775772095,
          0.1350044459104538,
          0.13071011006832123,
          0.1660803258419037,
          0.16484515368938446,
          0.16678258776664734,
          0.17314854264259338,
          0.20165832340717316,
          0.18934689462184906,
          0.2206462323665619,
          0.21638450026512146,
          0.22862474620342255,
          0.2336507886648178,
          0.24008578062057495,
          0.24523060023784637,
          0.25488659739494324,
          0.25747913122177124,
          0.26270100474357605,
          0.26756373047828674,
          0.27250564098358154,
          0.27643993496894836,
          0.28027454018592834,
          0.2856059670448303,
          0.2894235849380493,
          0.29271599650382996,
          0.296495646238327,
          0.301055371761322,
          0.3045797646045685,
          0.30715054273605347,
          0.30971136689186096,
          0.3148106038570404,
          0.3187929391860962,
          0.322421133518219,
          0.3256397843360901,
          0.32920190691947937,
          0.33364132046699524,
          0.3349290192127228,
          0.33889561891555786,
          0.3419755697250366,
          0.3467811644077301,
          0.3486021161079407,
          0.35419246554374695,
          0.35563942790031433,
          0.35862261056900024,
          0.36294838786125183,
          0.3678573966026306,
          0.36834725737571716,
          0.37307462096214294,
          0.37587910890579224,
          0.377959668636322,
          0.3818165361881256,
          0.38511115312576294,
          0.38934722542762756,
          0.39154303073883057,
          0.39537349343299866,
          0.39683106541633606,
          0.40075406432151794,
          0.4023488163948059,
          0.40614232420921326,
          0.40889960527420044,
          0.4115280210971832,
          0.41335949301719666,
          0.4175783097743988,
          0.4197966456413269,
          0.4234132766723633,
          0.42467600107192993,
          0.4283709228038788,
          0.43001484870910645,
          0.43120643496513367,
          0.4352850317955017,
          0.4362558126449585,
          0.4385918378829956,
          0.441361665725708,
          0.44261837005615234,
          0.4438956081867218,
          0.44596901535987854,
          0.44788235425949097,
          0.44933921098709106,
          0.4508368968963623,
          0.45256200432777405,
          0.45429033041000366,
          0.45567193627357483,
          0.45619165897369385,
          0.45765170454978943,
          0.45830753445625305,
          0.45972681045532227,
          0.46096906065940857,
          0.46169397234916687,
          0.462279349565506,
          0.4633181393146515,
          0.46394529938697815,
          0.46475622057914734,
          0.4652661979198456,
          0.4659962058067322
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Loss CIFAR"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train accuracy",
         "type": "scatter",
         "y": [
          0.7942000031471252,
          0.8863999843597412,
          0.9200999736785889,
          0.9383999705314636,
          0.9491999745368958,
          0.9567999839782715,
          0.9656000137329102,
          0.9726999998092651,
          0.9768000245094299,
          0.9840999841690063,
          0.9847999811172485,
          0.9919000267982483,
          0.9937000274658203,
          0.9958999752998352,
          0.9983000159263611,
          0.9994999766349792,
          0.9998999834060669,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ]
        },
        {
         "name": "Validation accuracy",
         "type": "scatter",
         "y": [
          0.8765000104904175,
          0.9144999980926514,
          0.9309999942779541,
          0.9380000233650208,
          0.9465000033378601,
          0.9474999904632568,
          0.9484999775886536,
          0.9505000114440918,
          0.9545000195503235,
          0.9459999799728394,
          0.953499972820282,
          0.953499972820282,
          0.9580000042915344,
          0.953000009059906,
          0.9555000066757202,
          0.9539999961853027,
          0.9595000147819519,
          0.9564999938011169,
          0.9585000276565552,
          0.9589999914169312,
          0.9589999914169312,
          0.9585000276565552,
          0.9585000276565552,
          0.9585000276565552,
          0.9595000147819519,
          0.9589999914169312,
          0.9595000147819519,
          0.9605000019073486,
          0.9585000276565552,
          0.9595000147819519,
          0.9605000019073486,
          0.9599999785423279,
          0.9610000252723694,
          0.9599999785423279,
          0.9605000019073486,
          0.9610000252723694,
          0.9605000019073486,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279,
          0.9610000252723694,
          0.9605000019073486,
          0.9605000019073486,
          0.9605000019073486,
          0.9599999785423279,
          0.9599999785423279,
          0.9610000252723694,
          0.9605000019073486,
          0.9605000019073486,
          0.9605000019073486,
          0.9605000019073486,
          0.9605000019073486,
          0.9605000019073486,
          0.9605000019073486,
          0.9595000147819519,
          0.9599999785423279,
          0.9599999785423279,
          0.9595000147819519,
          0.9599999785423279,
          0.9605000019073486,
          0.9599999785423279,
          0.9589999914169312,
          0.9599999785423279,
          0.9605000019073486,
          0.9595000147819519,
          0.9605000019073486,
          0.9599999785423279,
          0.9595000147819519,
          0.9589999914169312,
          0.9595000147819519,
          0.9595000147819519,
          0.9605000019073486,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279,
          0.9605000019073486,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279,
          0.9605000019073486,
          0.9605000019073486,
          0.9605000019073486,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279,
          0.9599999785423279
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Accuracy CIFAR"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Accuracy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train F1 score",
         "type": "scatter",
         "y": [
          0.794195294380188,
          0.8863940834999084,
          0.9200993776321411,
          0.9383997917175293,
          0.9491999745368958,
          0.9567999839782715,
          0.9656000137329102,
          0.9726999998092651,
          0.9767999053001404,
          0.9840999841690063,
          0.9847999811172485,
          0.9918999671936035,
          0.9937000274658203,
          0.99590003490448,
          0.9982999563217163,
          0.999500036239624,
          0.9999000430107117,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ]
        },
        {
         "name": "Validation F1 score",
         "type": "scatter",
         "y": [
          0.8761842250823975,
          0.9143524765968323,
          0.9309823513031006,
          0.9379994869232178,
          0.9464940428733826,
          0.9474819898605347,
          0.9484999775886536,
          0.9504979848861694,
          0.9544974565505981,
          0.9459973573684692,
          0.9534958004951477,
          0.9534996747970581,
          0.9580000042915344,
          0.9529998302459717,
          0.9554980993270874,
          0.9539992809295654,
          0.9594999551773071,
          0.9564996957778931,
          0.9584991335868835,
          0.9590000510215759,
          0.9589998722076416,
          0.9585000276565552,
          0.9584997296333313,
          0.9584999084472656,
          0.959499716758728,
          0.9589992761611938,
          0.9594999551773071,
          0.9604999423027039,
          0.9584994912147522,
          0.9594991207122803,
          0.9605000019073486,
          0.9599999785423279,
          0.9610000252723694,
          0.9599997997283936,
          0.9605000019073486,
          0.9610000252723694,
          0.9605000019073486,
          0.9599999189376831,
          0.9599999785423279,
          0.959999680519104,
          0.9599999189376831,
          0.9610000252723694,
          0.9605000019073486,
          0.9605000019073486,
          0.9605000019073486,
          0.9599999189376831,
          0.9599999189376831,
          0.9609999060630798,
          0.9604999423027039,
          0.9605000019073486,
          0.9605000019073486,
          0.9605000019073486,
          0.9605000019073486,
          0.9604999423027039,
          0.9604999423027039,
          0.9594999551773071,
          0.9599997997283936,
          0.9599999189376831,
          0.9594999551773071,
          0.9599999785423279,
          0.9604995250701904,
          0.9599997997283936,
          0.9589998722076416,
          0.9599999189376831,
          0.9604999423027039,
          0.9594999551773071,
          0.9604999423027039,
          0.9599999785423279,
          0.9594999551773071,
          0.9589998722076416,
          0.9594999551773071,
          0.9594999551773071,
          0.9604999423027039,
          0.9599999785423279,
          0.9599999189376831,
          0.9599999189376831,
          0.9604999423027039,
          0.9599999189376831,
          0.9599999189376831,
          0.9599999189376831,
          0.9605000019073486,
          0.9605000019073486,
          0.9605000019073486,
          0.9599999189376831,
          0.9599999189376831,
          0.9599999189376831,
          0.9599999189376831,
          0.9599999189376831,
          0.9599999189376831,
          0.9599999189376831,
          0.9599999189376831,
          0.9599999189376831,
          0.9599999189376831,
          0.9599999189376831,
          0.9599999189376831,
          0.9599999189376831,
          0.9599999189376831,
          0.9599999189376831,
          0.9599999189376831,
          0.9599999189376831
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Accuracy CIFAR"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "F1 score"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'..\\\\..\\\\Outputs\\\\COVID\\\\LR\\\\NuovoTest_Not_Kfold\\\\GRAPHS\\\\training CIFAR F1 score.html'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLOT HISTORY\n",
    "plt.clf()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['loss'],\n",
    "                    name='Train loss'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_loss'],\n",
    "                    name='Validation loss'))\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Loss CIFAR',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='Loss')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training CIFAR loss.html\"))), auto_open=False)\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    # y=history.history['categorical_accuracy'],\n",
    "                    y=history.history['binary_accuracy'],\n",
    "                    name='Train accuracy'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    # y=history.history['val_categorical_accuracy'],\n",
    "                    y=history.history['val_binary_accuracy'],\n",
    "                    name='Validation accuracy'))\n",
    "\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Accuracy CIFAR',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='Accuracy')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training CIFAR accuracy.html\"))), auto_open=False)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['f1_score'],\n",
    "                    name='Train F1 score'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_f1_score'],\n",
    "                    name='Validation F1 score'))\n",
    "\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Accuracy CIFAR',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='F1 score')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training CIFAR F1 score.html\"))), auto_open=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth of the validation\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3011 - binary_accuracy: 0.9610 - f1_score: 0.9610\n",
      "63/63 [==============================] - 0s 1ms/step\n",
      "[0 1 1 ... 0 0 1]\n",
      "[0 1 1 ... 0 0 1]\n",
      "accuracy:  0.961\n",
      "log_loss:  0.5680742741469222\n",
      "[[961  39]\n",
      " [ 39 961]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.96100   0.96100   0.96100      1000\n",
      "           1    0.96100   0.96100   0.96100      1000\n",
      "\n",
      "    accuracy                        0.96100      2000\n",
      "   macro avg    0.96100   0.96100   0.96100      2000\n",
      "weighted avg    0.96100   0.96100   0.96100      2000\n",
      "\n",
      "{'loss': 0.301055371761322, 'binary_accuracy': 0.9610000252723694, 'f1_score': 0.9610000252723694}\n",
      "Ground truth of the validation\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3011 - binary_accuracy: 0.9610 - f1_score: 0.9610\n",
      "63/63 [==============================] - 0s 968us/step\n",
      "[0 1 1 ... 0 0 1]\n",
      "[0 1 1 ... 0 0 1]\n",
      "accuracy:  0.961\n",
      "log_loss:  0.5680742741469222\n",
      "[[961  39]\n",
      " [ 39 961]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.96100   0.96100   0.96100      1000\n",
      "           1    0.96100   0.96100   0.96100      1000\n",
      "\n",
      "    accuracy                        0.96100      2000\n",
      "   macro avg    0.96100   0.96100   0.96100      2000\n",
      "weighted avg    0.96100   0.96100   0.96100      2000\n",
      "\n",
      "{'loss': 0.301055371761322, 'binary_accuracy': 0.9610000252723694, 'f1_score': 0.9610000252723694}\n"
     ]
    }
   ],
   "source": [
    "## EVALUATION\n",
    "gc.collect()\n",
    "\n",
    "bestPt = checkpointPath / Path('CIFAR_Classification_F1')\n",
    "model = tf.keras.models.load_model(bestPt)\n",
    "\n",
    "print(\"Ground truth of the validation\")\n",
    "\n",
    "evaluation = model.evaluate(x = test_images_binary, y = test_labels_binary)\n",
    "prediction = model.predict(test_images_binary)\n",
    "\n",
    "ypreds = np.argmax(prediction, axis=1)\n",
    "yground = np.argmax(test_labels_binary, axis=1)\n",
    "# yground = np.argmax(targets, axis=1)\n",
    "\n",
    "print(ypreds)\n",
    "print(yground)\n",
    "\n",
    "\n",
    "\n",
    "score = accuracy_score(yground, ypreds)\n",
    "print('accuracy: ', score)\n",
    "\n",
    "loss = log_loss(yground, prediction)\n",
    "print('log_loss: ', loss)\n",
    "\n",
    "# Print the confusion matrix\n",
    "cmatrix = confusion_matrix(yground, ypreds)\n",
    "print(cmatrix)\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "creport = classification_report(yground, ypreds, digits=5)\n",
    "print(creport)\n",
    "\n",
    "\n",
    "evaluation = dict(zip(model.metrics_names,evaluation))\n",
    "\n",
    "# FF_f1_values.append(evaluation['f1_score'])\n",
    "\n",
    "\n",
    "\n",
    "print(evaluation)\n",
    "with open(str(evalspath) + 'CIFAR_F1_EVALUATION.txt', 'w') as f:\n",
    "    f.write('CIFAR FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    for k, v in evaluation.items():\n",
    "        f.write(str(k) + ' >>> '+ str(v) + '\\n\\n')\n",
    "    f.write('CIFAR FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    f.write('SKLEARN METRICS:')\n",
    "    f.write(str('Loss') + ' >>> '+ str(loss) + '\\n\\n')\n",
    "    f.write(str('Accuracy') + ' >>> '+ str(score) + '\\n\\n')\n",
    "    f.write(str('Confusion Matrix') + ' >>> \\n'+ str(cmatrix) + '\\n\\n')\n",
    "    f.write(str('CLASSIFICATION REPORT WITH CLASSES') + ' >>> \\n'+ str(creport) + '\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bestPt = checkpointPath / Path('CIFAR_Classification_acc')\n",
    "model = tf.keras.models.load_model(bestPt)\n",
    "\n",
    "print(\"Ground truth of the validation\")\n",
    "\n",
    "evaluation = model.evaluate(x = test_images_binary, y = test_labels_binary)\n",
    "prediction = model.predict(test_images_binary)\n",
    "\n",
    "ypreds = np.argmax(prediction, axis=1)\n",
    "yground = np.argmax(test_labels_binary, axis=1)\n",
    "# yground = np.argmax(targets, axis=1)\n",
    "\n",
    "print(ypreds)\n",
    "print(yground)\n",
    "\n",
    "\n",
    "\n",
    "score = accuracy_score(yground, ypreds)\n",
    "print('accuracy: ', score)\n",
    "\n",
    "loss = log_loss(yground, prediction)\n",
    "print('log_loss: ', loss)\n",
    "\n",
    "\n",
    "# Print the confusion matrix\n",
    "cmatrix = confusion_matrix(yground, ypreds)\n",
    "print(cmatrix)\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "creport = classification_report(yground, ypreds, digits=5)\n",
    "print(creport)\n",
    "\n",
    "\n",
    "# FF_accuracy_values.append(score)\n",
    "\n",
    "evaluation = dict(zip(model.metrics_names,evaluation))\n",
    "print(evaluation)\n",
    "with open(str(evalspath) + 'CIFAR_acc_EVALUATION.txt', 'w') as f:\n",
    "    f.write('CIFAR FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    for k, v in evaluation.items():\n",
    "        f.write(str(k) + ' >>> '+ str(v) + '\\n\\n')\n",
    "    f.write('CIFAR FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    f.write('SKLEARN METRICS:')\n",
    "    f.write(str('Loss') + ' >>> '+ str(loss) + '\\n\\n')\n",
    "    f.write(str('Accuracy') + ' >>> '+ str(score) + '\\n\\n')\n",
    "    f.write(str('Confusion Matrix') + ' >>> \\n'+ str(cmatrix) + '\\n\\n')\n",
    "    f.write(str('CLASSIFICATION REPORT WITH CLASSES') + ' >>> \\n'+ str(creport) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dEMPSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decision_CIFAR_DEMPSTER( CNN, uncertainty = False):\n",
    "\n",
    "    cnnModel = CNN.output\n",
    "\n",
    "    massesCNN, uncCNN = MassesWithUncertainty(cnnModel,1)\n",
    "\n",
    "    if uncertainty:\n",
    "        massesCNN = Multiply()([massesCNN,uncCNN])\n",
    "\n",
    "\n",
    "    pigProb = Lambda(mass_to_pignistic, name='pigProb')(massesCNN)\n",
    "    pigProb = Reshape((pigProb.shape[-1],))(pigProb)\n",
    "    output = pigProb\n",
    "\n",
    "    model = Model(inputs=CNN.input, outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model = create_decision_CIFAR_DEMPSTER(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS\n",
      "Epoch 1/200\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "307/313 [============================>.] - ETA: 0s - loss: 0.5981 - binary_accuracy: 0.7338 - f1_score: 0.7496(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.5996 - binary_accuracy: 0.7344 - f1_score: 0.7510 - val_loss: 0.5801 - val_binary_accuracy: 0.7925 - val_f1_score: 0.8050\n",
      "Epoch 2/200\n",
      "301/313 [===========================>..] - ETA: 0s - loss: 0.5114 - binary_accuracy: 0.7807 - f1_score: 0.8012(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.5207 - binary_accuracy: 0.7828 - f1_score: 0.8025 - val_loss: 0.4351 - val_binary_accuracy: 0.8605 - val_f1_score: 0.8606\n",
      "Epoch 3/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4978 - binary_accuracy: 0.8067 - f1_score: 0.8280 - val_loss: 0.5781 - val_binary_accuracy: 0.8420 - val_f1_score: 0.8485\n",
      "Epoch 4/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5176 - binary_accuracy: 0.7837 - f1_score: 0.8000 - val_loss: 0.5259 - val_binary_accuracy: 0.7200 - val_f1_score: 0.7100\n",
      "Epoch 5/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.6562 - binary_accuracy: 0.7036 - f1_score: 0.7539 - val_loss: 0.5351 - val_binary_accuracy: 0.7205 - val_f1_score: 0.7848\n",
      "Epoch 6/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4802 - binary_accuracy: 0.7955 - f1_score: 0.8267 - val_loss: 0.5033 - val_binary_accuracy: 0.7770 - val_f1_score: 0.7792\n",
      "Epoch 7/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5385 - binary_accuracy: 0.7930 - f1_score: 0.8191 - val_loss: 0.4683 - val_binary_accuracy: 0.8027 - val_f1_score: 0.8232\n",
      "Epoch 8/200\n",
      "306/313 [============================>.] - ETA: 0s - loss: 0.4364 - binary_accuracy: 0.8322 - f1_score: 0.8486(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.4368 - binary_accuracy: 0.8321 - f1_score: 0.8486 - val_loss: 0.3549 - val_binary_accuracy: 0.8535 - val_f1_score: 0.8765\n",
      "Epoch 9/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5045 - binary_accuracy: 0.7934 - f1_score: 0.8067 - val_loss: 0.4169 - val_binary_accuracy: 0.8363 - val_f1_score: 0.8519\n",
      "Epoch 10/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3981 - binary_accuracy: 0.8466 - f1_score: 0.8569(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3981 - binary_accuracy: 0.8466 - f1_score: 0.8569 - val_loss: 0.3475 - val_binary_accuracy: 0.8740 - val_f1_score: 0.8855\n",
      "Epoch 11/200\n",
      "306/313 [============================>.] - ETA: 0s - loss: 0.3647 - binary_accuracy: 0.8701 - f1_score: 0.8807(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3647 - binary_accuracy: 0.8708 - f1_score: 0.8810 - val_loss: 0.3062 - val_binary_accuracy: 0.8815 - val_f1_score: 0.8900\n",
      "Epoch 12/200\n",
      "304/313 [============================>.] - ETA: 0s - loss: 0.3448 - binary_accuracy: 0.8788 - f1_score: 0.8874(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3432 - binary_accuracy: 0.8795 - f1_score: 0.8877 - val_loss: 0.2836 - val_binary_accuracy: 0.9005 - val_f1_score: 0.9105\n",
      "Epoch 13/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3265 - binary_accuracy: 0.8889 - f1_score: 0.8986(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3264 - binary_accuracy: 0.8888 - f1_score: 0.8986 - val_loss: 0.2733 - val_binary_accuracy: 0.9032 - val_f1_score: 0.9120\n",
      "Epoch 14/200\n",
      "304/313 [============================>.] - ETA: 0s - loss: 0.3180 - binary_accuracy: 0.8811 - f1_score: 0.8961(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3155 - binary_accuracy: 0.8820 - f1_score: 0.8965 - val_loss: 0.2655 - val_binary_accuracy: 0.9075 - val_f1_score: 0.9205\n",
      "Epoch 15/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3311 - binary_accuracy: 0.8717 - f1_score: 0.8900 - val_loss: 0.2968 - val_binary_accuracy: 0.8970 - val_f1_score: 0.9078\n",
      "Epoch 16/200\n",
      "304/313 [============================>.] - ETA: 0s - loss: 0.2791 - binary_accuracy: 0.8954 - f1_score: 0.9070(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2797 - binary_accuracy: 0.8954 - f1_score: 0.9068 - val_loss: 0.2488 - val_binary_accuracy: 0.9095 - val_f1_score: 0.9190\n",
      "Epoch 17/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3573 - binary_accuracy: 0.8522 - f1_score: 0.8631 - val_loss: 0.2867 - val_binary_accuracy: 0.8842 - val_f1_score: 0.9000\n",
      "Epoch 18/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2812 - binary_accuracy: 0.8945 - f1_score: 0.9056 - val_loss: 0.2698 - val_binary_accuracy: 0.9013 - val_f1_score: 0.9135\n",
      "Epoch 19/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2737 - binary_accuracy: 0.9011 - f1_score: 0.9099 - val_loss: 0.2760 - val_binary_accuracy: 0.9062 - val_f1_score: 0.9175\n",
      "Epoch 20/200\n",
      "307/313 [============================>.] - ETA: 0s - loss: 0.2655 - binary_accuracy: 0.9036 - f1_score: 0.9116(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2667 - binary_accuracy: 0.9034 - f1_score: 0.9115 - val_loss: 0.2767 - val_binary_accuracy: 0.9090 - val_f1_score: 0.9205\n",
      "Epoch 21/200\n",
      "300/313 [===========================>..] - ETA: 0s - loss: 0.2586 - binary_accuracy: 0.9054 - f1_score: 0.9140(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.2587 - binary_accuracy: 0.9052 - f1_score: 0.9140 - val_loss: 0.2728 - val_binary_accuracy: 0.9105 - val_f1_score: 0.9225\n",
      "Epoch 22/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2615 - binary_accuracy: 0.9036 - f1_score: 0.9104 - val_loss: 0.2579 - val_binary_accuracy: 0.9103 - val_f1_score: 0.9190\n",
      "Epoch 23/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2655 - binary_accuracy: 0.9040 - f1_score: 0.9134 - val_loss: 0.2815 - val_binary_accuracy: 0.9085 - val_f1_score: 0.9200\n",
      "Epoch 24/200\n",
      "310/313 [============================>.] - ETA: 0s - loss: 0.2547 - binary_accuracy: 0.9060 - f1_score: 0.9169(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 8ms/step - loss: 0.2549 - binary_accuracy: 0.9060 - f1_score: 0.9173 - val_loss: 0.2624 - val_binary_accuracy: 0.9110 - val_f1_score: 0.9245\n",
      "Epoch 25/200\n",
      "307/313 [============================>.] - ETA: 0s - loss: 0.2468 - binary_accuracy: 0.9091 - f1_score: 0.9152(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2452 - binary_accuracy: 0.9096 - f1_score: 0.9153 - val_loss: 0.2593 - val_binary_accuracy: 0.9130 - val_f1_score: 0.9240\n",
      "Epoch 26/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 0.2422 - binary_accuracy: 0.9097 - f1_score: 0.9167(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.2417 - binary_accuracy: 0.9099 - f1_score: 0.9170 - val_loss: 0.2566 - val_binary_accuracy: 0.9147 - val_f1_score: 0.9260\n",
      "Epoch 27/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2411 - binary_accuracy: 0.9089 - f1_score: 0.9171(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2412 - binary_accuracy: 0.9089 - f1_score: 0.9171 - val_loss: 0.2405 - val_binary_accuracy: 0.9143 - val_f1_score: 0.9290\n",
      "Epoch 28/200\n",
      "306/313 [============================>.] - ETA: 0s - loss: 0.2413 - binary_accuracy: 0.9106 - f1_score: 0.9179(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2396 - binary_accuracy: 0.9111 - f1_score: 0.9181 - val_loss: 0.2502 - val_binary_accuracy: 0.9162 - val_f1_score: 0.9275\n",
      "Epoch 29/200\n",
      "308/313 [============================>.] - ETA: 0s - loss: 0.2391 - binary_accuracy: 0.9113 - f1_score: 0.9195(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_F1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2389 - binary_accuracy: 0.9113 - f1_score: 0.9196 - val_loss: 0.2402 - val_binary_accuracy: 0.9153 - val_f1_score: 0.9305\n",
      "Epoch 30/200\n",
      "302/313 [===========================>..] - ETA: 0s - loss: 0.2358 - binary_accuracy: 0.9121 - f1_score: 0.9179(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2357 - binary_accuracy: 0.9126 - f1_score: 0.9185 - val_loss: 0.2439 - val_binary_accuracy: 0.9185 - val_f1_score: 0.9295\n",
      "Epoch 31/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2338 - binary_accuracy: 0.9136 - f1_score: 0.9194 - val_loss: 0.2406 - val_binary_accuracy: 0.9178 - val_f1_score: 0.9300\n",
      "Epoch 32/200\n",
      "301/313 [===========================>..] - ETA: 0s - loss: 0.2322 - binary_accuracy: 0.9137 - f1_score: 0.9204(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2324 - binary_accuracy: 0.9137 - f1_score: 0.9204 - val_loss: 0.2417 - val_binary_accuracy: 0.9193 - val_f1_score: 0.9300\n",
      "Epoch 33/200\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.2313 - binary_accuracy: 0.9147 - f1_score: 0.9199 - val_loss: 0.2439 - val_binary_accuracy: 0.9190 - val_f1_score: 0.9305\n",
      "Epoch 34/200\n",
      "301/313 [===========================>..] - ETA: 0s - loss: 0.2294 - binary_accuracy: 0.9153 - f1_score: 0.9205(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2301 - binary_accuracy: 0.9154 - f1_score: 0.9208 - val_loss: 0.2401 - val_binary_accuracy: 0.9200 - val_f1_score: 0.9305\n",
      "Epoch 35/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 0.2286 - binary_accuracy: 0.9162 - f1_score: 0.9206(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2287 - binary_accuracy: 0.9161 - f1_score: 0.9205 - val_loss: 0.2451 - val_binary_accuracy: 0.9208 - val_f1_score: 0.9295\n",
      "Epoch 36/200\n",
      "309/313 [============================>.] - ETA: 0s - loss: 0.2287 - binary_accuracy: 0.9163 - f1_score: 0.9212(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2278 - binary_accuracy: 0.9165 - f1_score: 0.9211 - val_loss: 0.2411 - val_binary_accuracy: 0.9215 - val_f1_score: 0.9305\n",
      "Epoch 37/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2300 - binary_accuracy: 0.9155 - f1_score: 0.9211 - val_loss: 0.2414 - val_binary_accuracy: 0.9208 - val_f1_score: 0.9280\n",
      "Epoch 38/200\n",
      "310/313 [============================>.] - ETA: 0s - loss: 0.2236 - binary_accuracy: 0.9165 - f1_score: 0.9221(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2242 - binary_accuracy: 0.9162 - f1_score: 0.9220 - val_loss: 0.2428 - val_binary_accuracy: 0.9218 - val_f1_score: 0.9290\n",
      "Epoch 39/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2233 - binary_accuracy: 0.9162 - f1_score: 0.9234 - val_loss: 0.2426 - val_binary_accuracy: 0.9218 - val_f1_score: 0.9295\n",
      "Epoch 40/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2227 - binary_accuracy: 0.9167 - f1_score: 0.9231(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2226 - binary_accuracy: 0.9168 - f1_score: 0.9232 - val_loss: 0.2415 - val_binary_accuracy: 0.9222 - val_f1_score: 0.9295\n",
      "Epoch 41/200\n",
      "301/313 [===========================>..] - ETA: 0s - loss: 0.2247 - binary_accuracy: 0.9172 - f1_score: 0.9235(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2219 - binary_accuracy: 0.9178 - f1_score: 0.9238 - val_loss: 0.2411 - val_binary_accuracy: 0.9227 - val_f1_score: 0.9300\n",
      "Epoch 42/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 0.2214 - binary_accuracy: 0.9179 - f1_score: 0.9241(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2213 - binary_accuracy: 0.9179 - f1_score: 0.9240 - val_loss: 0.2403 - val_binary_accuracy: 0.9233 - val_f1_score: 0.9300\n",
      "Epoch 43/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2207 - binary_accuracy: 0.9189 - f1_score: 0.9240 - val_loss: 0.2398 - val_binary_accuracy: 0.9230 - val_f1_score: 0.9300\n",
      "Epoch 44/200\n",
      "308/313 [============================>.] - ETA: 0s - loss: 0.2208 - binary_accuracy: 0.9189 - f1_score: 0.9241(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2201 - binary_accuracy: 0.9189 - f1_score: 0.9240 - val_loss: 0.2390 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9285\n",
      "Epoch 45/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2196 - binary_accuracy: 0.9194 - f1_score: 0.9245 - val_loss: 0.2386 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9300\n",
      "Epoch 46/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2190 - binary_accuracy: 0.9194 - f1_score: 0.9244 - val_loss: 0.2390 - val_binary_accuracy: 0.9230 - val_f1_score: 0.9300\n",
      "Epoch 47/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2186 - binary_accuracy: 0.9197 - f1_score: 0.9239 - val_loss: 0.2381 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 48/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2182 - binary_accuracy: 0.9199 - f1_score: 0.9246 - val_loss: 0.2377 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 49/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2177 - binary_accuracy: 0.9201 - f1_score: 0.9245 - val_loss: 0.2375 - val_binary_accuracy: 0.9235 - val_f1_score: 0.9300\n",
      "Epoch 50/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2173 - binary_accuracy: 0.9202 - f1_score: 0.9247 - val_loss: 0.2371 - val_binary_accuracy: 0.9237 - val_f1_score: 0.9300\n",
      "Epoch 51/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2170 - binary_accuracy: 0.9205 - f1_score: 0.9251 - val_loss: 0.2370 - val_binary_accuracy: 0.9237 - val_f1_score: 0.9300\n",
      "Epoch 52/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2167 - binary_accuracy: 0.9207 - f1_score: 0.9251 - val_loss: 0.2366 - val_binary_accuracy: 0.9237 - val_f1_score: 0.9300\n",
      "Epoch 53/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2163 - binary_accuracy: 0.9208 - f1_score: 0.9255 - val_loss: 0.2394 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9285\n",
      "Epoch 54/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2161 - binary_accuracy: 0.9208 - f1_score: 0.9246 - val_loss: 0.2361 - val_binary_accuracy: 0.9237 - val_f1_score: 0.9300\n",
      "Epoch 55/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2159 - binary_accuracy: 0.9211 - f1_score: 0.9254 - val_loss: 0.2360 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9300\n",
      "Epoch 56/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2157 - binary_accuracy: 0.9210 - f1_score: 0.9254 - val_loss: 0.2361 - val_binary_accuracy: 0.9235 - val_f1_score: 0.9300\n",
      "Epoch 57/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2155 - binary_accuracy: 0.9214 - f1_score: 0.9256 - val_loss: 0.2384 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9290\n",
      "Epoch 58/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2153 - binary_accuracy: 0.9212 - f1_score: 0.9254 - val_loss: 0.2382 - val_binary_accuracy: 0.9237 - val_f1_score: 0.9295\n",
      "Epoch 59/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2152 - binary_accuracy: 0.9212 - f1_score: 0.9256 - val_loss: 0.2381 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9295\n",
      "Epoch 60/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2150 - binary_accuracy: 0.9211 - f1_score: 0.9254 - val_loss: 0.2379 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9300\n",
      "Epoch 61/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2149 - binary_accuracy: 0.9214 - f1_score: 0.9256 - val_loss: 0.2378 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9300\n",
      "Epoch 62/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2148 - binary_accuracy: 0.9213 - f1_score: 0.9253 - val_loss: 0.2377 - val_binary_accuracy: 0.9237 - val_f1_score: 0.9305\n",
      "Epoch 63/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2147 - binary_accuracy: 0.9212 - f1_score: 0.9257 - val_loss: 0.2377 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9300\n",
      "Epoch 64/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2146 - binary_accuracy: 0.9212 - f1_score: 0.9257 - val_loss: 0.2376 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9300\n",
      "Epoch 65/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2146 - binary_accuracy: 0.9214 - f1_score: 0.9256 - val_loss: 0.2375 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9300\n",
      "Epoch 66/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2145 - binary_accuracy: 0.9213 - f1_score: 0.9257 - val_loss: 0.2375 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9300\n",
      "Epoch 67/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2144 - binary_accuracy: 0.9212 - f1_score: 0.9257 - val_loss: 0.2375 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9300\n",
      "Epoch 68/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2144 - binary_accuracy: 0.9212 - f1_score: 0.9258 - val_loss: 0.2374 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9300\n",
      "Epoch 69/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2144 - binary_accuracy: 0.9214 - f1_score: 0.9258 - val_loss: 0.2374 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9300\n",
      "Epoch 70/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2143 - binary_accuracy: 0.9213 - f1_score: 0.9258 - val_loss: 0.2373 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9300\n",
      "Epoch 71/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2143 - binary_accuracy: 0.9213 - f1_score: 0.9258 - val_loss: 0.2373 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9300\n",
      "Epoch 72/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2143 - binary_accuracy: 0.9214 - f1_score: 0.9258 - val_loss: 0.2373 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9300\n",
      "Epoch 73/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2142 - binary_accuracy: 0.9214 - f1_score: 0.9258 - val_loss: 0.2373 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9300\n",
      "Epoch 74/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2142 - binary_accuracy: 0.9214 - f1_score: 0.9258 - val_loss: 0.2373 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9300\n",
      "Epoch 75/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2142 - binary_accuracy: 0.9214 - f1_score: 0.9258 - val_loss: 0.2372 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9300\n",
      "Epoch 76/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2142 - binary_accuracy: 0.9215 - f1_score: 0.9258 - val_loss: 0.2372 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9305\n",
      "Epoch 77/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2142 - binary_accuracy: 0.9214 - f1_score: 0.9258 - val_loss: 0.2372 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9305\n",
      "Epoch 78/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2372 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9305\n",
      "Epoch 79/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2372 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9305\n",
      "Epoch 80/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2372 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9305\n",
      "Epoch 81/200\n",
      "307/313 [============================>.] - ETA: 0s - loss: 0.2155 - binary_accuracy: 0.9210 - f1_score: 0.9256(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "(1, 2)\n",
      "(None, 2)\n",
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ..\\..\\Outputs\\COVID\\LR\\NuovoTest_Not_Kfold\\CHECKPOINTS\\CIFAR_DS_Classification_acc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2372 - val_binary_accuracy: 0.9245 - val_f1_score: 0.9305\n",
      "Epoch 82/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2372 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9305\n",
      "Epoch 83/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2372 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9305\n",
      "Epoch 84/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 85/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 86/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 87/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9305\n",
      "Epoch 88/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9305\n",
      "Epoch 89/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9305\n",
      "Epoch 90/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9305\n",
      "Epoch 91/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9245 - val_f1_score: 0.9305\n",
      "Epoch 92/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9245 - val_f1_score: 0.9305\n",
      "Epoch 93/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9245 - val_f1_score: 0.9305\n",
      "Epoch 94/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9305\n",
      "Epoch 95/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9305\n",
      "Epoch 96/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9305\n",
      "Epoch 97/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9305\n",
      "Epoch 98/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9243 - val_f1_score: 0.9305\n",
      "Epoch 99/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 100/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 101/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 102/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 103/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 104/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 105/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 106/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 107/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 108/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 109/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 110/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 111/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 112/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 113/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 114/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 115/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 116/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 117/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 118/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 119/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 120/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 121/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 122/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 123/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 124/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 125/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 126/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 127/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 128/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 129/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9259 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 130/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 131/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 132/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 133/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 134/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 135/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 136/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 137/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 138/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 139/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 140/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 141/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 142/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 143/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 144/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 145/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 146/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 147/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 148/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 149/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 150/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 151/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 152/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 153/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 154/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 155/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 156/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 157/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 158/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 159/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 160/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 161/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 162/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 163/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 164/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 165/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 166/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 167/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 168/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 169/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 170/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 171/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 172/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 173/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 174/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 175/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 176/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 177/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 178/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 179/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 180/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 181/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 182/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 183/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 184/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 185/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 186/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 187/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 188/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 189/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 190/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 191/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 192/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 193/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 194/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 195/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 196/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 197/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 198/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 199/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n",
      "Epoch 200/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2141 - binary_accuracy: 0.9214 - f1_score: 0.9260 - val_loss: 0.2371 - val_binary_accuracy: 0.9240 - val_f1_score: 0.9305\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plot_model(model, to_file=plotpath / Path('cifar_DS_only.png'), show_shapes=True, show_layer_names=True)\n",
    "\n",
    "\n",
    "initial_learning_rate = 1e-3  # migliore come accuracy massima  0.69\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=100, decay_rate=0.96, staircase=False)\n",
    "\n",
    "model.compile(\n",
    "        # optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-2, decay=1e-4),   #FUSION 2022\n",
    "        # optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-4),   #FUSION 2022\n",
    "        optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "        # optimizer=tf.keras.optimizers.Adam(),\n",
    "        # optimizer=tf.keras.optimizers.Adam(0.0011),\n",
    "        # optimizer = tf.keras.optimizers.Adam(1e-2),\n",
    "        # optimizer=tf.keras.optimizers.Adam(1.1e-3),\n",
    "        # loss=\"categorical_crossentropy\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        # loss=\"mean_squared_logarithmic_error\",\n",
    "        # metrics=[\"accuracy\",dice_coef,\"categorical_accuracy\"],\n",
    "        # metrics=[\"categorical_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=4)],\n",
    "        # metrics=[\"categorical_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=2)],\n",
    "        metrics=[\"binary_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=2)],\n",
    "        # metrics=[\"categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "# CREATE CALLBACKS\n",
    "checkpointClassification = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        checkpointPath / Path('CIFAR_DS_Classification_acc'),\n",
    "                        monitor='val_binary_accuracy', verbose=0, \n",
    "                        # monitor='val_dm_pignistic_16_categorical_accuracy', verbose=0, \n",
    "                        save_best_only=True, mode='max')\n",
    "                                                # CREATE CALLBACKS\n",
    "\n",
    "\n",
    "checkpointClassificationF1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        checkpointPath / Path('CIFAR_DS_Classification_F1'), \n",
    "                        monitor='val_f1_score', verbose=0, \n",
    "                        save_best_only=True, mode='max')\n",
    "                                                # CREATE CALLBACKS\n",
    "\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "\n",
    "    \n",
    "# lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "\n",
    "cb = TimingCallback()\n",
    "\n",
    "print(checkpointPath / Path('CIFAR_DS'))\n",
    "\n",
    "# callbacks_list = [checkpointClassification]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb, schedule_lr,lrate]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb, schedule_lr]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, cb, schedule_lr]\n",
    "callbacks_list = [checkpointClassification, checkpointClassificationF1, cb]\n",
    "# callbacks_list = [checkpointClassification, cb]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb]\n",
    "# callbacks_list = [checkpointClassificationF1, earlyStopping]\n",
    "\n",
    "history = model.fit(\n",
    "        # x = X_train_IMGS,\n",
    "        # trainset,\n",
    "        x = train_images_binary,\n",
    "        # y = [Y_train,Y_tr_unc],\n",
    "        y = train_labels_binary,\n",
    "        # epochs = 1,\n",
    "        epochs = 200,\n",
    "        validation_data = (test_images_binary,test_labels_binary),\n",
    "        # validation_data = [X_val_IMGS,[Y_val,Y_val_unc]],\n",
    "        # validation_data = validset,\n",
    "        callbacks = callbacks_list,    #some problems with DS layers and callbacks\n",
    "        # batch_size=32,\n",
    "        # batch_size=1,\n",
    "        # class_weight=c_w,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train loss",
         "type": "scatter",
         "y": [
          0.5995769500732422,
          0.520651638507843,
          0.49783578515052795,
          0.5176427960395813,
          0.6561831831932068,
          0.4802391231060028,
          0.5384826064109802,
          0.4367952048778534,
          0.5044958591461182,
          0.39806678891181946,
          0.3647410273551941,
          0.3432220220565796,
          0.32636937499046326,
          0.3154947757720947,
          0.33105915784835815,
          0.27974733710289,
          0.35731586813926697,
          0.28118059039115906,
          0.27372658252716064,
          0.26674598455429077,
          0.25869911909103394,
          0.26152241230010986,
          0.2654869258403778,
          0.25490128993988037,
          0.24515566229820251,
          0.24167832732200623,
          0.24116194248199463,
          0.23956172168254852,
          0.23890578746795654,
          0.23574399948120117,
          0.23378120362758636,
          0.23235094547271729,
          0.23126664757728577,
          0.23006092011928558,
          0.22873249650001526,
          0.22776789963245392,
          0.2300315946340561,
          0.22423049807548523,
          0.22331416606903076,
          0.22255493700504303,
          0.22193613648414612,
          0.2212677001953125,
          0.22067412734031677,
          0.22011379897594452,
          0.21958988904953003,
          0.21904821693897247,
          0.21864281594753265,
          0.21816542744636536,
          0.21773843467235565,
          0.2173440158367157,
          0.21698036789894104,
          0.21666991710662842,
          0.2163219004869461,
          0.21612146496772766,
          0.21589089930057526,
          0.21569140255451202,
          0.21547216176986694,
          0.215338334441185,
          0.21516211330890656,
          0.21502172946929932,
          0.21491330862045288,
          0.2148059606552124,
          0.2147158980369568,
          0.2146315723657608,
          0.2145555168390274,
          0.21449506282806396,
          0.21443915367126465,
          0.21439068019390106,
          0.2143518030643463,
          0.21431243419647217,
          0.21428178250789642,
          0.21425151824951172,
          0.21422666311264038,
          0.21420234441757202,
          0.21418510377407074,
          0.21416673064231873,
          0.21415354311466217,
          0.21413539350032806,
          0.21412904560565948,
          0.21411271393299103,
          0.21411067247390747,
          0.2141014188528061,
          0.21409110724925995,
          0.21408355236053467,
          0.21407970786094666,
          0.2140769511461258,
          0.2140728086233139,
          0.214066743850708,
          0.21406616270542145,
          0.21406184136867523,
          0.2140607386827469,
          0.2140566110610962,
          0.2140582799911499,
          0.21405579149723053,
          0.21405595541000366,
          0.21405385434627533,
          0.21405327320098877,
          0.21405243873596191,
          0.21405307948589325,
          0.21405243873596191,
          0.21405117213726044,
          0.21405063569545746,
          0.21405097842216492,
          0.2140512764453888,
          0.21405167877674103,
          0.2140517383813858,
          0.21405194699764252,
          0.21405170857906342,
          0.21405205130577087,
          0.21405193209648132,
          0.21405208110809326,
          0.21405190229415894,
          0.21405212581157684,
          0.21405155956745148,
          0.21405154466629028,
          0.21405187249183655,
          0.214051753282547,
          0.2140519767999649,
          0.21405185759067535,
          0.21405163407325745,
          0.21405163407325745,
          0.21405178308486938,
          0.2140517383813858,
          0.21405155956745148,
          0.214051753282547,
          0.21405185759067535,
          0.2140519767999649,
          0.21405178308486938,
          0.21405170857906342,
          0.21405170857906342,
          0.21405178308486938,
          0.2140517383813858,
          0.21405190229415894,
          0.21405187249183655,
          0.21405205130577087,
          0.21405190229415894,
          0.2140519767999649,
          0.21405194699764252,
          0.2140519767999649,
          0.21405212581157684,
          0.21405205130577087,
          0.21405209600925446,
          0.2140519767999649,
          0.21405193209648132,
          0.21405185759067535,
          0.21405193209648132,
          0.2140520066022873,
          0.21405193209648132,
          0.21405194699764252,
          0.21405193209648132,
          0.21405193209648132,
          0.21405190229415894,
          0.2140520215034485,
          0.2140520066022873,
          0.21405193209648132,
          0.21405194699764252,
          0.2140520066022873,
          0.21405190229415894,
          0.21405193209648132,
          0.21405190229415894,
          0.2140519767999649,
          0.21405187249183655,
          0.2140519767999649,
          0.21405208110809326,
          0.2140519767999649,
          0.21405209600925446,
          0.21405187249183655,
          0.2140520215034485,
          0.2140519767999649,
          0.2140519767999649,
          0.21405187249183655,
          0.2140519767999649,
          0.21405190229415894,
          0.21405194699764252,
          0.2140520215034485,
          0.2140519767999649,
          0.21405190229415894,
          0.21405194699764252,
          0.21405194699764252,
          0.2140519767999649,
          0.2140520066022873,
          0.21405187249183655,
          0.2140519767999649,
          0.2140520215034485,
          0.21405194699764252,
          0.21405193209648132,
          0.2140520066022873,
          0.21405190229415894,
          0.2140520215034485,
          0.2140520215034485,
          0.2140520066022873,
          0.21405190229415894,
          0.21405205130577087,
          0.2140519767999649,
          0.21405193209648132,
          0.21405208110809326,
          0.2140520066022873,
          0.21405208110809326,
          0.2140520215034485,
          0.2140519767999649
         ]
        },
        {
         "name": "Validation loss",
         "type": "scatter",
         "y": [
          0.5801478624343872,
          0.43511340022087097,
          0.5781166553497314,
          0.5258853435516357,
          0.5351409316062927,
          0.5032767653465271,
          0.46830886602401733,
          0.3549458086490631,
          0.41687604784965515,
          0.3474920988082886,
          0.3061563968658447,
          0.28358879685401917,
          0.27331045269966125,
          0.26553747057914734,
          0.29683542251586914,
          0.2488410919904709,
          0.2867354452610016,
          0.26984235644340515,
          0.2759753167629242,
          0.2767266035079956,
          0.2727559208869934,
          0.25786879658699036,
          0.2815311551094055,
          0.26239877939224243,
          0.25934484601020813,
          0.25656968355178833,
          0.2404737025499344,
          0.2501654624938965,
          0.24017536640167236,
          0.2438599020242691,
          0.24056988954544067,
          0.24170728027820587,
          0.24394896626472473,
          0.24008245766162872,
          0.24507394433021545,
          0.24110674858093262,
          0.24138006567955017,
          0.2428468018770218,
          0.24263691902160645,
          0.24151834845542908,
          0.24105824530124664,
          0.24028940498828888,
          0.23977170884609222,
          0.23899982869625092,
          0.23861819505691528,
          0.23895907402038574,
          0.2380785048007965,
          0.23772379755973816,
          0.23748444020748138,
          0.2371455430984497,
          0.23697815835475922,
          0.23657526075839996,
          0.23938991129398346,
          0.23607583343982697,
          0.2360408455133438,
          0.23614773154258728,
          0.23843596875667572,
          0.23817987740039825,
          0.23807713389396667,
          0.23793667554855347,
          0.23779287934303284,
          0.2376883625984192,
          0.23765665292739868,
          0.23758462071418762,
          0.23753215372562408,
          0.23748554289340973,
          0.23745159804821014,
          0.23738659918308258,
          0.23736177384853363,
          0.23733118176460266,
          0.23730245232582092,
          0.2372879683971405,
          0.23726621270179749,
          0.23725078999996185,
          0.2372438609600067,
          0.23721307516098022,
          0.23718400299549103,
          0.23718814551830292,
          0.2371758669614792,
          0.23717227578163147,
          0.2371603101491928,
          0.2371673732995987,
          0.2371545284986496,
          0.23714910447597504,
          0.23714762926101685,
          0.23713982105255127,
          0.23714163899421692,
          0.23713110387325287,
          0.2371302992105484,
          0.2371336668729782,
          0.23713183403015137,
          0.23712873458862305,
          0.23712193965911865,
          0.23712630569934845,
          0.23712413012981415,
          0.2371247410774231,
          0.23712410032749176,
          0.23712535202503204,
          0.23712275922298431,
          0.23712600767612457,
          0.23712530732154846,
          0.23712578415870667,
          0.23712700605392456,
          0.2371280938386917,
          0.237127423286438,
          0.2371271699666977,
          0.23712663352489471,
          0.23712649941444397,
          0.23712608218193054,
          0.23712754249572754,
          0.23712700605392456,
          0.23712682723999023,
          0.2371269017457962,
          0.23712733387947083,
          0.2371269017457962,
          0.23712722957134247,
          0.237126886844635,
          0.23712678253650665,
          0.23712648451328278,
          0.2371271550655365,
          0.2371273785829544,
          0.23712652921676636,
          0.2371264398097992,
          0.23712657392024994,
          0.23712676763534546,
          0.2371271550655365,
          0.23712706565856934,
          0.23712745308876038,
          0.23712733387947083,
          0.23712661862373352,
          0.2371273636817932,
          0.23712731897830963,
          0.23712719976902008,
          0.2371273934841156,
          0.23712721467018127,
          0.23712721467018127,
          0.23712721467018127,
          0.23712721467018127,
          0.23712721467018127,
          0.23712721467018127,
          0.23712721467018127,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156,
          0.2371273934841156
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Loss CIFAR"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train accuracy",
         "type": "scatter",
         "y": [
          0.7344499826431274,
          0.7827500104904175,
          0.8066999912261963,
          0.7837499976158142,
          0.7035999894142151,
          0.7954999804496765,
          0.7929999828338623,
          0.8320500254631042,
          0.79339998960495,
          0.8465999960899353,
          0.8707500100135803,
          0.8794999718666077,
          0.8888499736785889,
          0.8820499777793884,
          0.8716999888420105,
          0.8954499959945679,
          0.8522499799728394,
          0.8944500088691711,
          0.9010999798774719,
          0.9034000039100647,
          0.9052000045776367,
          0.9035500288009644,
          0.9039999842643738,
          0.906000018119812,
          0.9095500111579895,
          0.909850001335144,
          0.9088500142097473,
          0.9111499786376953,
          0.911300003528595,
          0.9125999808311462,
          0.9135500192642212,
          0.9136999845504761,
          0.9147499799728394,
          0.9154499769210815,
          0.9160500168800354,
          0.9164999723434448,
          0.9155499935150146,
          0.9162499904632568,
          0.9161999821662903,
          0.9167500138282776,
          0.9177500009536743,
          0.917900025844574,
          0.9189000129699707,
          0.9188500046730042,
          0.9193999767303467,
          0.9193500280380249,
          0.919700026512146,
          0.9199000000953674,
          0.9200500249862671,
          0.920199990272522,
          0.9204999804496765,
          0.9207000136375427,
          0.9208499789237976,
          0.9207500219345093,
          0.9210500121116638,
          0.9210000038146973,
          0.9213500022888184,
          0.9211500287055969,
          0.9212499856948853,
          0.9210500121116638,
          0.9214000105857849,
          0.9212999939918518,
          0.9211999773979187,
          0.9212499856948853,
          0.9213500022888184,
          0.9212999939918518,
          0.9212499856948853,
          0.9212499856948853,
          0.9214000105857849,
          0.9212999939918518,
          0.9212999939918518,
          0.9214000105857849,
          0.9213500022888184,
          0.9213500022888184,
          0.9214000105857849,
          0.9214500188827515,
          0.9214000105857849,
          0.9214000105857849,
          0.9213500022888184,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849,
          0.9214000105857849
         ]
        },
        {
         "name": "Validation accuracy",
         "type": "scatter",
         "y": [
          0.7925000190734863,
          0.8604999780654907,
          0.8420000076293945,
          0.7200000286102295,
          0.7204999923706055,
          0.7770000100135803,
          0.8027499914169312,
          0.8535000085830688,
          0.8362500071525574,
          0.8740000128746033,
          0.8815000057220459,
          0.9004999995231628,
          0.903249979019165,
          0.9075000286102295,
          0.8970000147819519,
          0.909500002861023,
          0.8842499852180481,
          0.9012500047683716,
          0.90625,
          0.9089999794960022,
          0.9104999899864197,
          0.9102500081062317,
          0.9085000157356262,
          0.9110000133514404,
          0.9129999876022339,
          0.9147499799728394,
          0.9142500162124634,
          0.9162499904632568,
          0.9152500033378601,
          0.9185000061988831,
          0.9177500009536743,
          0.9192500114440918,
          0.9190000295639038,
          0.9200000166893005,
          0.9207500219345093,
          0.921500027179718,
          0.9207500219345093,
          0.921750009059906,
          0.921750009059906,
          0.922249972820282,
          0.9227499961853027,
          0.9232500195503235,
          0.9229999780654907,
          0.9242500066757202,
          0.9242500066757202,
          0.9229999780654907,
          0.9240000247955322,
          0.9240000247955322,
          0.9235000014305115,
          0.9237499833106995,
          0.9237499833106995,
          0.9237499833106995,
          0.9240000247955322,
          0.9237499833106995,
          0.9240000247955322,
          0.9235000014305115,
          0.9242500066757202,
          0.9237499833106995,
          0.9242500066757202,
          0.9240000247955322,
          0.9240000247955322,
          0.9237499833106995,
          0.9242500066757202,
          0.9240000247955322,
          0.9242500066757202,
          0.9240000247955322,
          0.9240000247955322,
          0.9242500066757202,
          0.9242500066757202,
          0.9242500066757202,
          0.9240000247955322,
          0.9242500066757202,
          0.9242500066757202,
          0.9242500066757202,
          0.9240000247955322,
          0.9242500066757202,
          0.9242500066757202,
          0.9242500066757202,
          0.9242500066757202,
          0.9242500066757202,
          0.9244999885559082,
          0.9242500066757202,
          0.9242500066757202,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9242500066757202,
          0.9242500066757202,
          0.9242500066757202,
          0.9242500066757202,
          0.9244999885559082,
          0.9244999885559082,
          0.9244999885559082,
          0.9242500066757202,
          0.9242500066757202,
          0.9242500066757202,
          0.9242500066757202,
          0.9242500066757202,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322,
          0.9240000247955322
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Accuracy CIFAR"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Accuracy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train F1 score",
         "type": "scatter",
         "y": [
          0.7509511709213257,
          0.8025271892547607,
          0.8280385732650757,
          0.800005316734314,
          0.7538629770278931,
          0.8266794681549072,
          0.8191488981246948,
          0.8485809564590454,
          0.8067047595977783,
          0.8568973541259766,
          0.8809894323348999,
          0.8876999616622925,
          0.8985985517501831,
          0.8964877128601074,
          0.889999270439148,
          0.9067865610122681,
          0.8631078004837036,
          0.905598521232605,
          0.9098999500274658,
          0.9114995002746582,
          0.9139999151229858,
          0.9103963375091553,
          0.9133883714675903,
          0.9172988533973694,
          0.9152964353561401,
          0.9169992804527283,
          0.9170894622802734,
          0.9180943369865417,
          0.9195906519889832,
          0.9184980392456055,
          0.9193984270095825,
          0.9203988313674927,
          0.919899582862854,
          0.9207984209060669,
          0.9204990863800049,
          0.9210983514785767,
          0.9210997819900513,
          0.9219982624053955,
          0.9233982563018799,
          0.9231988787651062,
          0.923798680305481,
          0.9239985346794128,
          0.9239983558654785,
          0.9239990711212158,
          0.9244985580444336,
          0.9243981242179871,
          0.9238985776901245,
          0.9245980978012085,
          0.9244983792304993,
          0.9246985912322998,
          0.9250983595848083,
          0.9250984191894531,
          0.9254977703094482,
          0.9245995283126831,
          0.9253981113433838,
          0.9253984689712524,
          0.9255976676940918,
          0.9253982305526733,
          0.92559814453125,
          0.9253984689712524,
          0.9255985021591187,
          0.9252983331680298,
          0.9256983995437622,
          0.9256985187530518,
          0.9255982637405396,
          0.9256983995437622,
          0.9256985187530518,
          0.9257984161376953,
          0.9257984161376953,
          0.9257984161376953,
          0.9257984161376953,
          0.9257984161376953,
          0.9257984161376953,
          0.9257984161376953,
          0.9257984161376953,
          0.9257984161376953,
          0.9257984161376953,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9259984493255615,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9258984327316284,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9258984327316284,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9258984327316284,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615,
          0.9259984493255615
         ]
        },
        {
         "name": "Validation F1 score",
         "type": "scatter",
         "y": [
          0.8049623966217041,
          0.8605934977531433,
          0.8484981060028076,
          0.7100210189819336,
          0.7847619652748108,
          0.7791550755500793,
          0.8231654167175293,
          0.8764888644218445,
          0.8518666625022888,
          0.885498583316803,
          0.8899718523025513,
          0.9104623198509216,
          0.9119956493377686,
          0.920478343963623,
          0.907796323299408,
          0.9189999103546143,
          0.8999558687210083,
          0.9134842157363892,
          0.9174826145172119,
          0.9204832911491394,
          0.9224976301193237,
          0.9189980030059814,
          0.9199997186660767,
          0.9244999885559082,
          0.9240000247955322,
          0.9259999990463257,
          0.9289982318878174,
          0.9274969696998596,
          0.9304998517036438,
          0.9294999837875366,
          0.9299988746643066,
          0.9299997091293335,
          0.9304998517036438,
          0.9304998517036438,
          0.929498553276062,
          0.9304998517036438,
          0.9279993772506714,
          0.9289993643760681,
          0.9294991493225098,
          0.9294995069503784,
          0.9299993515014648,
          0.9299993515014648,
          0.9299993515014648,
          0.9284998178482056,
          0.9299997091293335,
          0.9299988746643066,
          0.9304995536804199,
          0.9304995536804199,
          0.9299993515014648,
          0.9299993515014648,
          0.9299993515014648,
          0.9299993515014648,
          0.9284985661506653,
          0.9299993515014648,
          0.9299993515014648,
          0.9299993515014648,
          0.9289988279342651,
          0.9294991493225098,
          0.9294991493225098,
          0.9299993515014648,
          0.9299993515014648,
          0.9304995536804199,
          0.9299993515014648,
          0.9299993515014648,
          0.9299993515014648,
          0.9299993515014648,
          0.9299993515014648,
          0.9299993515014648,
          0.9299993515014648,
          0.9299993515014648,
          0.9299993515014648,
          0.9299993515014648,
          0.9299993515014648,
          0.9299993515014648,
          0.9299993515014648,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199,
          0.9304995536804199
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Accuracy CIFAR"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "F1 score"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'..\\\\..\\\\Outputs\\\\COVID\\\\LR\\\\NuovoTest_Not_Kfold\\\\GRAPHS\\\\training CIFAR_DS F1 score.html'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLOT HISTORY\n",
    "plt.clf()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['loss'],\n",
    "                    name='Train loss'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_loss'],\n",
    "                    name='Validation loss'))\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Loss CIFAR',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='Loss')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training CIFAR_DS loss.html\"))), auto_open=False)\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    # y=history.history['categorical_accuracy'],\n",
    "                    y=history.history['binary_accuracy'],\n",
    "                    name='Train accuracy'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    # y=history.history['val_categorical_accuracy'],\n",
    "                    y=history.history['val_binary_accuracy'],\n",
    "                    name='Validation accuracy'))\n",
    "\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Accuracy CIFAR',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='Accuracy')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training CIFAR_DS accuracy.html\"))), auto_open=False)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['f1_score'],\n",
    "                    name='Train F1 score'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_f1_score'],\n",
    "                    name='Validation F1 score'))\n",
    "\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Accuracy CIFAR',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='F1 score')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training CIFAR_DS F1 score.html\"))), auto_open=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth of the validation\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2402 - binary_accuracy: 0.9153 - f1_score: 0.9305\n",
      "63/63 [==============================] - 0s 1ms/step\n",
      "[0 1 1 ... 0 1 1]\n",
      "[0 1 1 ... 0 0 1]\n",
      "accuracy:  0.9305\n",
      "log_loss:  0.19159563882455763\n",
      "[[929  71]\n",
      " [ 68 932]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.93180   0.92900   0.93040      1000\n",
      "           1    0.92921   0.93200   0.93060      1000\n",
      "\n",
      "    accuracy                        0.93050      2000\n",
      "   macro avg    0.93050   0.93050   0.93050      2000\n",
      "weighted avg    0.93050   0.93050   0.93050      2000\n",
      "\n",
      "{'loss': 0.24017536640167236, 'binary_accuracy': 0.9152500033378601, 'f1_score': 0.9304998517036438}\n",
      "Ground truth of the validation\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2372 - binary_accuracy: 0.9245 - f1_score: 0.9305\n",
      "63/63 [==============================] - 0s 1ms/step\n",
      "[0 1 1 ... 0 1 1]\n",
      "[0 1 1 ... 0 0 1]\n",
      "accuracy:  0.9305\n",
      "log_loss:  0.17546626750729616\n",
      "[[933  67]\n",
      " [ 72 928]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.92836   0.93300   0.93067      1000\n",
      "           1    0.93266   0.92800   0.93033      1000\n",
      "\n",
      "    accuracy                        0.93050      2000\n",
      "   macro avg    0.93051   0.93050   0.93050      2000\n",
      "weighted avg    0.93051   0.93050   0.93050      2000\n",
      "\n",
      "{'loss': 0.2371603101491928, 'binary_accuracy': 0.9244999885559082, 'f1_score': 0.9304995536804199}\n"
     ]
    }
   ],
   "source": [
    "## EVALUATION\n",
    "gc.collect()\n",
    "\n",
    "bestPt = checkpointPath / Path('CIFAR_DS_Classification_F1')\n",
    "model = tf.keras.models.load_model(bestPt)\n",
    "\n",
    "print(\"Ground truth of the validation\")\n",
    "\n",
    "evaluation = model.evaluate(x = test_images_binary, y = test_labels_binary)\n",
    "prediction = model.predict(test_images_binary)\n",
    "\n",
    "ypreds = np.argmax(prediction, axis=1)\n",
    "yground = np.argmax(test_labels_binary, axis=1)\n",
    "# yground = np.argmax(targets, axis=1)\n",
    "\n",
    "print(ypreds)\n",
    "print(yground)\n",
    "\n",
    "\n",
    "\n",
    "score = accuracy_score(yground, ypreds)\n",
    "print('accuracy: ', score)\n",
    "\n",
    "loss = log_loss(yground, prediction)\n",
    "print('log_loss: ', loss)\n",
    "\n",
    "# Print the confusion matrix\n",
    "cmatrix = confusion_matrix(yground, ypreds)\n",
    "print(cmatrix)\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "creport = classification_report(yground, ypreds, digits=5)\n",
    "print(creport)\n",
    "\n",
    "\n",
    "evaluation = dict(zip(model.metrics_names,evaluation))\n",
    "\n",
    "# FF_f1_values.append(evaluation['f1_score'])\n",
    "\n",
    "\n",
    "\n",
    "print(evaluation)\n",
    "with open(str(evalspath) + 'CIFAR_DS_F1_EVALUATION.txt', 'w') as f:\n",
    "    f.write('CIFAR_DS FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    for k, v in evaluation.items():\n",
    "        f.write(str(k) + ' >>> '+ str(v) + '\\n\\n')\n",
    "    f.write('CIFAR_DS FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    f.write('SKLEARN METRICS:')\n",
    "    f.write(str('Loss') + ' >>> '+ str(loss) + '\\n\\n')\n",
    "    f.write(str('Accuracy') + ' >>> '+ str(score) + '\\n\\n')\n",
    "    f.write(str('Confusion Matrix') + ' >>> \\n'+ str(cmatrix) + '\\n\\n')\n",
    "    f.write(str('CLASSIFICATION REPORT WITH CLASSES') + ' >>> \\n'+ str(creport) + '\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bestPt = checkpointPath / Path('CIFAR_DS_Classification_acc')\n",
    "model = tf.keras.models.load_model(bestPt)\n",
    "\n",
    "print(\"Ground truth of the validation\")\n",
    "\n",
    "evaluation = model.evaluate(x = test_images_binary, y = test_labels_binary)\n",
    "prediction = model.predict(test_images_binary)\n",
    "\n",
    "ypreds = np.argmax(prediction, axis=1)\n",
    "yground = np.argmax(test_labels_binary, axis=1)\n",
    "# yground = np.argmax(targets, axis=1)\n",
    "\n",
    "print(ypreds)\n",
    "print(yground)\n",
    "\n",
    "\n",
    "\n",
    "score = accuracy_score(yground, ypreds)\n",
    "print('accuracy: ', score)\n",
    "\n",
    "loss = log_loss(yground, prediction)\n",
    "print('log_loss: ', loss)\n",
    "\n",
    "\n",
    "# Print the confusion matrix\n",
    "cmatrix = confusion_matrix(yground, ypreds)\n",
    "print(cmatrix)\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "creport = classification_report(yground, ypreds, digits=5)\n",
    "print(creport)\n",
    "\n",
    "\n",
    "# FF_accuracy_values.append(score)\n",
    "\n",
    "evaluation = dict(zip(model.metrics_names,evaluation))\n",
    "print(evaluation)\n",
    "with open(str(evalspath) + 'CIFAR_DS_acc_EVALUATION.txt', 'w') as f:\n",
    "    f.write('CIFAR_DS FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    for k, v in evaluation.items():\n",
    "        f.write(str(k) + ' >>> '+ str(v) + '\\n\\n')\n",
    "    f.write('CIFAR_DS FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    f.write('SKLEARN METRICS:')\n",
    "    f.write(str('Loss') + ' >>> '+ str(loss) + '\\n\\n')\n",
    "    f.write(str('Accuracy') + ' >>> '+ str(score) + '\\n\\n')\n",
    "    f.write(str('Confusion Matrix') + ' >>> \\n'+ str(cmatrix) + '\\n\\n')\n",
    "    f.write(str('CLASSIFICATION REPORT WITH CLASSES') + ' >>> \\n'+ str(creport) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT implementation of DS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# class DempsterShaferLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(DempsterShaferLayer, self).__init__()\n",
    "#         self.num_classes = num_classes\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         # No additional layer-specific parameters or variables are needed.\n",
    "#         pass\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         # Inputs should contain probability values for each class.\n",
    "#         # Assuming inputs is a tensor of shape (batch_size, num_classes).\n",
    "\n",
    "#         # Calculate belief and plausibility values for each class.\n",
    "#         belief = inputs\n",
    "#         plausibility = tf.subtract(1.0, inputs)\n",
    "\n",
    "#         # Calculate the individual mass functions for each class.\n",
    "#         mass = tf.subtract(belief, plausibility)\n",
    "\n",
    "#         # Calculate the total mass.\n",
    "#         total_mass = tf.reduce_sum(mass, axis=-1, keepdims=True)\n",
    "\n",
    "#         # Calculate the combined mass for all classes.\n",
    "#         combined_mass = tf.subtract(1.0, total_mass)\n",
    "\n",
    "#         # Concatenate the individual and combined masses as the output.\n",
    "#         output = tf.concat([mass, combined_mass], axis=-1)\n",
    "\n",
    "#         return output\n",
    "\n",
    "# class PignisticProbabilityLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(PignisticProbabilityLayer, self).__init__()\n",
    "#         self.num_classes = num_classes\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         # No additional layer-specific parameters or variables are needed.\n",
    "#         pass\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         # Inputs should contain the mass functions produced by the Dempster-Shafer layer.\n",
    "#         # Assuming inputs is a tensor of shape (batch_size, num_classes + 1),\n",
    "#         # where the last element is the combined mass.\n",
    "\n",
    "#         # Calculate the Pignistic Probability for each class.\n",
    "#         pignistic_prob = tf.reduce_sum(inputs[:, :-1], axis=-1, keepdims=True)\n",
    "\n",
    "#         return pignistic_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# class DempsterShaferLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self):\n",
    "#         super(DempsterShaferLayer, self).__init__()\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         # No trainable parameters or variables are needed.\n",
    "#         pass\n",
    "\n",
    "#     @tf.function\n",
    "#     def call(self, inputs):\n",
    "#         # Inputs should be probability values for each class.\n",
    "#         # Assuming inputs is a tensor of shape (batch_size, num_classes).\n",
    "\n",
    "#         # Calculate the mass values using Dempster's rule for each input\n",
    "#         batch_size = tf.shape(inputs)[0]\n",
    "#         num_classes = inputs.shape[1]\n",
    "\n",
    "#         # Create a variable to accumulate mass values\n",
    "#         total_mass_values = tf.zeros((batch_size, num_classes + 1), dtype=tf.float32)\n",
    "\n",
    "#         for i in tf.range(batch_size):\n",
    "#             input_probabilities = inputs[i:i+1, :]\n",
    "#             mass_values = tf.reduce_prod(1 + input_probabilities, axis=1) - 1\n",
    "\n",
    "#             # Calculate the remaining mass for each input\n",
    "#             remaining_mass = 1.0 - tf.reduce_sum(mass_values)\n",
    "\n",
    "#             # Stack the mass values for this input\n",
    "#             stacked_mass_values = tf.concat([mass_values, [remaining_mass]], axis=0)\n",
    "\n",
    "#             # Update the total mass values using scatter_nd_add\n",
    "#             indices = tf.constant([[i, 0]], dtype=tf.int32)\n",
    "#             total_mass_values = tf.tensor_scatter_nd_add(total_mass_values, indices, stacked_mass_values)\n",
    "\n",
    "#         return total_mass_values\n",
    "    \n",
    "class DempsterCombinationLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(DempsterCombinationLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # No trainable parameters or variables are needed.\n",
    "        pass\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Inputs should be a list of `n` sets of mass values.\n",
    "        # Each element in the list is a tensor of shape (batch_size, num_classes + 1).\n",
    "\n",
    "        if len(inputs) < 2:\n",
    "            raise ValueError(\"At least two sets of mass values are required for combination.\")\n",
    "\n",
    "        # Initialize the combined mass with the first set\n",
    "        combined_mass = inputs[0]\n",
    "\n",
    "        for i in range(1, len(inputs)):\n",
    "            mass_set = inputs[i]\n",
    "            new_combined_mass = tf.zeros_like(combined_mass)\n",
    "\n",
    "            for j in range(combined_mass.shape[-1]):\n",
    "                for k in range(mass_set.shape[-1]):\n",
    "                    if j & k == 0:  # Check if j and k have no overlap (bitwise AND is 0)\n",
    "                        new_combined_mass += combined_mass[:, j:j+1] * mass_set[:, k:k+1]\n",
    "\n",
    "            combined_mass = new_combined_mass\n",
    "\n",
    "        return combined_mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DempsterShaferLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(DempsterShaferLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # No trainable parameters or variables are needed.\n",
    "        pass\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Inputs should be probability values for each class.\n",
    "        # Assuming inputs is a tensor of shape (batch_size, num_classes).\n",
    "        print(inputs.shape)\n",
    "\n",
    "        # Calculate the mass values using Dempster's rule for each input\n",
    "        mass_values = tf.reduce_prod(tf.add(1.0, inputs), axis=1, keepdims=True) - 1.0\n",
    "\n",
    "        print(mass_values.shape)\n",
    "\n",
    "        # Calculate the remaining mass for each input\n",
    "        remaining_mass = 1.0 - tf.reduce_sum(mass_values, axis=-1, keepdims=True)\n",
    "        print(remaining_mass.shape)\n",
    "\n",
    "        # Stack mass_values and remaining_mass\n",
    "        stacked_mass_values = tf.concat([mass_values, remaining_mass], axis=1)\n",
    "        print(stacked_mass_values.shape)\n",
    "\n",
    "        # Calculate Belief and Plausibility\n",
    "        belief = mass_values\n",
    "        plausibility = 1.0 - remaining_mass\n",
    "\n",
    "        # Calculate Uncertainty Probability\n",
    "        uncertainty_probability = plausibility - belief\n",
    "\n",
    "        # Calculate S_X using the given formula\n",
    "        S_X = -tf.multiply(tf.subtract(1.0, uncertainty_probability), tf.multiply(mass_values, tf.math.log(tf.add(mass_values, 1e-10)))) + tf.multiply(uncertainty_probability, tf.subtract(1.0, mass_values))\n",
    "\n",
    "        # Calculate Overall_Uncertainty as the sum of S_X elements\n",
    "        Overall_Uncertainty = tf.reduce_sum(S_X, axis=-1)\n",
    "\n",
    "        return stacked_mass_values, Overall_Uncertainty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DempsterCombinationLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self):\n",
    "#         super(DempsterCombinationLayer, self).__init__()\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         # No trainable parameters are needed for this layer.\n",
    "#         pass\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         # Inputs should be a list of two tensors representing the two sets of masses.\n",
    "#         mass_A, mass_B = inputs\n",
    "\n",
    "#         # Ensure that the input masses have the same shape.\n",
    "#         assert mass_A.shape == mass_B.shape\n",
    "\n",
    "#         # Calculate the combined mass using Dempster's rule of combination.\n",
    "#         combined_mass = tf.zeros_like(mass_A)\n",
    "\n",
    "#         for i in range(mass_A.shape[-1]):\n",
    "#             sum_value = 0.0\n",
    "#             for j in range(mass_A.shape[-1]):\n",
    "#                 if i & j == 0:\n",
    "#                     sum_value += mass_A[:, i:i+1] * mass_B[:, j:j+1]\n",
    "\n",
    "#             combined_mass += sum_value / (1.0 - tf.reduce_sum(combined_mass, axis=-1, keepdims=True))\n",
    "\n",
    "#         return combined_mass\n",
    "    \n",
    "\n",
    "class GeneralizedPignisticProbabilityLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(GeneralizedPignisticProbabilityLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # No trainable parameters or variables are needed.\n",
    "        pass\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Inputs should be the combined mass values.\n",
    "        # Assuming inputs is a tensor of shape (batch_size, m).\n",
    "\n",
    "        # Calculate the Pignistic Probabilities for all classes except the first one\n",
    "        pignistic_probabilities = tf.subtract(1.0, inputs[:, 0:2])\n",
    "\n",
    "        return pignistic_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decision_Multimodal_DEMPSTER_GPT(MLP, CNN, mlpOutL = -1, cnnOutL = -1, uncertainty = False, num_classes=2):\n",
    "\n",
    "    # outMLP = MLP.layers[cnnOutL].output\n",
    "    outMLP = MLP.output\n",
    "    # outCNN = CNN.layers[mlpOutL].output\n",
    "    outCNN = CNN.output\n",
    "\n",
    "    # outMLP = BatchNormalization()(outMLP)\n",
    "    # outCNN = BatchNormalization()(outCNN)\n",
    "    \n",
    "    # conc = Concatenate(axis=-1)([outCNN,outMLP])\n",
    "\n",
    "    # final_prob = Dense(2, activation=\"softmax\")(conc)\n",
    "\n",
    "    masses_CNN = DempsterShaferLayer()(outCNN)\n",
    "\n",
    "    print(masses_CNN.shape)\n",
    " \n",
    "    masses_MLP = DempsterShaferLayer()(outMLP)\n",
    "\n",
    "    print(masses_MLP.shape)\n",
    "\n",
    "\n",
    "    combinedMasses = DempsterCombinationLayer()([masses_CNN,masses_MLP])\n",
    "\n",
    "    print(combinedMasses.shape)\n",
    "\n",
    "\n",
    "    final_prob = GeneralizedPignisticProbabilityLayer()(combinedMasses)\n",
    "\n",
    "    print(final_prob.shape)\n",
    "\n",
    "    model = Model(inputs=[CNN.input,MLP.input], outputs=final_prob)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2)\n",
      "(None, 1)\n",
      "(None, 1)\n",
      "(None, 2)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mm:\\UNIUD\\INDUSTRIAL PROJECTS\\Notebooks\\Severstal\\Covid_Uncertanty_test_new_unc.ipynb Cell 113\u001b[0m line \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y216sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m MLPmod \u001b[39m=\u001b[39m changeModelLayers(MLPmod,\u001b[39m'\u001b[39m\u001b[39m_MLP_ds\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y216sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m MLPmod\u001b[39m.\u001b[39mtrainable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y216sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m concatMultimodal \u001b[39m=\u001b[39m create_decision_Multimodal_DEMPSTER_GPT(MLPmod,CNNmod)\n\u001b[0;32m     <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y216sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m plot_model(concatMultimodal, to_file\u001b[39m=\u001b[39mplotpath \u001b[39m/\u001b[39m Path(\u001b[39m'\u001b[39m\u001b[39mMULTIMODAL DEMPSTER_GPT_only.png\u001b[39m\u001b[39m'\u001b[39m), show_shapes\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, show_layer_names\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y216sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m initial_learning_rate \u001b[39m=\u001b[39m \u001b[39m1e-4\u001b[39m  \u001b[39m# migliore come accuracy massima  0.69\u001b[39;00m\n",
      "\u001b[1;32mm:\\UNIUD\\INDUSTRIAL PROJECTS\\Notebooks\\Severstal\\Covid_Uncertanty_test_new_unc.ipynb Cell 113\u001b[0m line \u001b[0;36mcreate_decision_Multimodal_DEMPSTER_GPT\u001b[1;34m(MLP, CNN, mlpOutL, cnnOutL, uncertainty, num_classes)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y216sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# outMLP = BatchNormalization()(outMLP)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y216sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# outCNN = BatchNormalization()(outCNN)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y216sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y216sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# conc = Concatenate(axis=-1)([outCNN,outMLP])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y216sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y216sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# final_prob = Dense(2, activation=\"softmax\")(conc)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y216sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m masses_CNN \u001b[39m=\u001b[39m DempsterShaferLayer()(outCNN)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y216sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(masses_CNN\u001b[39m.\u001b[39;49mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y216sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m masses_MLP \u001b[39m=\u001b[39m DempsterShaferLayer()(outMLP)\n\u001b[0;32m     <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y216sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(masses_MLP\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "bestPt = checkpointPath / Path('efficientnet_Classification_acc')\n",
    "CNNmod = tf.keras.models.load_model(bestPt)\n",
    "CNNmod = changeModelLayers(CNNmod,'_CNN_ds')\n",
    "CNNmod.trainable = False\n",
    "\n",
    "bestPt = checkpointPath / Path('MLP_Classification_acc')\n",
    "MLPmod = tf.keras.models.load_model(bestPt)\n",
    "MLPmod = changeModelLayers(MLPmod,'_MLP_ds')\n",
    "MLPmod.trainable = False\n",
    "\n",
    "\n",
    "concatMultimodal = create_decision_Multimodal_DEMPSTER_GPT(MLPmod,CNNmod)\n",
    "\n",
    "plot_model(concatMultimodal, to_file=plotpath / Path('MULTIMODAL DEMPSTER_GPT_only.png'), show_shapes=True, show_layer_names=True)\n",
    "\n",
    "\n",
    "initial_learning_rate = 1e-4  # migliore come accuracy massima  0.69\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=100, decay_rate=0.99, staircase=False)\n",
    "\n",
    "concatMultimodal.compile(\n",
    "        # optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-2, decay=1e-4),   #FUSION 2022\n",
    "        # optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-4),   #FUSION 2022\n",
    "        optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "        # optimizer=tf.keras.optimizers.Adam(0.0011),\n",
    "        # optimizer = tf.keras.optimizers.Adam(1e-2),\n",
    "        # optimizer=tf.keras.optimizers.Adam(1.1e-3),\n",
    "        # loss=\"categorical_crossentropy\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        # loss=\"mean_squared_logarithmic_error\",\n",
    "        # metrics=[\"accuracy\",dice_coef,\"categorical_accuracy\"],\n",
    "        # metrics=[\"categorical_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=4)],\n",
    "        # metrics=[\"categorical_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=2)],\n",
    "        metrics=[\"binary_accuracy\",tfa.metrics.F1Score(average='macro',num_classes=2)],\n",
    "        # metrics=[\"categorical_accuracy\"],\n",
    ")\n",
    "# CREATE CALLBACKS\n",
    "checkpointClassification = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        checkpointPath / Path('MULTIMODAL_DEMPSTER_GPT_Classification_acc'),\n",
    "                        monitor='val_binary_accuracy', verbose=0, \n",
    "                        # monitor='val_dm_pignistic_16_categorical_accuracy', verbose=0, \n",
    "                        save_best_only=True, mode='max')\n",
    "                                                # CREATE CALLBACKS\n",
    "\n",
    "\n",
    "checkpointClassificationF1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        checkpointPath / Path('MULTIMODAL_DEMPSTER_GPT_Classification_F1'), \n",
    "                        monitor='val_f1_score', verbose=0, \n",
    "                        save_best_only=True, mode='max')\n",
    "                                                # CREATE CALLBACKS\n",
    "\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "\n",
    "    \n",
    "# lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "\n",
    "cb = TimingCallback()\n",
    "\n",
    "print(checkpointPath / Path('MULTIMODAL_DEMPSTER_GPT'))\n",
    "\n",
    "# callbacks_list = [checkpointClassification]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb, schedule_lr,lrate]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb, schedule_lr]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, cb, schedule_lr]\n",
    "callbacks_list = [checkpointClassification, checkpointClassificationF1, cb]\n",
    "# callbacks_list = [checkpointClassification, cb]\n",
    "# callbacks_list = [checkpointClassification, checkpointClassificationF1, earlyStopping, cb]\n",
    "# callbacks_list = [checkpointClassificationF1, earlyStopping]\n",
    "\n",
    "history = concatMultimodal.fit(\n",
    "        # x = X_train_IMGS,\n",
    "        # trainset,\n",
    "        x = [X_train_IMGS,X_train],\n",
    "        # y = [Y_train,Y_tr_unc],\n",
    "        y = Y_train,\n",
    "        # epochs = 1,\n",
    "        epochs = 10,\n",
    "        validation_data = [[X_val_IMGS,X_val],Y_val],\n",
    "        # validation_data = [X_val_IMGS,[Y_val,Y_val_unc]],\n",
    "        # validation_data = validset,\n",
    "        callbacks = callbacks_list,    #some problems with DS layers and callbacks\n",
    "        batch_size=32,\n",
    "        # batch_size=1,\n",
    "        # class_weight=c_w,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train loss",
         "type": "scatter",
         "y": [
          5.233654022216797,
          5.306439399719238,
          5.3593926429748535,
          5.165203094482422,
          5.339138984680176,
          5.269654273986816,
          5.251550674438477,
          5.2157816886901855,
          5.220151901245117,
          5.250588893890381,
          5.282780170440674,
          5.20430326461792,
          5.205779075622559,
          5.210830211639404,
          5.362090110778809,
          5.220607280731201,
          5.2573957443237305,
          5.332555294036865,
          5.233356475830078,
          5.375547885894775,
          5.273846626281738,
          5.377123832702637,
          5.209431171417236,
          5.114678859710693,
          5.346318244934082,
          5.147696018218994,
          5.135366916656494,
          5.197714805603027,
          5.192773342132568,
          5.263889789581299,
          5.346548557281494,
          5.234180450439453,
          5.122859001159668,
          5.2676591873168945,
          5.189431190490723,
          5.2418622970581055,
          5.2636566162109375,
          5.268970012664795,
          5.2021708488464355,
          5.272684097290039,
          5.358591556549072,
          5.251894950866699,
          5.324068069458008,
          5.217899799346924,
          5.388271331787109,
          5.347949504852295,
          5.188594341278076,
          5.150120735168457,
          5.170929908752441,
          5.196031093597412,
          5.314045429229736,
          5.121741771697998,
          5.3357720375061035,
          5.156874179840088,
          5.327731132507324,
          5.2483906745910645,
          5.266076564788818,
          5.222010612487793,
          5.247450351715088,
          5.300189971923828,
          5.248348236083984,
          5.285317897796631,
          5.296077251434326,
          5.309295177459717,
          5.282283306121826,
          5.171764850616455,
          5.2869181632995605,
          5.170132637023926,
          5.340609550476074,
          5.209794521331787,
          5.210946559906006,
          5.3307719230651855,
          5.2204413414001465,
          5.280864238739014,
          5.290773391723633,
          5.361214637756348,
          5.256042003631592,
          5.4022088050842285,
          5.054113388061523,
          5.220988750457764,
          5.253175735473633,
          5.1855363845825195,
          5.272891044616699,
          5.276176929473877,
          5.104058742523193,
          5.3612141609191895,
          5.2162861824035645,
          5.22895622253418,
          5.26316499710083,
          5.096989154815674,
          5.256659030914307,
          5.241574287414551,
          5.216874599456787,
          5.182911396026611,
          5.226144313812256,
          5.3352742195129395,
          5.061177730560303,
          5.169210910797119,
          5.096848487854004,
          5.266888618469238
         ]
        },
        {
         "name": "Validation loss",
         "type": "scatter",
         "y": [
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492,
          6.737577438354492
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Loss MULTIMODAL_DEMPSTER_GPT"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train accuracy",
         "type": "scatter",
         "y": [
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5
         ]
        },
        {
         "name": "Validation accuracy",
         "type": "scatter",
         "y": [
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Accuracy MULTIMODAL_DEMPSTER_GPT"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Accuracy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Train F1 score",
         "type": "scatter",
         "y": [
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071,
          0.6665312051773071
         ]
        },
        {
         "name": "Validation F1 score",
         "type": "scatter",
         "y": [
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695,
          0.6665584444999695
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "END 2 END Accuracy MULTIMODAL_DEMPSTER_GPT"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "F1 score"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'..\\\\..\\\\Outputs\\\\COVID\\\\LR\\\\NuovoTest_Not_Kfold\\\\GRAPHS\\\\training MULTIMODAL_DEMPSTER_GPT F1 score.html'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLOT HISTORY\n",
    "plt.clf()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['loss'],\n",
    "                    name='Train loss'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_loss'],\n",
    "                    name='Validation loss'))\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Loss MULTIMODAL_DEMPSTER_GPT',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='Loss')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training MULTIMODAL_DEMPSTER_GPT loss.html\"))), auto_open=False)\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    # y=history.history['categorical_accuracy'],\n",
    "                    y=history.history['binary_accuracy'],\n",
    "                    name='Train accuracy'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    # y=history.history['val_categorical_accuracy'],\n",
    "                    y=history.history['val_binary_accuracy'],\n",
    "                    name='Validation accuracy'))\n",
    "\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Accuracy MULTIMODAL_DEMPSTER_GPT',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='Accuracy')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training MULTIMODAL_DEMPSTER_GPT accuracy.html\"))), auto_open=False)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['f1_score'],\n",
    "                    name='Train F1 score'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_f1_score'],\n",
    "                    name='Validation F1 score'))\n",
    "\n",
    "fig.update_layout(height=500, \n",
    "                width=700,\n",
    "                title='END 2 END Accuracy MULTIMODAL_DEMPSTER_GPT',\n",
    "                xaxis_title='Epoch',\n",
    "                yaxis_title='F1 score')\n",
    "fig.show()\n",
    "plotly_plot(fig, filename=str(Path(graphPath / Path(\"training MULTIMODAL_DEMPSTER_GPT F1 score.html\"))), auto_open=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth of the validation\n",
      "4/4 [==============================] - 1s 28ms/step - loss: 6.7376 - binary_accuracy: 0.5000 - f1_score: 0.6666\n",
      "4/4 [==============================] - 1s 27ms/step\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1]\n",
      "accuracy:  0.4864864864864865\n",
      "log_loss:  0.6931471824645996\n",
      "[[54  0]\n",
      " [57  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.48649   1.00000   0.65455        54\n",
      "           1    0.00000   0.00000   0.00000        57\n",
      "\n",
      "    accuracy                        0.48649       111\n",
      "   macro avg    0.24324   0.50000   0.32727       111\n",
      "weighted avg    0.23667   0.48649   0.31843       111\n",
      "\n",
      "{'loss': 6.737577438354492, 'binary_accuracy': 0.5, 'f1_score': 0.6665584444999695}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Michele\\anaconda3\\envs\\tfNew2023_2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\Michele\\anaconda3\\envs\\tfNew2023_2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\Michele\\anaconda3\\envs\\tfNew2023_2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth of the validation\n",
      "4/4 [==============================] - 1s 26ms/step - loss: 6.7376 - binary_accuracy: 0.5000 - f1_score: 0.6666\n",
      "4/4 [==============================] - 1s 27ms/step\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1]\n",
      "accuracy:  0.4864864864864865\n",
      "log_loss:  0.6931471824645996\n",
      "[[54  0]\n",
      " [57  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.48649   1.00000   0.65455        54\n",
      "           1    0.00000   0.00000   0.00000        57\n",
      "\n",
      "    accuracy                        0.48649       111\n",
      "   macro avg    0.24324   0.50000   0.32727       111\n",
      "weighted avg    0.23667   0.48649   0.31843       111\n",
      "\n",
      "{'loss': 6.737577438354492, 'binary_accuracy': 0.5, 'f1_score': 0.6665584444999695}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Michele\\anaconda3\\envs\\tfNew2023_2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\Michele\\anaconda3\\envs\\tfNew2023_2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\Michele\\anaconda3\\envs\\tfNew2023_2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## EVALUATION\n",
    "gc.collect()\n",
    "\n",
    "bestPt = checkpointPath / Path('MULTIMODAL_DEMPSTER_GPT_Classification_F1')\n",
    "model = tf.keras.models.load_model(bestPt)\n",
    "\n",
    "print(\"Ground truth of the validation\")\n",
    "\n",
    "evaluation = model.evaluate(x = [X_val_IMGS,X_val], y = Y_val)\n",
    "prediction = model.predict([X_val_IMGS,X_val])\n",
    "\n",
    "ypreds = np.argmax(prediction, axis=1)\n",
    "yground = np.argmax(Y_val.to_numpy(), axis=1)\n",
    "# yground = np.argmax(targets, axis=1)\n",
    "\n",
    "print(ypreds)\n",
    "print(yground)\n",
    "\n",
    "\n",
    "\n",
    "score = accuracy_score(yground, ypreds)\n",
    "print('accuracy: ', score)\n",
    "\n",
    "loss = log_loss(yground, prediction)\n",
    "print('log_loss: ', loss)\n",
    "\n",
    "# Print the confusion matrix\n",
    "cmatrix = confusion_matrix(yground, ypreds)\n",
    "print(cmatrix)\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "creport = classification_report(yground, ypreds, digits=5)\n",
    "print(creport)\n",
    "\n",
    "\n",
    "evaluation = dict(zip(model.metrics_names,evaluation))\n",
    "\n",
    "# FF_f1_values.append(evaluation['f1_score'])\n",
    "\n",
    "\n",
    "\n",
    "print(evaluation)\n",
    "with open(str(evalspath) + 'MULTIMODAL_DEMPSTER_GPT_F1_EVALUATION.txt', 'w') as f:\n",
    "    f.write('MULTIMODAL_DEMPSTER_GPT FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    for k, v in evaluation.items():\n",
    "        f.write(str(k) + ' >>> '+ str(v) + '\\n\\n')\n",
    "    f.write('MULTIMODAL_DEMPSTER_GPT FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    f.write('SKLEARN METRICS:')\n",
    "    f.write(str('Loss') + ' >>> '+ str(loss) + '\\n\\n')\n",
    "    f.write(str('Accuracy') + ' >>> '+ str(score) + '\\n\\n')\n",
    "    f.write(str('Confusion Matrix') + ' >>> \\n'+ str(cmatrix) + '\\n\\n')\n",
    "    f.write(str('CLASSIFICATION REPORT WITH CLASSES') + ' >>> \\n'+ str(creport) + '\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bestPt = checkpointPath / Path('MULTIMODAL_DEMPSTER_GPT_Classification_acc')\n",
    "model = tf.keras.models.load_model(bestPt)\n",
    "\n",
    "print(\"Ground truth of the validation\")\n",
    "\n",
    "evaluation = model.evaluate(x = [X_val_IMGS,X_val], y = Y_val)\n",
    "prediction = model.predict([X_val_IMGS,X_val])\n",
    "\n",
    "ypreds = np.argmax(prediction, axis=1)\n",
    "yground = np.argmax(Y_val.to_numpy(), axis=1)\n",
    "# yground = np.argmax(targets, axis=1)\n",
    "\n",
    "print(ypreds)\n",
    "print(yground)\n",
    "\n",
    "\n",
    "\n",
    "score = accuracy_score(yground, ypreds)\n",
    "print('accuracy: ', score)\n",
    "\n",
    "loss = log_loss(yground, prediction)\n",
    "print('log_loss: ', loss)\n",
    "\n",
    "\n",
    "# Print the confusion matrix\n",
    "cmatrix = confusion_matrix(yground, ypreds)\n",
    "print(cmatrix)\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "creport = classification_report(yground, ypreds, digits=5)\n",
    "print(creport)\n",
    "\n",
    "\n",
    "# FF_accuracy_values.append(score)\n",
    "\n",
    "evaluation = dict(zip(model.metrics_names,evaluation))\n",
    "print(evaluation)\n",
    "with open(str(evalspath) + 'MULTIMODAL_DEMPSTER_GPT_acc_EVALUATION.txt', 'w') as f:\n",
    "    f.write('MULTIMODAL_DEMPSTER_GPT FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    for k, v in evaluation.items():\n",
    "        f.write(str(k) + ' >>> '+ str(v) + '\\n\\n')\n",
    "    f.write('MULTIMODAL_DEMPSTER_GPT FUSION BY DS TR: 80 VAL: 20'+'\\n\\n')\n",
    "    f.write('SKLEARN METRICS:')\n",
    "    f.write(str('Loss') + ' >>> '+ str(loss) + '\\n\\n')\n",
    "    f.write(str('Accuracy') + ' >>> '+ str(score) + '\\n\\n')\n",
    "    f.write(str('Confusion Matrix') + ' >>> \\n'+ str(cmatrix) + '\\n\\n')\n",
    "    f.write(str('CLASSIFICATION REPORT WITH CLASSES') + ' >>> \\n'+ str(creport) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_17_CNN_ds (InputLayer)   [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " stem_conv_CNN_ds (Conv2D)      (None, 112, 112, 32  864         ['input_17_CNN_ds[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_bn_CNN_ds (BatchNormaliza  (None, 112, 112, 32  128        ['stem_conv_CNN_ds[0][0]']       \n",
      " tion)                          )                                                                 \n",
      "                                                                                                  \n",
      " stem_activation_CNN_ds (Activa  (None, 112, 112, 32  0          ['stem_bn_CNN_ds[0][0]']         \n",
      " tion)                          )                                                                 \n",
      "                                                                                                  \n",
      " block1a_dwconv_CNN_ds (Depthwi  (None, 112, 112, 32  288        ['stem_activation_CNN_ds[0][0]'] \n",
      " seConv2D)                      )                                                                 \n",
      "                                                                                                  \n",
      " block1a_bn_CNN_ds (BatchNormal  (None, 112, 112, 32  128        ['block1a_dwconv_CNN_ds[0][0]']  \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block1a_activation_CNN_ds (Act  (None, 112, 112, 32  0          ['block1a_bn_CNN_ds[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block1a_se_squeeze_CNN_ds (Glo  (None, 32)          0           ['block1a_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block1a_se_reshape_CNN_ds (Res  (None, 1, 1, 32)    0           ['block1a_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block1a_se_reduce_CNN_ds (Conv  (None, 1, 1, 8)     264         ['block1a_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block1a_se_expand_CNN_ds (Conv  (None, 1, 1, 32)    288         ['block1a_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block1a_se_excite_CNN_ds (Mult  (None, 112, 112, 32  0          ['block1a_activation_CNN_ds[0][0]\n",
      " iply)                          )                                ',                               \n",
      "                                                                  'block1a_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block1a_project_conv_CNN_ds (C  (None, 112, 112, 16  512        ['block1a_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                         )                                ]                                \n",
      "                                                                                                  \n",
      " block1a_project_bn_CNN_ds (Bat  (None, 112, 112, 16  64         ['block1a_project_conv_CNN_ds[0][\n",
      " chNormalization)               )                                0]']                             \n",
      "                                                                                                  \n",
      " block1b_dwconv_CNN_ds (Depthwi  (None, 112, 112, 16  144        ['block1a_project_bn_CNN_ds[0][0]\n",
      " seConv2D)                      )                                ']                               \n",
      "                                                                                                  \n",
      " block1b_bn_CNN_ds (BatchNormal  (None, 112, 112, 16  64         ['block1b_dwconv_CNN_ds[0][0]']  \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block1b_activation_CNN_ds (Act  (None, 112, 112, 16  0          ['block1b_bn_CNN_ds[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block1b_se_squeeze_CNN_ds (Glo  (None, 16)          0           ['block1b_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block1b_se_reshape_CNN_ds (Res  (None, 1, 1, 16)    0           ['block1b_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block1b_se_reduce_CNN_ds (Conv  (None, 1, 1, 4)     68          ['block1b_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block1b_se_expand_CNN_ds (Conv  (None, 1, 1, 16)    80          ['block1b_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block1b_se_excite_CNN_ds (Mult  (None, 112, 112, 16  0          ['block1b_activation_CNN_ds[0][0]\n",
      " iply)                          )                                ',                               \n",
      "                                                                  'block1b_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block1b_project_conv_CNN_ds (C  (None, 112, 112, 16  256        ['block1b_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                         )                                ]                                \n",
      "                                                                                                  \n",
      " block1b_project_bn_CNN_ds (Bat  (None, 112, 112, 16  64         ['block1b_project_conv_CNN_ds[0][\n",
      " chNormalization)               )                                0]']                             \n",
      "                                                                                                  \n",
      " block1b_drop_CNN_ds (FixedDrop  (None, 112, 112, 16  0          ['block1b_project_bn_CNN_ds[0][0]\n",
      " out)                           )                                ']                               \n",
      "                                                                                                  \n",
      " block1b_add_CNN_ds (Add)       (None, 112, 112, 16  0           ['block1b_drop_CNN_ds[0][0]',    \n",
      "                                )                                 'block1a_project_bn_CNN_ds[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " block2a_expand_conv_CNN_ds (Co  (None, 112, 112, 96  1536       ['block1b_add_CNN_ds[0][0]']     \n",
      " nv2D)                          )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_bn_CNN_ds (Batc  (None, 112, 112, 96  384        ['block2a_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                )                                ]']                              \n",
      "                                                                                                  \n",
      " block2a_expand_activation_CNN_  (None, 112, 112, 96  0          ['block2a_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                )                                ]                                \n",
      "                                                                                                  \n",
      " block2a_dwconv_CNN_ds (Depthwi  (None, 56, 56, 96)  864         ['block2a_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block2a_bn_CNN_ds (BatchNormal  (None, 56, 56, 96)  384         ['block2a_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block2a_activation_CNN_ds (Act  (None, 56, 56, 96)  0           ['block2a_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block2a_se_squeeze_CNN_ds (Glo  (None, 96)          0           ['block2a_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block2a_se_reshape_CNN_ds (Res  (None, 1, 1, 96)    0           ['block2a_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block2a_se_reduce_CNN_ds (Conv  (None, 1, 1, 4)     388         ['block2a_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block2a_se_expand_CNN_ds (Conv  (None, 1, 1, 96)    480         ['block2a_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block2a_se_excite_CNN_ds (Mult  (None, 56, 56, 96)  0           ['block2a_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block2a_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block2a_project_conv_CNN_ds (C  (None, 56, 56, 24)  2304        ['block2a_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block2a_project_bn_CNN_ds (Bat  (None, 56, 56, 24)  96          ['block2a_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block2b_expand_conv_CNN_ds (Co  (None, 56, 56, 144)  3456       ['block2a_project_bn_CNN_ds[0][0]\n",
      " nv2D)                                                           ']                               \n",
      "                                                                                                  \n",
      " block2b_expand_bn_CNN_ds (Batc  (None, 56, 56, 144)  576        ['block2b_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block2b_expand_activation_CNN_  (None, 56, 56, 144)  0          ['block2b_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block2b_dwconv_CNN_ds (Depthwi  (None, 56, 56, 144)  1296       ['block2b_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block2b_bn_CNN_ds (BatchNormal  (None, 56, 56, 144)  576        ['block2b_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_activation_CNN_ds (Act  (None, 56, 56, 144)  0          ['block2b_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_se_squeeze_CNN_ds (Glo  (None, 144)         0           ['block2b_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block2b_se_reshape_CNN_ds (Res  (None, 1, 1, 144)   0           ['block2b_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block2b_se_reduce_CNN_ds (Conv  (None, 1, 1, 6)     870         ['block2b_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block2b_se_expand_CNN_ds (Conv  (None, 1, 1, 144)   1008        ['block2b_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block2b_se_excite_CNN_ds (Mult  (None, 56, 56, 144)  0          ['block2b_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block2b_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block2b_project_conv_CNN_ds (C  (None, 56, 56, 24)  3456        ['block2b_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block2b_project_bn_CNN_ds (Bat  (None, 56, 56, 24)  96          ['block2b_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block2b_drop_CNN_ds (FixedDrop  (None, 56, 56, 24)  0           ['block2b_project_bn_CNN_ds[0][0]\n",
      " out)                                                            ']                               \n",
      "                                                                                                  \n",
      " block2b_add_CNN_ds (Add)       (None, 56, 56, 24)   0           ['block2b_drop_CNN_ds[0][0]',    \n",
      "                                                                  'block2a_project_bn_CNN_ds[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " block2c_expand_conv_CNN_ds (Co  (None, 56, 56, 144)  3456       ['block2b_add_CNN_ds[0][0]']     \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block2c_expand_bn_CNN_ds (Batc  (None, 56, 56, 144)  576        ['block2c_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block2c_expand_activation_CNN_  (None, 56, 56, 144)  0          ['block2c_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block2c_dwconv_CNN_ds (Depthwi  (None, 56, 56, 144)  1296       ['block2c_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block2c_bn_CNN_ds (BatchNormal  (None, 56, 56, 144)  576        ['block2c_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block2c_activation_CNN_ds (Act  (None, 56, 56, 144)  0          ['block2c_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block2c_se_squeeze_CNN_ds (Glo  (None, 144)         0           ['block2c_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block2c_se_reshape_CNN_ds (Res  (None, 1, 1, 144)   0           ['block2c_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block2c_se_reduce_CNN_ds (Conv  (None, 1, 1, 6)     870         ['block2c_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block2c_se_expand_CNN_ds (Conv  (None, 1, 1, 144)   1008        ['block2c_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block2c_se_excite_CNN_ds (Mult  (None, 56, 56, 144)  0          ['block2c_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block2c_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block2c_project_conv_CNN_ds (C  (None, 56, 56, 24)  3456        ['block2c_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block2c_project_bn_CNN_ds (Bat  (None, 56, 56, 24)  96          ['block2c_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block2c_drop_CNN_ds (FixedDrop  (None, 56, 56, 24)  0           ['block2c_project_bn_CNN_ds[0][0]\n",
      " out)                                                            ']                               \n",
      "                                                                                                  \n",
      " block2c_add_CNN_ds (Add)       (None, 56, 56, 24)   0           ['block2c_drop_CNN_ds[0][0]',    \n",
      "                                                                  'block2b_add_CNN_ds[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_expand_conv_CNN_ds (Co  (None, 56, 56, 144)  3456       ['block2c_add_CNN_ds[0][0]']     \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block3a_expand_bn_CNN_ds (Batc  (None, 56, 56, 144)  576        ['block3a_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block3a_expand_activation_CNN_  (None, 56, 56, 144)  0          ['block3a_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block3a_dwconv_CNN_ds (Depthwi  (None, 28, 28, 144)  3600       ['block3a_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block3a_bn_CNN_ds (BatchNormal  (None, 28, 28, 144)  576        ['block3a_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_activation_CNN_ds (Act  (None, 28, 28, 144)  0          ['block3a_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_se_squeeze_CNN_ds (Glo  (None, 144)         0           ['block3a_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block3a_se_reshape_CNN_ds (Res  (None, 1, 1, 144)   0           ['block3a_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block3a_se_reduce_CNN_ds (Conv  (None, 1, 1, 6)     870         ['block3a_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block3a_se_expand_CNN_ds (Conv  (None, 1, 1, 144)   1008        ['block3a_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block3a_se_excite_CNN_ds (Mult  (None, 28, 28, 144)  0          ['block3a_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block3a_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block3a_project_conv_CNN_ds (C  (None, 28, 28, 40)  5760        ['block3a_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block3a_project_bn_CNN_ds (Bat  (None, 28, 28, 40)  160         ['block3a_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block3b_expand_conv_CNN_ds (Co  (None, 28, 28, 240)  9600       ['block3a_project_bn_CNN_ds[0][0]\n",
      " nv2D)                                                           ']                               \n",
      "                                                                                                  \n",
      " block3b_expand_bn_CNN_ds (Batc  (None, 28, 28, 240)  960        ['block3b_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block3b_expand_activation_CNN_  (None, 28, 28, 240)  0          ['block3b_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block3b_dwconv_CNN_ds (Depthwi  (None, 28, 28, 240)  6000       ['block3b_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block3b_bn_CNN_ds (BatchNormal  (None, 28, 28, 240)  960        ['block3b_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_activation_CNN_ds (Act  (None, 28, 28, 240)  0          ['block3b_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_se_squeeze_CNN_ds (Glo  (None, 240)         0           ['block3b_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block3b_se_reshape_CNN_ds (Res  (None, 1, 1, 240)   0           ['block3b_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block3b_se_reduce_CNN_ds (Conv  (None, 1, 1, 10)    2410        ['block3b_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block3b_se_expand_CNN_ds (Conv  (None, 1, 1, 240)   2640        ['block3b_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block3b_se_excite_CNN_ds (Mult  (None, 28, 28, 240)  0          ['block3b_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block3b_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block3b_project_conv_CNN_ds (C  (None, 28, 28, 40)  9600        ['block3b_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block3b_project_bn_CNN_ds (Bat  (None, 28, 28, 40)  160         ['block3b_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block3b_drop_CNN_ds (FixedDrop  (None, 28, 28, 40)  0           ['block3b_project_bn_CNN_ds[0][0]\n",
      " out)                                                            ']                               \n",
      "                                                                                                  \n",
      " block3b_add_CNN_ds (Add)       (None, 28, 28, 40)   0           ['block3b_drop_CNN_ds[0][0]',    \n",
      "                                                                  'block3a_project_bn_CNN_ds[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " block3c_expand_conv_CNN_ds (Co  (None, 28, 28, 240)  9600       ['block3b_add_CNN_ds[0][0]']     \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block3c_expand_bn_CNN_ds (Batc  (None, 28, 28, 240)  960        ['block3c_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block3c_expand_activation_CNN_  (None, 28, 28, 240)  0          ['block3c_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block3c_dwconv_CNN_ds (Depthwi  (None, 28, 28, 240)  6000       ['block3c_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block3c_bn_CNN_ds (BatchNormal  (None, 28, 28, 240)  960        ['block3c_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3c_activation_CNN_ds (Act  (None, 28, 28, 240)  0          ['block3c_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3c_se_squeeze_CNN_ds (Glo  (None, 240)         0           ['block3c_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block3c_se_reshape_CNN_ds (Res  (None, 1, 1, 240)   0           ['block3c_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block3c_se_reduce_CNN_ds (Conv  (None, 1, 1, 10)    2410        ['block3c_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block3c_se_expand_CNN_ds (Conv  (None, 1, 1, 240)   2640        ['block3c_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block3c_se_excite_CNN_ds (Mult  (None, 28, 28, 240)  0          ['block3c_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block3c_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block3c_project_conv_CNN_ds (C  (None, 28, 28, 40)  9600        ['block3c_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block3c_project_bn_CNN_ds (Bat  (None, 28, 28, 40)  160         ['block3c_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block3c_drop_CNN_ds (FixedDrop  (None, 28, 28, 40)  0           ['block3c_project_bn_CNN_ds[0][0]\n",
      " out)                                                            ']                               \n",
      "                                                                                                  \n",
      " block3c_add_CNN_ds (Add)       (None, 28, 28, 40)   0           ['block3c_drop_CNN_ds[0][0]',    \n",
      "                                                                  'block3b_add_CNN_ds[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_expand_conv_CNN_ds (Co  (None, 28, 28, 240)  9600       ['block3c_add_CNN_ds[0][0]']     \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block4a_expand_bn_CNN_ds (Batc  (None, 28, 28, 240)  960        ['block4a_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block4a_expand_activation_CNN_  (None, 28, 28, 240)  0          ['block4a_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block4a_dwconv_CNN_ds (Depthwi  (None, 14, 14, 240)  2160       ['block4a_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block4a_bn_CNN_ds (BatchNormal  (None, 14, 14, 240)  960        ['block4a_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_activation_CNN_ds (Act  (None, 14, 14, 240)  0          ['block4a_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_se_squeeze_CNN_ds (Glo  (None, 240)         0           ['block4a_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block4a_se_reshape_CNN_ds (Res  (None, 1, 1, 240)   0           ['block4a_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block4a_se_reduce_CNN_ds (Conv  (None, 1, 1, 10)    2410        ['block4a_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block4a_se_expand_CNN_ds (Conv  (None, 1, 1, 240)   2640        ['block4a_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block4a_se_excite_CNN_ds (Mult  (None, 14, 14, 240)  0          ['block4a_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block4a_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block4a_project_conv_CNN_ds (C  (None, 14, 14, 80)  19200       ['block4a_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block4a_project_bn_CNN_ds (Bat  (None, 14, 14, 80)  320         ['block4a_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block4b_expand_conv_CNN_ds (Co  (None, 14, 14, 480)  38400      ['block4a_project_bn_CNN_ds[0][0]\n",
      " nv2D)                                                           ']                               \n",
      "                                                                                                  \n",
      " block4b_expand_bn_CNN_ds (Batc  (None, 14, 14, 480)  1920       ['block4b_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block4b_expand_activation_CNN_  (None, 14, 14, 480)  0          ['block4b_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block4b_dwconv_CNN_ds (Depthwi  (None, 14, 14, 480)  4320       ['block4b_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block4b_bn_CNN_ds (BatchNormal  (None, 14, 14, 480)  1920       ['block4b_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_activation_CNN_ds (Act  (None, 14, 14, 480)  0          ['block4b_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_se_squeeze_CNN_ds (Glo  (None, 480)         0           ['block4b_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block4b_se_reshape_CNN_ds (Res  (None, 1, 1, 480)   0           ['block4b_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block4b_se_reduce_CNN_ds (Conv  (None, 1, 1, 20)    9620        ['block4b_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block4b_se_expand_CNN_ds (Conv  (None, 1, 1, 480)   10080       ['block4b_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block4b_se_excite_CNN_ds (Mult  (None, 14, 14, 480)  0          ['block4b_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block4b_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block4b_project_conv_CNN_ds (C  (None, 14, 14, 80)  38400       ['block4b_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block4b_project_bn_CNN_ds (Bat  (None, 14, 14, 80)  320         ['block4b_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block4b_drop_CNN_ds (FixedDrop  (None, 14, 14, 80)  0           ['block4b_project_bn_CNN_ds[0][0]\n",
      " out)                                                            ']                               \n",
      "                                                                                                  \n",
      " block4b_add_CNN_ds (Add)       (None, 14, 14, 80)   0           ['block4b_drop_CNN_ds[0][0]',    \n",
      "                                                                  'block4a_project_bn_CNN_ds[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " block4c_expand_conv_CNN_ds (Co  (None, 14, 14, 480)  38400      ['block4b_add_CNN_ds[0][0]']     \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block4c_expand_bn_CNN_ds (Batc  (None, 14, 14, 480)  1920       ['block4c_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block4c_expand_activation_CNN_  (None, 14, 14, 480)  0          ['block4c_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block4c_dwconv_CNN_ds (Depthwi  (None, 14, 14, 480)  4320       ['block4c_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block4c_bn_CNN_ds (BatchNormal  (None, 14, 14, 480)  1920       ['block4c_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_activation_CNN_ds (Act  (None, 14, 14, 480)  0          ['block4c_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_se_squeeze_CNN_ds (Glo  (None, 480)         0           ['block4c_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block4c_se_reshape_CNN_ds (Res  (None, 1, 1, 480)   0           ['block4c_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block4c_se_reduce_CNN_ds (Conv  (None, 1, 1, 20)    9620        ['block4c_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block4c_se_expand_CNN_ds (Conv  (None, 1, 1, 480)   10080       ['block4c_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block4c_se_excite_CNN_ds (Mult  (None, 14, 14, 480)  0          ['block4c_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block4c_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block4c_project_conv_CNN_ds (C  (None, 14, 14, 80)  38400       ['block4c_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block4c_project_bn_CNN_ds (Bat  (None, 14, 14, 80)  320         ['block4c_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block4c_drop_CNN_ds (FixedDrop  (None, 14, 14, 80)  0           ['block4c_project_bn_CNN_ds[0][0]\n",
      " out)                                                            ']                               \n",
      "                                                                                                  \n",
      " block4c_add_CNN_ds (Add)       (None, 14, 14, 80)   0           ['block4c_drop_CNN_ds[0][0]',    \n",
      "                                                                  'block4b_add_CNN_ds[0][0]']     \n",
      "                                                                                                  \n",
      " block4d_expand_conv_CNN_ds (Co  (None, 14, 14, 480)  38400      ['block4c_add_CNN_ds[0][0]']     \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block4d_expand_bn_CNN_ds (Batc  (None, 14, 14, 480)  1920       ['block4d_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block4d_expand_activation_CNN_  (None, 14, 14, 480)  0          ['block4d_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block4d_dwconv_CNN_ds (Depthwi  (None, 14, 14, 480)  4320       ['block4d_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block4d_bn_CNN_ds (BatchNormal  (None, 14, 14, 480)  1920       ['block4d_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4d_activation_CNN_ds (Act  (None, 14, 14, 480)  0          ['block4d_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4d_se_squeeze_CNN_ds (Glo  (None, 480)         0           ['block4d_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block4d_se_reshape_CNN_ds (Res  (None, 1, 1, 480)   0           ['block4d_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block4d_se_reduce_CNN_ds (Conv  (None, 1, 1, 20)    9620        ['block4d_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block4d_se_expand_CNN_ds (Conv  (None, 1, 1, 480)   10080       ['block4d_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block4d_se_excite_CNN_ds (Mult  (None, 14, 14, 480)  0          ['block4d_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block4d_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block4d_project_conv_CNN_ds (C  (None, 14, 14, 80)  38400       ['block4d_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block4d_project_bn_CNN_ds (Bat  (None, 14, 14, 80)  320         ['block4d_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block4d_drop_CNN_ds (FixedDrop  (None, 14, 14, 80)  0           ['block4d_project_bn_CNN_ds[0][0]\n",
      " out)                                                            ']                               \n",
      "                                                                                                  \n",
      " block4d_add_CNN_ds (Add)       (None, 14, 14, 80)   0           ['block4d_drop_CNN_ds[0][0]',    \n",
      "                                                                  'block4c_add_CNN_ds[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_expand_conv_CNN_ds (Co  (None, 14, 14, 480)  38400      ['block4d_add_CNN_ds[0][0]']     \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block5a_expand_bn_CNN_ds (Batc  (None, 14, 14, 480)  1920       ['block5a_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block5a_expand_activation_CNN_  (None, 14, 14, 480)  0          ['block5a_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block5a_dwconv_CNN_ds (Depthwi  (None, 14, 14, 480)  12000      ['block5a_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block5a_bn_CNN_ds (BatchNormal  (None, 14, 14, 480)  1920       ['block5a_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_activation_CNN_ds (Act  (None, 14, 14, 480)  0          ['block5a_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_se_squeeze_CNN_ds (Glo  (None, 480)         0           ['block5a_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block5a_se_reshape_CNN_ds (Res  (None, 1, 1, 480)   0           ['block5a_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block5a_se_reduce_CNN_ds (Conv  (None, 1, 1, 20)    9620        ['block5a_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block5a_se_expand_CNN_ds (Conv  (None, 1, 1, 480)   10080       ['block5a_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block5a_se_excite_CNN_ds (Mult  (None, 14, 14, 480)  0          ['block5a_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block5a_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block5a_project_conv_CNN_ds (C  (None, 14, 14, 112)  53760      ['block5a_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block5a_project_bn_CNN_ds (Bat  (None, 14, 14, 112)  448        ['block5a_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block5b_expand_conv_CNN_ds (Co  (None, 14, 14, 672)  75264      ['block5a_project_bn_CNN_ds[0][0]\n",
      " nv2D)                                                           ']                               \n",
      "                                                                                                  \n",
      " block5b_expand_bn_CNN_ds (Batc  (None, 14, 14, 672)  2688       ['block5b_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block5b_expand_activation_CNN_  (None, 14, 14, 672)  0          ['block5b_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block5b_dwconv_CNN_ds (Depthwi  (None, 14, 14, 672)  16800      ['block5b_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block5b_bn_CNN_ds (BatchNormal  (None, 14, 14, 672)  2688       ['block5b_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_activation_CNN_ds (Act  (None, 14, 14, 672)  0          ['block5b_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_se_squeeze_CNN_ds (Glo  (None, 672)         0           ['block5b_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block5b_se_reshape_CNN_ds (Res  (None, 1, 1, 672)   0           ['block5b_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block5b_se_reduce_CNN_ds (Conv  (None, 1, 1, 28)    18844       ['block5b_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block5b_se_expand_CNN_ds (Conv  (None, 1, 1, 672)   19488       ['block5b_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block5b_se_excite_CNN_ds (Mult  (None, 14, 14, 672)  0          ['block5b_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block5b_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block5b_project_conv_CNN_ds (C  (None, 14, 14, 112)  75264      ['block5b_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block5b_project_bn_CNN_ds (Bat  (None, 14, 14, 112)  448        ['block5b_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block5b_drop_CNN_ds (FixedDrop  (None, 14, 14, 112)  0          ['block5b_project_bn_CNN_ds[0][0]\n",
      " out)                                                            ']                               \n",
      "                                                                                                  \n",
      " block5b_add_CNN_ds (Add)       (None, 14, 14, 112)  0           ['block5b_drop_CNN_ds[0][0]',    \n",
      "                                                                  'block5a_project_bn_CNN_ds[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " block5c_expand_conv_CNN_ds (Co  (None, 14, 14, 672)  75264      ['block5b_add_CNN_ds[0][0]']     \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block5c_expand_bn_CNN_ds (Batc  (None, 14, 14, 672)  2688       ['block5c_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block5c_expand_activation_CNN_  (None, 14, 14, 672)  0          ['block5c_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block5c_dwconv_CNN_ds (Depthwi  (None, 14, 14, 672)  16800      ['block5c_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block5c_bn_CNN_ds (BatchNormal  (None, 14, 14, 672)  2688       ['block5c_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_activation_CNN_ds (Act  (None, 14, 14, 672)  0          ['block5c_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_se_squeeze_CNN_ds (Glo  (None, 672)         0           ['block5c_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block5c_se_reshape_CNN_ds (Res  (None, 1, 1, 672)   0           ['block5c_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block5c_se_reduce_CNN_ds (Conv  (None, 1, 1, 28)    18844       ['block5c_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block5c_se_expand_CNN_ds (Conv  (None, 1, 1, 672)   19488       ['block5c_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block5c_se_excite_CNN_ds (Mult  (None, 14, 14, 672)  0          ['block5c_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block5c_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block5c_project_conv_CNN_ds (C  (None, 14, 14, 112)  75264      ['block5c_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block5c_project_bn_CNN_ds (Bat  (None, 14, 14, 112)  448        ['block5c_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block5c_drop_CNN_ds (FixedDrop  (None, 14, 14, 112)  0          ['block5c_project_bn_CNN_ds[0][0]\n",
      " out)                                                            ']                               \n",
      "                                                                                                  \n",
      " block5c_add_CNN_ds (Add)       (None, 14, 14, 112)  0           ['block5c_drop_CNN_ds[0][0]',    \n",
      "                                                                  'block5b_add_CNN_ds[0][0]']     \n",
      "                                                                                                  \n",
      " block5d_expand_conv_CNN_ds (Co  (None, 14, 14, 672)  75264      ['block5c_add_CNN_ds[0][0]']     \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block5d_expand_bn_CNN_ds (Batc  (None, 14, 14, 672)  2688       ['block5d_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block5d_expand_activation_CNN_  (None, 14, 14, 672)  0          ['block5d_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block5d_dwconv_CNN_ds (Depthwi  (None, 14, 14, 672)  16800      ['block5d_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block5d_bn_CNN_ds (BatchNormal  (None, 14, 14, 672)  2688       ['block5d_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5d_activation_CNN_ds (Act  (None, 14, 14, 672)  0          ['block5d_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5d_se_squeeze_CNN_ds (Glo  (None, 672)         0           ['block5d_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block5d_se_reshape_CNN_ds (Res  (None, 1, 1, 672)   0           ['block5d_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block5d_se_reduce_CNN_ds (Conv  (None, 1, 1, 28)    18844       ['block5d_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block5d_se_expand_CNN_ds (Conv  (None, 1, 1, 672)   19488       ['block5d_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block5d_se_excite_CNN_ds (Mult  (None, 14, 14, 672)  0          ['block5d_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block5d_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block5d_project_conv_CNN_ds (C  (None, 14, 14, 112)  75264      ['block5d_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block5d_project_bn_CNN_ds (Bat  (None, 14, 14, 112)  448        ['block5d_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block5d_drop_CNN_ds (FixedDrop  (None, 14, 14, 112)  0          ['block5d_project_bn_CNN_ds[0][0]\n",
      " out)                                                            ']                               \n",
      "                                                                                                  \n",
      " block5d_add_CNN_ds (Add)       (None, 14, 14, 112)  0           ['block5d_drop_CNN_ds[0][0]',    \n",
      "                                                                  'block5c_add_CNN_ds[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_expand_conv_CNN_ds (Co  (None, 14, 14, 672)  75264      ['block5d_add_CNN_ds[0][0]']     \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block6a_expand_bn_CNN_ds (Batc  (None, 14, 14, 672)  2688       ['block6a_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block6a_expand_activation_CNN_  (None, 14, 14, 672)  0          ['block6a_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block6a_dwconv_CNN_ds (Depthwi  (None, 7, 7, 672)   16800       ['block6a_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block6a_bn_CNN_ds (BatchNormal  (None, 7, 7, 672)   2688        ['block6a_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_activation_CNN_ds (Act  (None, 7, 7, 672)   0           ['block6a_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_se_squeeze_CNN_ds (Glo  (None, 672)         0           ['block6a_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block6a_se_reshape_CNN_ds (Res  (None, 1, 1, 672)   0           ['block6a_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block6a_se_reduce_CNN_ds (Conv  (None, 1, 1, 28)    18844       ['block6a_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block6a_se_expand_CNN_ds (Conv  (None, 1, 1, 672)   19488       ['block6a_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block6a_se_excite_CNN_ds (Mult  (None, 7, 7, 672)   0           ['block6a_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block6a_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block6a_project_conv_CNN_ds (C  (None, 7, 7, 192)   129024      ['block6a_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block6a_project_bn_CNN_ds (Bat  (None, 7, 7, 192)   768         ['block6a_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block6b_expand_conv_CNN_ds (Co  (None, 7, 7, 1152)  221184      ['block6a_project_bn_CNN_ds[0][0]\n",
      " nv2D)                                                           ']                               \n",
      "                                                                                                  \n",
      " block6b_expand_bn_CNN_ds (Batc  (None, 7, 7, 1152)  4608        ['block6b_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block6b_expand_activation_CNN_  (None, 7, 7, 1152)  0           ['block6b_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block6b_dwconv_CNN_ds (Depthwi  (None, 7, 7, 1152)  28800       ['block6b_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block6b_bn_CNN_ds (BatchNormal  (None, 7, 7, 1152)  4608        ['block6b_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6b_activation_CNN_ds (Act  (None, 7, 7, 1152)  0           ['block6b_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6b_se_squeeze_CNN_ds (Glo  (None, 1152)        0           ['block6b_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block6b_se_reshape_CNN_ds (Res  (None, 1, 1, 1152)  0           ['block6b_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block6b_se_reduce_CNN_ds (Conv  (None, 1, 1, 48)    55344       ['block6b_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block6b_se_expand_CNN_ds (Conv  (None, 1, 1, 1152)  56448       ['block6b_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block6b_se_excite_CNN_ds (Mult  (None, 7, 7, 1152)  0           ['block6b_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block6b_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block6b_project_conv_CNN_ds (C  (None, 7, 7, 192)   221184      ['block6b_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block6b_project_bn_CNN_ds (Bat  (None, 7, 7, 192)   768         ['block6b_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block6b_drop_CNN_ds (FixedDrop  (None, 7, 7, 192)   0           ['block6b_project_bn_CNN_ds[0][0]\n",
      " out)                                                            ']                               \n",
      "                                                                                                  \n",
      " block6b_add_CNN_ds (Add)       (None, 7, 7, 192)    0           ['block6b_drop_CNN_ds[0][0]',    \n",
      "                                                                  'block6a_project_bn_CNN_ds[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " block6c_expand_conv_CNN_ds (Co  (None, 7, 7, 1152)  221184      ['block6b_add_CNN_ds[0][0]']     \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block6c_expand_bn_CNN_ds (Batc  (None, 7, 7, 1152)  4608        ['block6c_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block6c_expand_activation_CNN_  (None, 7, 7, 1152)  0           ['block6c_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block6c_dwconv_CNN_ds (Depthwi  (None, 7, 7, 1152)  28800       ['block6c_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block6c_bn_CNN_ds (BatchNormal  (None, 7, 7, 1152)  4608        ['block6c_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6c_activation_CNN_ds (Act  (None, 7, 7, 1152)  0           ['block6c_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6c_se_squeeze_CNN_ds (Glo  (None, 1152)        0           ['block6c_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block6c_se_reshape_CNN_ds (Res  (None, 1, 1, 1152)  0           ['block6c_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block6c_se_reduce_CNN_ds (Conv  (None, 1, 1, 48)    55344       ['block6c_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block6c_se_expand_CNN_ds (Conv  (None, 1, 1, 1152)  56448       ['block6c_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block6c_se_excite_CNN_ds (Mult  (None, 7, 7, 1152)  0           ['block6c_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block6c_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block6c_project_conv_CNN_ds (C  (None, 7, 7, 192)   221184      ['block6c_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block6c_project_bn_CNN_ds (Bat  (None, 7, 7, 192)   768         ['block6c_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block6c_drop_CNN_ds (FixedDrop  (None, 7, 7, 192)   0           ['block6c_project_bn_CNN_ds[0][0]\n",
      " out)                                                            ']                               \n",
      "                                                                                                  \n",
      " block6c_add_CNN_ds (Add)       (None, 7, 7, 192)    0           ['block6c_drop_CNN_ds[0][0]',    \n",
      "                                                                  'block6b_add_CNN_ds[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_expand_conv_CNN_ds (Co  (None, 7, 7, 1152)  221184      ['block6c_add_CNN_ds[0][0]']     \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block6d_expand_bn_CNN_ds (Batc  (None, 7, 7, 1152)  4608        ['block6d_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block6d_expand_activation_CNN_  (None, 7, 7, 1152)  0           ['block6d_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block6d_dwconv_CNN_ds (Depthwi  (None, 7, 7, 1152)  28800       ['block6d_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block6d_bn_CNN_ds (BatchNormal  (None, 7, 7, 1152)  4608        ['block6d_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6d_activation_CNN_ds (Act  (None, 7, 7, 1152)  0           ['block6d_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6d_se_squeeze_CNN_ds (Glo  (None, 1152)        0           ['block6d_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block6d_se_reshape_CNN_ds (Res  (None, 1, 1, 1152)  0           ['block6d_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block6d_se_reduce_CNN_ds (Conv  (None, 1, 1, 48)    55344       ['block6d_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block6d_se_expand_CNN_ds (Conv  (None, 1, 1, 1152)  56448       ['block6d_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block6d_se_excite_CNN_ds (Mult  (None, 7, 7, 1152)  0           ['block6d_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block6d_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block6d_project_conv_CNN_ds (C  (None, 7, 7, 192)   221184      ['block6d_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block6d_project_bn_CNN_ds (Bat  (None, 7, 7, 192)   768         ['block6d_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block6d_drop_CNN_ds (FixedDrop  (None, 7, 7, 192)   0           ['block6d_project_bn_CNN_ds[0][0]\n",
      " out)                                                            ']                               \n",
      "                                                                                                  \n",
      " block6d_add_CNN_ds (Add)       (None, 7, 7, 192)    0           ['block6d_drop_CNN_ds[0][0]',    \n",
      "                                                                  'block6c_add_CNN_ds[0][0]']     \n",
      "                                                                                                  \n",
      " block6e_expand_conv_CNN_ds (Co  (None, 7, 7, 1152)  221184      ['block6d_add_CNN_ds[0][0]']     \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block6e_expand_bn_CNN_ds (Batc  (None, 7, 7, 1152)  4608        ['block6e_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block6e_expand_activation_CNN_  (None, 7, 7, 1152)  0           ['block6e_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block6e_dwconv_CNN_ds (Depthwi  (None, 7, 7, 1152)  28800       ['block6e_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block6e_bn_CNN_ds (BatchNormal  (None, 7, 7, 1152)  4608        ['block6e_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6e_activation_CNN_ds (Act  (None, 7, 7, 1152)  0           ['block6e_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6e_se_squeeze_CNN_ds (Glo  (None, 1152)        0           ['block6e_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block6e_se_reshape_CNN_ds (Res  (None, 1, 1, 1152)  0           ['block6e_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block6e_se_reduce_CNN_ds (Conv  (None, 1, 1, 48)    55344       ['block6e_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block6e_se_expand_CNN_ds (Conv  (None, 1, 1, 1152)  56448       ['block6e_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block6e_se_excite_CNN_ds (Mult  (None, 7, 7, 1152)  0           ['block6e_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block6e_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block6e_project_conv_CNN_ds (C  (None, 7, 7, 192)   221184      ['block6e_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block6e_project_bn_CNN_ds (Bat  (None, 7, 7, 192)   768         ['block6e_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block6e_drop_CNN_ds (FixedDrop  (None, 7, 7, 192)   0           ['block6e_project_bn_CNN_ds[0][0]\n",
      " out)                                                            ']                               \n",
      "                                                                                                  \n",
      " block6e_add_CNN_ds (Add)       (None, 7, 7, 192)    0           ['block6e_drop_CNN_ds[0][0]',    \n",
      "                                                                  'block6d_add_CNN_ds[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_expand_conv_CNN_ds (Co  (None, 7, 7, 1152)  221184      ['block6e_add_CNN_ds[0][0]']     \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block7a_expand_bn_CNN_ds (Batc  (None, 7, 7, 1152)  4608        ['block7a_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block7a_expand_activation_CNN_  (None, 7, 7, 1152)  0           ['block7a_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block7a_dwconv_CNN_ds (Depthwi  (None, 7, 7, 1152)  10368       ['block7a_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block7a_bn_CNN_ds (BatchNormal  (None, 7, 7, 1152)  4608        ['block7a_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block7a_activation_CNN_ds (Act  (None, 7, 7, 1152)  0           ['block7a_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block7a_se_squeeze_CNN_ds (Glo  (None, 1152)        0           ['block7a_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block7a_se_reshape_CNN_ds (Res  (None, 1, 1, 1152)  0           ['block7a_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block7a_se_reduce_CNN_ds (Conv  (None, 1, 1, 48)    55344       ['block7a_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block7a_se_expand_CNN_ds (Conv  (None, 1, 1, 1152)  56448       ['block7a_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block7a_se_excite_CNN_ds (Mult  (None, 7, 7, 1152)  0           ['block7a_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block7a_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block7a_project_conv_CNN_ds (C  (None, 7, 7, 320)   368640      ['block7a_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block7a_project_bn_CNN_ds (Bat  (None, 7, 7, 320)   1280        ['block7a_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block7b_expand_conv_CNN_ds (Co  (None, 7, 7, 1920)  614400      ['block7a_project_bn_CNN_ds[0][0]\n",
      " nv2D)                                                           ']                               \n",
      "                                                                                                  \n",
      " block7b_expand_bn_CNN_ds (Batc  (None, 7, 7, 1920)  7680        ['block7b_expand_conv_CNN_ds[0][0\n",
      " hNormalization)                                                 ]']                              \n",
      "                                                                                                  \n",
      " block7b_expand_activation_CNN_  (None, 7, 7, 1920)  0           ['block7b_expand_bn_CNN_ds[0][0]'\n",
      " ds (Activation)                                                 ]                                \n",
      "                                                                                                  \n",
      " block7b_dwconv_CNN_ds (Depthwi  (None, 7, 7, 1920)  17280       ['block7b_expand_activation_CNN_d\n",
      " seConv2D)                                                       s[0][0]']                        \n",
      "                                                                                                  \n",
      " block7b_bn_CNN_ds (BatchNormal  (None, 7, 7, 1920)  7680        ['block7b_dwconv_CNN_ds[0][0]']  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block7b_activation_CNN_ds (Act  (None, 7, 7, 1920)  0           ['block7b_bn_CNN_ds[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block7b_se_squeeze_CNN_ds (Glo  (None, 1920)        0           ['block7b_activation_CNN_ds[0][0]\n",
      " balAveragePooling2D)                                            ']                               \n",
      "                                                                                                  \n",
      " block7b_se_reshape_CNN_ds (Res  (None, 1, 1, 1920)  0           ['block7b_se_squeeze_CNN_ds[0][0]\n",
      " hape)                                                           ']                               \n",
      "                                                                                                  \n",
      " block7b_se_reduce_CNN_ds (Conv  (None, 1, 1, 80)    153680      ['block7b_se_reshape_CNN_ds[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block7b_se_expand_CNN_ds (Conv  (None, 1, 1, 1920)  155520      ['block7b_se_reduce_CNN_ds[0][0]'\n",
      " 2D)                                                             ]                                \n",
      "                                                                                                  \n",
      " block7b_se_excite_CNN_ds (Mult  (None, 7, 7, 1920)  0           ['block7b_activation_CNN_ds[0][0]\n",
      " iply)                                                           ',                               \n",
      "                                                                  'block7b_se_expand_CNN_ds[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block7b_project_conv_CNN_ds (C  (None, 7, 7, 320)   614400      ['block7b_se_excite_CNN_ds[0][0]'\n",
      " onv2D)                                                          ]                                \n",
      "                                                                                                  \n",
      " block7b_project_bn_CNN_ds (Bat  (None, 7, 7, 320)   1280        ['block7b_project_conv_CNN_ds[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " block7b_drop_CNN_ds (FixedDrop  (None, 7, 7, 320)   0           ['block7b_project_bn_CNN_ds[0][0]\n",
      " out)                                                            ']                               \n",
      "                                                                                                  \n",
      " block7b_add_CNN_ds (Add)       (None, 7, 7, 320)    0           ['block7b_drop_CNN_ds[0][0]',    \n",
      "                                                                  'block7a_project_bn_CNN_ds[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " top_conv_CNN_ds (Conv2D)       (None, 7, 7, 1280)   409600      ['block7b_add_CNN_ds[0][0]']     \n",
      "                                                                                                  \n",
      " top_bn_CNN_ds (BatchNormalizat  (None, 7, 7, 1280)  5120        ['top_conv_CNN_ds[0][0]']        \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " top_activation_CNN_ds (Activat  (None, 7, 7, 1280)  0           ['top_bn_CNN_ds[0][0]']          \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " avg_pool_CNN_ds (GlobalAverage  (None, 1280)        0           ['top_activation_CNN_ds[0][0]']  \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_46_CNN_ds   (None, 1280)        5120        ['avg_pool_CNN_ds[0][0]']        \n",
      " (BatchNormalization)                                                                             \n",
      "                                                                                                  \n",
      " dense_15_input (InputLayer)    [(None, 47)]         0           []                               \n",
      "                                                                                                  \n",
      " top_dropout_CNN_ds (Dropout)   (None, 1280)         0           ['batch_normalization_46_CNN_ds[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " dense_15_MLP_ds (Dense)        (None, 16)           768         ['dense_15_input[0][0]']         \n",
      "                                                                                                  \n",
      " dense_86_CNN_ds (Dense)        (None, 512)          655872      ['top_dropout_CNN_ds[0][0]']     \n",
      "                                                                                                  \n",
      " dropout_16_MLP_ds (Dropout)    (None, 16)           0           ['dense_15_MLP_ds[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_55_CNN_ds (Dropout)    (None, 512)          0           ['dense_86_CNN_ds[0][0]']        \n",
      "                                                                                                  \n",
      " dense_16_MLP_ds (Dense)        (None, 8)            136         ['dropout_16_MLP_ds[0][0]']      \n",
      "                                                                                                  \n",
      " dense_87_CNN_ds (Dense)        (None, 128)          65664       ['dropout_55_CNN_ds[0][0]']      \n",
      "                                                                                                  \n",
      " dense_17_MLP_ds (Dense)        (None, 8)            72          ['dense_16_MLP_ds[0][0]']        \n",
      "                                                                                                  \n",
      " dense_88_CNN_ds (Dense)        (None, 16)           2064        ['dense_87_CNN_ds[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_17_MLP_ds (Dropout)    (None, 8)            0           ['dense_17_MLP_ds[0][0]']        \n",
      "                                                                                                  \n",
      " dense_89_CNN_ds (Dense)        (None, 4)            68          ['dense_88_CNN_ds[0][0]']        \n",
      "                                                                                                  \n",
      " dense_18_MLP_ds (Dense)        (None, 4)            36          ['dropout_17_MLP_ds[0][0]']      \n",
      "                                                                                                  \n",
      " pred_CNN_ds (Dense)            (None, 2)            10          ['dense_89_CNN_ds[0][0]']        \n",
      "                                                                                                  \n",
      " dense_19_MLP_ds (Dense)        (None, 2)            10          ['dense_18_MLP_ds[0][0]']        \n",
      "                                                                                                  \n",
      " dempster_shafer_layer_4 (Demps  (None, 2)           0           ['pred_CNN_ds[0][0]']            \n",
      " terShaferLayer)                                                                                  \n",
      "                                                                                                  \n",
      " dempster_shafer_layer_5 (Demps  (None, 2)           0           ['dense_19_MLP_ds[0][0]']        \n",
      " terShaferLayer)                                                                                  \n",
      "                                                                                                  \n",
      " dempster_combination_layer (De  (None, 2)           0           ['dempster_shafer_layer_4[0][0]',\n",
      " mpsterCombinationLayer)                                          'dempster_shafer_layer_5[0][0]']\n",
      "                                                                                                  \n",
      " generalized_pignistic_probabil  (None, 2)           0           ['dempster_combination_layer[0][0\n",
      " ity_layer (GeneralizedPignisti                                  ]']                              \n",
      " cProbabilityLayer)                                                                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,305,052\n",
      "Trainable params: 0\n",
      "Non-trainable params: 7,305,052\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## EVALUATION\n",
    "gc.collect()\n",
    "\n",
    "bestPt = checkpointPath / Path('MULTIMODAL_DEMPSTER_GPT_Classification_F1')\n",
    "model = tf.keras.models.load_model(bestPt)\n",
    "\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      CNN_pred_1  CNN_pred_2\n",
       "0      0.543555    0.469571\n",
       "1      0.715446    0.289588\n",
       "2      0.496123    0.503565\n",
       "3      0.496123    0.503565\n",
       "4      0.496123    0.503565\n",
       "..          ...         ...\n",
       "106    0.504614    0.497487\n",
       "107    0.496123    0.503565\n",
       "108    0.496123    0.503565\n",
       "109    0.503013    0.498633\n",
       "110    0.588955    0.436770\n",
       "\n",
       "[111 rows x 2 columns]>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "#                                        outputs=model.get_layer(\"pred_CNN_ds\").output)\n",
    "intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "                                       outputs=model.layers[-6].output)\n",
    "intermediate_output = intermediate_layer_model([X_val_IMGS,X_val])\n",
    "uncerainties_Lr2 = intermediate_output.numpy()\n",
    "uncerainties_Lr2.shape\n",
    "uncerainties_Lr2_rs = uncerainties_Lr2.reshape(uncerainties_Lr2.shape[0],uncerainties_Lr2.shape[1])\n",
    "CNN_pred = pd.DataFrame(uncerainties_Lr2_rs, columns = ['CNN_pred_1','CNN_pred_2'])\n",
    "CNN_pred.head\n",
    "# tetha_Other.to_csv('../massesFromProb_theta_bel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      MLP_pred_1  MLP_pred_2\n",
       "0      0.543555    0.469571\n",
       "1      0.715446    0.289588\n",
       "2      0.496123    0.503565\n",
       "3      0.496123    0.503565\n",
       "4      0.496123    0.503565\n",
       "..          ...         ...\n",
       "106    0.504614    0.497487\n",
       "107    0.496123    0.503565\n",
       "108    0.496123    0.503565\n",
       "109    0.503013    0.498633\n",
       "110    0.588955    0.436770\n",
       "\n",
       "[111 rows x 2 columns]>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "#                                        outputs=model.get_layer(\"dense_19_MLP_ds\").output)\n",
    "intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "                                       outputs=model.layers[-5].output)\n",
    "intermediate_output = intermediate_layer_model.predict([X_val_IMGS,X_val])\n",
    "# uncerainties_Lr2 = intermediate_output.numpy()\n",
    "uncerainties_Lr2.shape\n",
    "# uncerainties_Lr2_rs = uncerainties_Lr2.reshape(uncerainties_Lr2.shape[0],uncerainties_Lr2.shape[1])\n",
    "MLP_pred = pd.DataFrame(uncerainties_Lr2_rs, columns = ['MLP_pred_1','MLP_pred_2'])\n",
    "MLP_pred.head\n",
    "# tetha_Other.to_csv('../massesFromProb_theta_bel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (111, 2), indices imply (111, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mm:\\UNIUD\\INDUSTRIAL PROJECTS\\Notebooks\\Severstal\\Covid_Uncertanty_test_new_unc.ipynb Cell 119\u001b[0m line \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y225sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m uncerainties_Lr2\u001b[39m.\u001b[39mshape\n\u001b[0;32m      <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y225sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m uncerainties_Lr2_rs \u001b[39m=\u001b[39m uncerainties_Lr2\u001b[39m.\u001b[39mreshape(uncerainties_Lr2\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],uncerainties_Lr2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y225sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m CNN_masses \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(uncerainties_Lr2_rs, columns \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39mMASS_CNN_1\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mMASS_CNN_2\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mMASS_CNN_theta\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/m%3A/UNIUD/INDUSTRIAL%20PROJECTS/Notebooks/Severstal/Covid_Uncertanty_test_new_unc.ipynb#Y225sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m CNN_masses\u001b[39m.\u001b[39mhead\n",
      "File \u001b[1;32mc:\\Users\\Michele\\anaconda3\\envs\\tfNew2023_2\\lib\\site-packages\\pandas\\core\\frame.py:694\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    684\u001b[0m         mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    685\u001b[0m             \u001b[39m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    686\u001b[0m             \u001b[39m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    691\u001b[0m             typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    692\u001b[0m         )\n\u001b[0;32m    693\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    695\u001b[0m             data,\n\u001b[0;32m    696\u001b[0m             index,\n\u001b[0;32m    697\u001b[0m             columns,\n\u001b[0;32m    698\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    699\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    700\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[0;32m    701\u001b[0m         )\n\u001b[0;32m    703\u001b[0m \u001b[39m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mc:\\Users\\Michele\\anaconda3\\envs\\tfNew2023_2\\lib\\site-packages\\pandas\\core\\internals\\construction.py:351\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[39m# _prep_ndarray ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    347\u001b[0m index, columns \u001b[39m=\u001b[39m _get_axes(\n\u001b[0;32m    348\u001b[0m     values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], index\u001b[39m=\u001b[39mindex, columns\u001b[39m=\u001b[39mcolumns\n\u001b[0;32m    349\u001b[0m )\n\u001b[1;32m--> 351\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    353\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    355\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Michele\\anaconda3\\envs\\tfNew2023_2\\lib\\site-packages\\pandas\\core\\internals\\construction.py:422\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    420\u001b[0m passed \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mshape\n\u001b[0;32m    421\u001b[0m implied \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(index), \u001b[39mlen\u001b[39m(columns))\n\u001b[1;32m--> 422\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of passed values is \u001b[39m\u001b[39m{\u001b[39;00mpassed\u001b[39m}\u001b[39;00m\u001b[39m, indices imply \u001b[39m\u001b[39m{\u001b[39;00mimplied\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (111, 2), indices imply (111, 3)"
     ]
    }
   ],
   "source": [
    "# intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "#                                        outputs=model.get_layer(\"dempster_shafer_layer\").output)\n",
    "intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "                                       outputs=model.layers[-4].output)\n",
    "intermediate_output = intermediate_layer_model([X_val_IMGS,X_val])\n",
    "uncerainties_Lr2 = intermediate_output.numpy()\n",
    "uncerainties_Lr2.shape\n",
    "uncerainties_Lr2_rs = uncerainties_Lr2.reshape(uncerainties_Lr2.shape[0],uncerainties_Lr2.shape[1])\n",
    "CNN_masses = pd.DataFrame(uncerainties_Lr2_rs, columns = ['MASS_CNN_1','MASS_CNN_2','MASS_CNN_theta'])\n",
    "CNN_masses.head\n",
    "# tetha_Other.to_csv('../massesFromProb_theta_bel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      MASS_MLP_1  MASS_MLP_2  MASS_MLP_theta\n",
       "0      0.087271   -0.060974        0.973703\n",
       "1      0.430782   -0.420346        0.989563\n",
       "2     -0.007755    0.007130        1.000625\n",
       "3     -0.007755    0.007130        1.000625\n",
       "4     -0.007755    0.007130        1.000625\n",
       "..          ...         ...             ...\n",
       "106    0.009308   -0.005084        0.995776\n",
       "107   -0.007755    0.007130        1.000625\n",
       "108   -0.007755    0.007130        1.000625\n",
       "109    0.006086   -0.002778        0.996692\n",
       "110    0.178067   -0.126574        0.948507\n",
       "\n",
       "[111 rows x 3 columns]>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "#                                        outputs=model.get_layer(\"dempster_shafer_layer_1\").output)\n",
    "intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "                                       outputs=model.layers[-3].output)\n",
    "intermediate_output = intermediate_layer_model.predict([X_val_IMGS,X_val])\n",
    "# uncerainties_Lr2 = intermediate_output.numpy()\n",
    "uncerainties_Lr2.shape\n",
    "uncerainties_Lr2_rs = uncerainties_Lr2.reshape(uncerainties_Lr2.shape[0],uncerainties_Lr2.shape[1])\n",
    "MLP_masses = pd.DataFrame(uncerainties_Lr2_rs, columns = ['MASS_MLP_1','MASS_MLP_2','MASS_MLP_theta'])\n",
    "MLP_masses.head\n",
    "# tetha_Other.to_csv('../massesFromProb_theta_bel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 85ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      MASS_1_COMB  MASS_2_COMB  MASS_theta_COMB\n",
       "0       0.087271    -0.060974         0.973703\n",
       "1       0.430782    -0.420346         0.989563\n",
       "2      -0.007755     0.007130         1.000625\n",
       "3      -0.007755     0.007130         1.000625\n",
       "4      -0.007755     0.007130         1.000625\n",
       "..           ...          ...              ...\n",
       "106     0.009308    -0.005084         0.995776\n",
       "107    -0.007755     0.007130         1.000625\n",
       "108    -0.007755     0.007130         1.000625\n",
       "109     0.006086    -0.002778         0.996692\n",
       "110     0.178067    -0.126574         0.948507\n",
       "\n",
       "[111 rows x 3 columns]>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "#                                        outputs=model.get_layer(\"dempster_combination_layer\").output)\n",
    "intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "                                       outputs=model.layers[-2].output)\n",
    "intermediate_output = intermediate_layer_model.predict([X_val_IMGS,X_val])\n",
    "# uncerainties_Lr2 = intermediate_output.numpy()\n",
    "uncerainties_Lr2.shape\n",
    "uncerainties_Lr2_rs = uncerainties_Lr2.reshape(uncerainties_Lr2.shape[0],uncerainties_Lr2.shape[-1])\n",
    "COMBINATION = pd.DataFrame(uncerainties_Lr2_rs, columns = ['MASS_1_COMB','MASS_2_COMB','MASS_theta_COMB'])\n",
    "COMBINATION.head\n",
    "# tetha_Other.to_csv('../massesFromProb_theta_bel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 105ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         PIG_1     PIG_2  PIG_THETA??\n",
       "0    0.087271 -0.060974     0.973703\n",
       "1    0.430782 -0.420346     0.989563\n",
       "2   -0.007755  0.007130     1.000625\n",
       "3   -0.007755  0.007130     1.000625\n",
       "4   -0.007755  0.007130     1.000625\n",
       "..        ...       ...          ...\n",
       "106  0.009308 -0.005084     0.995776\n",
       "107 -0.007755  0.007130     1.000625\n",
       "108 -0.007755  0.007130     1.000625\n",
       "109  0.006086 -0.002778     0.996692\n",
       "110  0.178067 -0.126574     0.948507\n",
       "\n",
       "[111 rows x 3 columns]>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "#                                        outputs=model.get_layer(\"generalized_pignistic_probability_layer\").output)\n",
    "intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "                                       outputs=model.layers[-1].output)\n",
    "intermediate_output = intermediate_layer_model.predict([X_val_IMGS,X_val])\n",
    "# uncerainties_Lr2 = intermediate_output.numpy()\n",
    "uncerainties_Lr2.shape\n",
    "uncerainties_Lr2_rs = uncerainties_Lr2.reshape(uncerainties_Lr2.shape[0],uncerainties_Lr2.shape[-1])\n",
    "pigProb = pd.DataFrame(uncerainties_Lr2_rs, columns = ['PIG_1','PIG_2','PIG_THETA??'])\n",
    "pigProb.head\n",
    "# tetha_Other.to_csv('../massesFromProb_theta_bel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      CNN_pred_1  CNN_pred_2  MLP_pred_1  MLP_pred_2  MASS_CNN_1  MASS_CNN_2  \\\n",
       "0      0.543635    0.469513    0.543635    0.469513    0.087271   -0.060974   \n",
       "1      0.715391    0.289827    0.715391    0.289827    0.430782   -0.420346   \n",
       "2      0.496123    0.503565    0.496123    0.503565   -0.007755    0.007130   \n",
       "3      0.496123    0.503565    0.496123    0.503565   -0.007755    0.007130   \n",
       "4      0.496123    0.503565    0.496123    0.503565   -0.007755    0.007130   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "106    0.504654    0.497458    0.504654    0.497458    0.009308   -0.005084   \n",
       "107    0.496123    0.503565    0.496123    0.503565   -0.007755    0.007130   \n",
       "108    0.496123    0.503565    0.496123    0.503565   -0.007755    0.007130   \n",
       "109    0.503043    0.498611    0.503043    0.498611    0.006086   -0.002778   \n",
       "110    0.589033    0.436713    0.589033    0.436713    0.178067   -0.126574   \n",
       "\n",
       "     MASS_CNN_theta  MASS_MLP_1  MASS_MLP_2  MASS_MLP_theta  MASS_1_COMB  \\\n",
       "0          0.973703    0.087271   -0.060974        0.973703     0.087271   \n",
       "1          0.989563    0.430782   -0.420346        0.989563     0.430782   \n",
       "2          1.000625   -0.007755    0.007130        1.000625    -0.007755   \n",
       "3          1.000625   -0.007755    0.007130        1.000625    -0.007755   \n",
       "4          1.000625   -0.007755    0.007130        1.000625    -0.007755   \n",
       "..              ...         ...         ...             ...          ...   \n",
       "106        0.995776    0.009308   -0.005084        0.995776     0.009308   \n",
       "107        1.000625   -0.007755    0.007130        1.000625    -0.007755   \n",
       "108        1.000625   -0.007755    0.007130        1.000625    -0.007755   \n",
       "109        0.996692    0.006086   -0.002778        0.996692     0.006086   \n",
       "110        0.948507    0.178067   -0.126574        0.948507     0.178067   \n",
       "\n",
       "     MASS_2_COMB  MASS_theta_COMB     PIG_1     PIG_2  PIG_THETA??  \n",
       "0      -0.060974         0.973703  0.087271 -0.060974     0.973703  \n",
       "1      -0.420346         0.989563  0.430782 -0.420346     0.989563  \n",
       "2       0.007130         1.000625 -0.007755  0.007130     1.000625  \n",
       "3       0.007130         1.000625 -0.007755  0.007130     1.000625  \n",
       "4       0.007130         1.000625 -0.007755  0.007130     1.000625  \n",
       "..           ...              ...       ...       ...          ...  \n",
       "106    -0.005084         0.995776  0.009308 -0.005084     0.995776  \n",
       "107     0.007130         1.000625 -0.007755  0.007130     1.000625  \n",
       "108     0.007130         1.000625 -0.007755  0.007130     1.000625  \n",
       "109    -0.002778         0.996692  0.006086 -0.002778     0.996692  \n",
       "110    -0.126574         0.948507  0.178067 -0.126574     0.948507  \n",
       "\n",
       "[111 rows x 16 columns]>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OutputToPrint = pd.concat([CNN_pred, MLP_pred, CNN_masses, MLP_masses, COMBINATION, pigProb], axis=1)\n",
    "OutputToPrint.to_csv('../massesCombination_GPT.csv')\n",
    "OutputToPrint.head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfNew2023_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
